\input{../../common/livre_begin.tex}
\firstpassagedo{\input{svm_titre.tex}}
\input{../../common/livre_table_begin.tex}
\firstpassagedo{\input{svm_chapter.tex}}



\label{annexe_svm}
\indexfr{SVM}
\indexsee{Support Vector Machine}{SVM}
\indexsee{S�parateur � Vastes Marges}{SVM}

\indexfrr{ACP}{SVM}
\indexfr{m�thodes � noyaux}

Les \emph{Support Vector Machine} ou \emph{S�parateurs � Vastes Marges} (SVM) ont �t� pour la premi�re fois pr�sent�s par V. Vapnik d�s 1979 (voir \citeindex{Vapnik1979}) et sont plus amplement d�velopp�s dans \citeindex{Vapnik1998}. Les d�finitions et r�sultats propos�s sont extraits de \citeindex{Burges1998}, document plus didactique d'apr�s son auteur, \citeindex{Smola2004} -~ cet article existe en une version plus �tendue (voir \citeindex{Smola1998})~- document plus complet qui pr�sente la r�gression � partir de SVM et l'article \citeindex{M�ller2001}, document plus r�cent qui �voque notamment l'analyse en composantes principales � partir de SVM. Ce dernier document applique les SVM � la reconnaissance de caract�res. Cette annexe n'a pas pour but de d�crire en d�tail ces mod�les mais seulement de les introduire sommairement. Le site internet \textit{http://www.kernel-machines.org/} r�f�rence tous ces documents et recense les derniers d�veloppements autour des m�thodes � noyaux dont font partie les SVM. Il r�f�rence �galement un large panel d'applications ou de code informatique permettant d'utiliser les m�thodes � noyaux.



%------------------------------------------------------------------------------------------------------------------
\section{S�parateur lin�aire}
%------------------------------------------------------------------------------------------------------------------

\label{svm_separateur_lineaire}

\subsection{Ensemble s�parable}
\indexfrr{ensemble}{s�parable}


On s'int�resse tout d'abord � l'hyperplan s�parateur d'un ensemble de points r�partis en deux classes. Cet ensemble est not� $\pa{X_i,Y_i}_{1 \infegal i \infegal N}$ o�, $\forall i$, $X_i \in \mathbb{R}^d$ et $Y_i \in \acc{-1,1}$. Pour simplifier les expressions par la suite, les deux classes sont donc labell�es -1 et~1. On cherche alors un vecteur $w$ et une constante $b$ qui v�rifient~:

            $$
            \forall i, \; 1 \infegal i \infegal N, \; 
                        Y_i = \left\{ \begin{array}{rl}
                                                        -1 & \text{ si } w.X_i + b \supegal 1 \\ 
                                                         1 & \text{ si } w.X_i + b \infegal -1
                                                    \end{array} \right.
            $$

On cherche donc $w$ et $b$ tels que~:

            $$
            \forall i, \; 1 \infegal i \infegal N, \; 
                        Y_i \pa{ w.X_i + b} - 1 \supegal 0
            $$

Comme on cherche �galement un vecteur $w$ de norme minimum, l'hyperplan cherch� est la solution du probl�me de minimsation suivant~:


            \begin{xproblem}{meilleur hyperplan s�parateur, cas s�parable}\label{svm_problem_def}
            \indexfr{s�parable}\indexfrr{hyperplan}{s�parateur}
            Le meilleur hyperplan s�parateur de l'ensemble de points labell�s
            $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{ \mathbb{R}^d \times \acc{-1,1} }^N$ est la solution
            d'un probl�me de minimisation. Cet hyperplan a pour �quation $x.w^* + b^* = 0$ o� 
            $w^*$ et $b^*$ v�rifient~:
                    $$
                    \begin{array}{rcl}    \pa{w^*,b^*} &=& \underset{w,b}{\arg \min} \frac{1}{2} \norme{w}^2 \\
                                         && \text{avec } \forall i, \; Y_i \pa{ X_i .w + b } -1 \supegal 0 
                    \end{array}                                        
                    $$
            \end{xproblem}


\indexfrr{Lagrange}{multiplicateurs}
La r�solution d'un tel probl�me s'effectue � l'aide des multiplicateurs de Lagrange, on affecte � chaque contrainte le coefficient $\alpha_i$, il s'agit alors de minimiser l'expression~:

            \begin{eqnarray}
            L_P = \frac{1}{2} \norme{w}^2 - \summy{i=1}{N} \alpha_i Y_i \pa{ X_i . w + b } + \summy{i=1}{N} \alpha_i
            \label{svm_lagrange_lineaire}
            \end{eqnarray}

En d�rivant par rapport � $w$ et $b$, on obtient que~:

        \begin{eqnarray}
        w                                                     &=& \sum_{i=1}^N \alpha_i Y_i X_i \\
        \summy{i=1}{N} \alpha_i Y_i &=& 0
        \end{eqnarray}
        
Par cons�quent, on peut substituer l'expression~\ref{svm_lagrange_lineaire} par~:

            \begin{eqnarray}
            L_D = \frac{1}{2} \summy{i=1}{N}\summy{j=1}{N} 
                                \alpha_i \alpha_j \, Y_i Y_j \, X_i . X_j - 
                                \summy{i=1}{N} \alpha_i
            \label{svm_lagrange_lineaire_2}
            \end{eqnarray}

\indexfr{noyau}\indexfrr{fonction}{noyau}
Cette derni�re �quation (\ref{svm_lagrange_lineaire_2}) est importante puisqu'elle permet d'introduire les SVM non lin�aires pour lesquels le produit scalaire $X_i. X_j$ sera remplac� par une fonction noyau $K\pa{X_i, X_j}$. 




\subsection{Ensemble non s�parable}
\indexfrr{ensemble}{non s�parable}

Le paragraphe pr�c�dent supposait que l'ensemble $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{ \mathbb{R}^d \times \acc{-1,1} }^N$ �tait s�parable ce qui, d'apr�s le paragraphe~\ref{svm_dimension_vc_lin} implique dans la plupart des cas que $N \infegal d+1$. Pour un ensemble non s�parable (voir figure~\ref{svm_non_separable_fig}), il est impossible de trouver un hyperplan s�parateur. Par cons�quent, il n'existe pas de solution au probl�me~\ref{svm_problem_def} v�rifiant les contraintes telles qu'elles ont �t� exprim�es. La recherche du meilleur hyperplan s�parateur devient alors l'�nonc�~\ref{svm_problem_def_2}.


        \begin{figure}[ht]
    $$\frame{$\begin{array}[c|c]{c}\includegraphics[height=3cm, width=3cm]
    {\filext{../svm/image/non}}\end{array}$}$$
    \caption{    Exemple d'un nuage de points non s�parable par un hyperplan.}
    \label{svm_non_separable_fig}
        \end{figure}



            \begin{xproblem}{meilleur hyperplan s�parateur, cas non s�parable}\label{svm_problem_def_2}
            \indexfr{non s�parable}
            Soit $C \in \mathbb{R}^*_+$ une constante et $k \in \N^*$ un entier,
            le meilleur hyperplan s�parateur de l'ensemble de points labell�s
            $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{ \mathbb{R}^d \times \acc{-1,1} }^N$ est la solution
            d'un probl�me de minimisation. Cet hyperplan a pour �quation $x.w^* + b^* = 0$ o� 
            $w^*$ et $b^*$ v�rifient~:
                    $$
                    \begin{array}{rcl}    \pa{w^*,b^*} &=& \underset{w,b}{\arg \min} \dfrac{1}{2} \norme{w}^2 + 
                                                                                                C \pa{\summy{i=1}{N} \xi_i}^k \\
                                         \text{avec }      && \forall i, \; Y_i \pa{ X_i .w + b + \xi_i } - 1 \supegal 0 \\
                                         \text{et }            && \forall i, \; \xi_i \supegal 0 
                    \end{array}                                        
                    $$
            \end{xproblem}

$C$ et $k$ sont des constantes � d�terminer. Toutefois, dans le cas o� $k = 1$, la solution du probl�me pr�c�dent est identique � celle du probl�me suivant~:


            \begin{xproblem}{meilleur hyperplan s�parateur, cas non s�parable, probl�me dual}
            \label{svm_problem_def_2p}\indexfr{non s�parable}\indexfrr{probl�me}{dual}
            Soit $C \in \mathbb{R}^*_+$ une constante,
            le meilleur hyperplan s�parateur de l'ensemble de points labell�s
            $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{ \mathbb{R}^d \times \acc{-1,1} }^N$ est la solution
            d'un probl�me de minimisation. 
                    $$
                    \begin{array}{rcl}    \pa{\alpha_i^*} &=& \underset{\alpha_i}{\arg \min} \dfrac{1}{2}  
                                                                                    \summy{i=1}{N}\summy{j=1}{N}
                                                                                            \alpha_i \alpha_j \,
                                                                                            Y_i Y_j \,
                                                                                            X_i . X_j
                                                                                    - \summy{i=1}{N} \alpha_i \\
                                         \text{avec }      && \forall i, \; 1 \infegal \alpha_i \infegal C \\
                                         \text{et }          && \summy{i=1}{N} Y_i \, \alpha_i = 0
                    \end{array}                                        
                    $$
            L'hyperplan s�parateur est donn� par l'�quation $ x.w + b = 0$ o� 
            $w = \summy{i=1}{N} \alpha_i Y_i X_i$.
            \end{xproblem}

\indexfr{dual}
Ce dernier probl�me est appel�e la forme duale du probl�me~\ref{svm_problem_def_2}.

%------------------------------------------------------------------------------------------------------------------
\section{Dimension de Vapnik-Chervonenkis (VC)}
%------------------------------------------------------------------------------------------------------------------
\label{svm_dimension_vc}



\subsection{D�finition}

\indexfr{dimension de Vapnik-Chervonenkis}



Dans le probl�me de classification introduit au chapitre~\ref{svm_separateur_lineaire}, la dimension de Vapnik-Chervonenkis sert � majorer le risque d'erreur de classification empirique au risque d'erreur th�orique. Nous allons tout d'abord d�finir la dimension de Vapnik-Chervonenkis pour un ensemble de points donn� et not� $\pa{X_i}_{1 \infegal i \infegal N}$ et une classe de fonction $f\pa{x,\alpha}$ param�tr�e par $\alpha$.



            \begin{xdefinition}{dimension de Vapnik-Chervonenkis}
            Soit $\pa{X_i}_{1 \infegal i \infegal N}$ un ensemble de points appartenant � $\mathbb{R}^d$. On d�finit une 
            fonction $f\pa{x,\alpha} : \mathbb{R}^d \times \Omega \longmapsto  \mathbb{R}$ 
            o� $x \in \mathbb{R}^d$ et $\alpha \in \Omega$.
            $\Omega$ est appel� l'ensemble des param�tres.
            On d�finit la dimension de Vapnik-Chervonenkis comme �tant le nombre de suites 
            $\pa{Y_i}_{1 \infegal i \infegal N} \in \acc{-1,1}^N$ v�rifiant~:
                    $$
                    \exists \alpha \in \Omega, \text{ tel que } \forall i, \; 1 \infegal i \infegal N, \;
                            sgn\pa{ f\pa{X_i,\alpha} } = Y_i
                    $$
            La fonction $sgn\pa{x}$ d�signe le signe de $x$~: $sgn\pa{x} = \left\{ \begin{array}{rl}
                                                                                                                         1 & \text{si } x \supegal 0 \\
                                                                                                                        -1 & \text{si } x < 0 
                                                                                                                        \end{array} \right. $
                                                                                                                        
            Par d�finition, cette dimension est inf�rieure � $2^N$.
            \end{xdefinition}





\subsection{R�sultats}
\label{svm_dimension_vc_lin}


Dans le cas o� la fonction $f$ est lin�aire, il existe quelques r�sultats int�ressants.


        \begin{xtheorem}{dimension VC d'un ensemble de vecteurs lin�airement ind�pendants}
        Soit un ensemble de $N$ points inclus dans l'espace vectoriel $\mathbb{R}^d$ dont un d�finit l'origine.
        Alors les $N$ points peuvent �tre s�par�s de n'importe quel mani�re en deux classes
        par des hyperplans orient�s si et seulement si les vecteurs positions sont lin�airement ind�pendants.
        \end{xtheorem}

        \begin{xcorollary}{dimension VC d'un ensemble de vecteurs lin�airement ind�pendants}
        La dimension de Vapnik-Chervonenkis d'un ensemble d'hyperplans s�parateurs de $\mathbb{R}^d$ est $d+1$
        puisqu'il est toujours possible de choisir $d+1$ points lin�airement ind�pendants qui puissent 
        �tre s�par�s quelque soit leurs classes.
        \end{xcorollary}



\subsection{Exemple}

On d�finit la suite de points $\pa{X_i}_{1 \infegal i \infegal N}$ par $\forall i, \, 1 \infegal i \infegal N, \; X_i = 10^{-i}$ et l'ensemble de fonctions~:

            $$
            \acc{\alpha \in \mathbb{R}, \; f\pa{x,\alpha} = \left\{
                                                                \begin{array}{rl}
                                                                1     & \text{ si } \sin \alpha x \supegal 0 \\
                                                                -1    & \text{ si } \sin \alpha x < 0 
                                                                \end{array} \right.}
            $$


Quelque soit la suite $\pa{Y_i}_{1 \infegal i \infegal N} \in \acc{-1,1}^N$, il est possible de choisir~:

            $$
            \alpha = \pi \pa{ 1 + \summy{i=1}{N} \frac{ \pa{ 1 - Y_i} 10^i}{ 2 } }
            $$

De telle sorte que~: $\forall i, \; f\pa{X_i,\alpha} = Y_i$. Par cons�quent, la dimension VC cet ensemble de points associ�s � l'ensemble de fonctions $f$ est $2^N$.



\subsection{Risque}

\indexfrr{risque}{th�orique}
On d�finit maintenant le risque th�orique de classification comme �tant~:

            \begin{eqnarray}
            R\pa{\alpha} = \int \frac{1}{2} \abs{y - f\pa{x,\alpha}} dP\pa{x,y}
            \label{svm_risque_theorique}
            \end{eqnarray}

\indexfrr{risque}{empirique}
Et le risque empirique pour le nuage de points $\pa{X_i,Y_i}_{1 \infegal i \infegal N}$ par~:

            \begin{eqnarray}
            R_{emp}\pa{\alpha} = \frac{1}{2N} \; \summy{i=1}{N} \abs{ Y_i - f\pa{X_i,\alpha}}
            \label{svm_risque_empirique}
            \end{eqnarray}

        \begin{xtheorem}{majoration du risque empirique}
        En reprenant les notations utilis�es dans les expressions 
        (\ref{svm_risque_theorique}) et (\ref{svm_risque_empirique}). Pour 
        un nuage de points  $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{\mathbb{R}^d \times \acc{-1,1} }^N$, 
        on d�montre (voir \citeindex{Vapnik1995}) que $\forall \eta \in \cro{0,1}$~:
                    $$
                    \pr{ 
                    R\pa{\alpha} \infegal R_{emp}\pa{\alpha} +
                            \sqrt{\frac    {h \pa{ 1+ \ln \frac{2N}{h} } - \ln \frac{\eta}{4} }
                                                    {N}
                            }
                    } = 1 - \eta
                    $$
        o� $h$ est la dimension de Vapnik-Chervonenkis.
        \end{xtheorem}
        





%------------------------------------------------------------------------------------------------------------------
\section{S�parateur non lin�aire}
%------------------------------------------------------------------------------------------------------------------



\subsection{Principe}

Il est possible d'�tendre les SVM au cas non lin�aire � partir du probl�me~\ref{svm_problem_def_2} d'apr�s \citeindex{Boser1992} en rempla�ant le produit scalaire $X_i . X_j$ par une fonction noyau telle qu'une fonction gaussienne\footnote{$K\pa{X,Y} = \exp\pa{ - \frac{ \norme{ X - Y }^2 }{2 \sigma^2}}$}.


            \begin{xproblem}{meilleur hyperplan, cas non s�parable, non lin�aire, probl�me dual}\label{svm_problem_def_3}
            \indexfr{non s�parable}
            \indexfrr{probl�me}{dual}
            Soit $C \in \mathbb{R}^*_+$ une constante, soit $K : \mathbb{R}^d \times \mathbb{R}^d \longmapsto \mathbb{R}^+$ une fonction noyau,
            le meilleur hyperplan s�parateur de l'ensemble de points labell�s
            $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{ \mathbb{R}^d \times \acc{-1,1} }^N$ est la solution
            d'un probl�me de minimisation~: 
                    $$
                    \begin{array}{rcl}    \pa{\alpha_i^*} &=& \underset{\alpha_i}{\arg \min} \dfrac{1}{2}  
                                                                                    \summy{i=1}{N}\summy{j=1}{N}
                                                                                            \alpha_i \alpha_j \,
                                                                                            Y_i Y_j \,
                                                                                            K\pa{X_i,X_j}
                                                                                    - \summy{i=1}{N} \alpha_i \\
                                         \text{avec }      && \forall i, \; 1 \infegal \alpha_i \infegal C \\
                                         \text{et }          && \summy{i=1}{N} Y_i \, \alpha_i = 0
                    \end{array}                                        
                    $$
            La classification d'un �l�ment $x \in \mathbb{R}^d$ d�pend du signe de la fonction~:
                    $$
                    f\pa{x} = \summy{i=1}{N} \alpha_i Y_i K\pa{X_i,x} + b
                    $$
            \end{xproblem}


\subsection{Interpr�tation, exemple}


Le probl�me~\ref{svm_problem_def_3} revient en fait � projeter l'�l�ment $X_i \in \mathbb{R}^d$ dans un autre espace de dimension g�n�ralement sup�rieure $\mathbb{R}^{d'}$ dans lequel la s�paration sera un hyperplan. Par exemple, on d�finit le noyau $K : \mathbb{R}^2 \times \mathbb{R}^2 \longmapsto \mathbb{R}^+$ par~:

            $$
            K\pa{X_i,X_j} = \pa{X_i.X_j}^2
            $$

On d�finit �galement la fonction $\Phi : \mathbb{R}^2 \longmapsto \mathbb{R}^3$ par~:

            $$
            \Phi\pa{x_1,x_2} = \pa{ \begin{array}{c} x_1^2 \\ \sqrt{2} x_1 x_2 \\ x_2^2 \end{array} }
            $$

On v�rifie alors que~:

            $$
            K\pa{X_i,X_j} = \Phi\pa{X_i} . \Phi\pa{X_j}
            $$

\indexfr{Mercer}
Plus g�n�ralement, pour qu'un noyau $K$ corresponde � un produit scalaire dans un espace de dimension sup�rieure, il suffit qu'il v�rifie la conditions de Mercer (voir \citeindex{Vapnik1995})~:
    
            $$
            \begin{tabular}{l}
            Pour toute fonction $g$ telle que~: $\displaystyle\int g(x)^2 dx < + \infty$ alors \\
            $\displaystyle\int K\pa{x,y} g(x) g(y) dx dy \supegal 0$
            \end{tabular}
            $$
            
            
            
\subsection{Autre formulation}

Le probl�me~\ref{svm_problem_def_3} peut �tre formul� d'une mani�re diff�rente proche de celle du probl�me~\ref{svm_problem_def_2}.
            

            \begin{xproblem}{meilleur hyperplan s�parateur, cas non s�parable, non lin�aire}\label{svm_problem_def_4}
            \indexfr{non s�parable}
            Soit $C \in \mathbb{R}^*_+$ une constante, soit $K : \mathbb{R}^d \times \mathbb{R}^d \longmapsto \mathbb{R}^+$ une fonction noyau,
            le meilleur hyperplan s�parateur de l'ensemble de points labell�s
            $\pa{X_i,Y_i}_{1 \infegal i \infegal N} \in \pa{ \mathbb{R}^d \times \acc{-1,1} }^N$ est la solution
            d'un probl�me de minimisation~: 
                    $$
                    \begin{array}{rcl}    \pa{\alpha_i^*, \xi_i^*} &=& \underset{\alpha_i}{\arg \min} \dfrac{1}{2}  
                                                                                    \summy{i=1}{N}\summy{j=1}{N}
                                                                                            \alpha_i \alpha_j \,
                                                                                            Y_i Y_j \,
                                                                                            K\pa{X_i,X_j}
                                                                                    + C \pa{\summy{i=1}{N} \xi_i}^k \\ \\
                                         \text{avec }      && \forall i, \;  Y_i \pa{ \summy{k=1}{N} \alpha_k Y_k K\pa{X_k,x} + b + \xi_i } - 1 \supegal 0 \\
                                         \text{et }            && \forall i, \; \xi_i \supegal 0 
                    \end{array}                                        
                    $$
            La classification d'un �l�ment $x \in \mathbb{R}^d$ d�pend du signe de la fonction~:
                    $$
                    f\pa{x} = \summy{i=1}{N} \alpha_i Y_i K\pa{X_i,x} + b
                    $$
            \end{xproblem}
            
            
%------------------------------------------------------------------------------------------------------------------
\section{Extensions}
%------------------------------------------------------------------------------------------------------------------

\subsection{Classification en plusieurs classes}

Jusqu'� pr�sent, le seul probl�me �voqu� concerne une classification en deux classes. Une classification en $N$ classes est n�anmoins possible selon deux strat�gies. La premi�re consiste � isoler une classe contre toutes les autres puis � proc�der r�cusivement de cette mani�re jusqu'� finalement obtenir un probl�me de classification en deux classes. La seconde strat�gie consiste � regrouper le nombre de classes en deux groupes puis � appliquer la m�thode des SVM. Ensuite, � l'int�rieur de chaque groupe, on r�it�re cette m�thode de mani�re � diviser le nombre de classes jusqu'� obtenir un probl�me de classification � deux classes.

Il existe une autre possibilit� plus co�teuse et plus fiable. Si on d�sire r�aliser une classification en $N$ classes, plut�t que de r�aliser au plus $N-1$ classifications en deux classes, on r�alise $\frac{(N-1)^2}{2}$ classifications pour tous les couples de deux diff�rentes classes. Il suffit de prendre la classe qui ressort le plus souvent vainqueur.


\subsection{Ensembles r�duits}

L'article \citeindex{Burges1997} propose une m�thode permettant de r�duire l'ensemble de point $\pa{X_i}$ afin d'acc�lerer la r�solution du probl�me de minimisation, tout en n'accroissant que l�g�rement l'erreur ($\sim$1\% d'apr�s les auteurs). 

\indexfr{courbure}
L'article \citeindex{Zhan2005} propose une autre m�thode qui supprime des points. Plusieurs optimisations sont r�alis�es et, apr�s chaque �tape, les points proches des zones de forte courbure de la fronti�re sont enlev�s de l'ensemble d'apprentissage. L'article �tudie la perte de performances en fonction du nombre de points supprim�s.


\subsection{S�lection des param�tres}

Les probl�mes de minimisation~\ref{svm_problem_def_2}, \ref{svm_problem_def_2p}, \ref{svm_problem_def_3} et~\ref{svm_problem_def_4} mentionnent une constante $C$ dont l'article \citeindex{Mattera1999} (voir �galement \citeindex{Cherkassky2004}) discute le choix dans le cas non pas d'un probl�me de classification mais dans celui d'une r�gression � l'aide des SVM.
\indexfr{r�gression}

\subsection{R�gression}
\indexfr{r�gression}








\newpage

\firstpassagedo{
    \begin{thebibliography}{99}
    \input{svm_biblio.tex}
    \end{thebibliography}
}


\input{../../common/livre_table_end.tex}
\input{../../common/livre_end.tex}




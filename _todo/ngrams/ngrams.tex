\input{../../common/livre_begin.tex}
\firstpassagedo{\input{ngrams_titre.tex}}
\input{../../common/livre_table_begin.tex}
\firstpassagedo{\input{ngrams_chapter.tex}}


Les \emph{n-grammes}\indexfr{n-grammes} sont une mod�lisation statistique du langage, l'id�e est d'observer la
fr�quence des encha�nements de lettres � l'int�rieur des mots, ou la fr�quence des encha�nements de mots � l'int�rieur
d'une phrase. Plus g�n�ralement, les n-grammes mod�lisent sous forme de cha�nes de Markov toute s�quence de symboles appartenant � un ensemble fini.

\indexfrr{s�quence}{symbols}
\indexfrr{cha�ne}{Markov}
\indexfr{Markov}

\label{annexe_ngrams}









%---------------------------------------------------------------------------------------------------------------------
\section{D�finition}
%---------------------------------------------------------------------------------------------------------------------

Les d�finitions qui vont suivre s'adaptent � toutes s�quences d'�l�ments appartement � $E$, un ensemble fini.

        \begin{xdefinition}{n-grammes}\label{n_grammes_definition}
    \indexfr{suite}
        Soit $A = \pa{e,f,\vecteurno{a_1}{a_N}}$ un ensemble fini nomm� \emph{alphabet}, les symboles $e$ et $f$ d�butent 
        et terminent toute s�quence de symboles appartenant � $A$. Cette convention permet de traiter les probabilit�s
        d'entr�e et de sortie d'une s�quence comme des probabilit�s de transitions.    Soit $M_A \subset A^{\N}$ 
        l'ensemble des suites $u=\pa{u_i}_{i \supegal 0}$ de $A$ d�finies comme suit~:
            $$
            u \in M_A \Longleftrightarrow  \left\{
            \begin{array}{l}
            u_1 = e \\
            \exists N > 2 \text{ tel que } \forall i \supegal N, \; u_i = f \text{ et } \forall i < N, \; u_i \neq f
            \end{array}
            \right.
            $$
        Par la suite, $M_A$ sera appel� l'ensemble des mots de $A$. \newline \indexfr{mot}%
        
        Soit $n \supegal 2$, $M_A$ est muni d'une distribution de probabilit� v�rifiant les hypoth�ses, si $u \in M_A$ :
            \begin{eqnarray_xd}
            \pr{u_1 = e} &=& 1  &\numequation\\
            \forall t > 1, \; \pr{u_t = f \; | \; u_{t-1} = f} &=& 1 &\numequation \\
            \forall t > 1, \; \pr{u_t = e } &=& 0  &\numequation \\
            \forall t \supegal n, \; \pr{u_t \; | \; \vecteurno{u_{t-1}}{u_1} } &=& P\pa{u_t \; | \;
                    \vecteurno{u_{t-1}}{u_{t-n+1}} }  &\numequation
            \end{eqnarray_xd}
        \end{xdefinition}

Si les n-grammes sont connus, ces hypoth�ses simplificatrices permettent d'exprimer la probabilit� d'un mot de mani�re
diff�rente~:





        \begin{xproperty}{expression de la probabilit�}\label{n_grammes_propriete_001}%
        Avec les notations de la d�finition~\ref{n_grammes_definition}, soit $u \in M_C$, on d�finit $l\pa{u}$ :
            $$
            l\pa{u} = \min \acc {i \in \N \; | \; u_i = f }
            $$
        Par d�finition de $u$, $l\pa{u}$ existe et la probabilit� de $u$ peut s'exprimer diff�remment :
            $$
            \pr{u} =
                \left\{
                \begin{array}{ll}
                \pr{u} & \text{ si } l\pa{u} < n \\
                \pr{ \vecteurno{u_1}{u_{n-1}} } \; \prody{t=n}{l\pa{u}} \pr{u_t \; | \; \vecteurno{u_{t-1}}{u_{t-n+1}}}  &
                \text{ si } l\pa{u} \supegal n
                \end{array}
                \right.
            $$
        \end{xproperty}






\begin{xdemo}{propri�t�}{\ref{n_grammes_propriete_001}}
La d�finition~\ref{n_grammes_definition} s'inspire de celle d'une cha�ne de Markov d'ordre $n$ (voir
paragraphe~\ref{definition_mmc_ordre_n}, page~\pageref{definition_mmc_ordre_n}), la d�monstration aussi.
\end{xdemo}













%----------------------------------------------------------------------------------------------------------------------
\section{Estimation}
%----------------------------------------------------------------------------------------------------------------------

\indexfrr{n-grammes}{estimation}%

L'estimation des n-grammes s'effectue pour un sous-ensemble $C \subset M_A$ donn�, on d�finit deux types de
probabilit�s~:
\begin{enumerate}
\indexfrr{n-grammes}{probabilit� de commencer}%
\indexfrr{n-grammes}{probabilit� de transiter}%
\item La probabilit� de commencer un mot par la s�quence $x$~:
    $$
    \forall x \in A^n, \; p_e\pa{x,C} = \widehat{P}\pa{u_1^{n-1} = x}
    $$
\item La probabilit� de transiter � l'int�rieur d'un mot de la s�quence $x$ � l'�l�ment $y$~:
    $$
    \forall x \in A^n, \; \forall y \in A, \; \forall t > n, \; p_t\pa{x,y,C} = \widehat{P}\pa{u_t = y \; | \;
    u_{t-n+1}^{t-1} = x }
    $$
\end{enumerate}

Ces deux probabilit�s peuvent �tre estim�es � l'aide de l'ensemble $C$ comme suit :
    \begin{eqnarray*}
    p_e\pa{x,C}     &=&     \dfrac  {  card \acc {u \in C \sachant u_1^{n-1} = x }}   { \card{C}} \\ \\
    p_t\pa{x,y,C}   &=&     \left\{\begin{array}{l}
                            0 \text{ si } card \acc { \; \pa{u,t} \sachant u \in C, \; 
                                            n \leqslant t \leqslant l\pa{u}, \; u_{t-n+1}^{n-1} = x} = 0 \\ \\
                            \text{sinon }
                            \dfrac  {  card \acc { \; \pa{u,t} \sachant u \in C, \; 
                                            n \leqslant t \leqslant l\pa{u}, \; u_{t-n+1}^{n-1} = x, \; u_t = y}}
                                    {  card \acc { \; \pa{u,t} \sachant u \in C, \; n \leqslant t \leqslant 
                                            l\pa{u}, \; u_{t-n+1}^{n-1} = x}}
                            \end{array}
                            \right.
    \end{eqnarray*}















%------------------------------------------------------------------------------------------------------------------
\section{Prolongations}
%------------------------------------------------------------------------------------------------------------------





\subsection{Densit� d'un dictionnaire}
\indexfrr{dictionnaire}{densit�}
\indexfrr{densit�}{dictionnaire}


L'id�e d'associer une densit� � un dictionnaire est tir�e de l'article~\citeindex{Govindaraju2002}. Effectu�e sur une m�me base d'images de mots cursifs, la reconnaissance de l'�criture voit ses performances d�cro�tre au fur et � mesure que la taille du dictionnaire augmente. Le choix est plus vaste, par cons�quent, la possibilit� de se tromper est plus grande. Toutefois, la taille n'est pas le seul param�tre � prendre en compte, un dictionnaire dans les mots sont tr�s proches les uns des autres propose de nombreux choix similaires. Soit $D$ un dictionnaire, la densit� de $D=\vecteur{m_1}{m_N}$ not�e $\rho\pa{D}$ est d�finie par~:

        \begin{eqnarray}
        \rho\pa{D}         &=&    \underset{ v_R\pa{D} } 
                                                {\underbrace{\frac{N\pa{N-1}} { \summyone { i \neq j} \, d_R \pa{m_i,m_j} } } }
                                             \; f_R\pa{N} 
        \end{eqnarray}
        


$f_R\pa{N}$ est une fonction croissante de $N$ telle que $f_R\pa{N} = \pa{\ln N}^p + \delta_R$ ou $f_R\pa{N} = N^p + \delta_R$ avec $p > 0$. $d_R\pa{m_i,m_j}$ est une distance qui mesure la confusion entre ces deux mots via un syst�me de reconnaissance $R$, elle sera d�finie plus loin. L'article~\citeindex{Govindaraju2002} montre de mani�re pratique que les performances $p_R\pa{D}$ du syst�me de reconnaissance $R$ �voluent lin�airement par rapport � la densit�\footnote{Pour une base donn�e, $p_R\pa{D}$ correspond au nombre de mots reconnus correctement sur le nombre de documents dans la base.}~:


        \begin{eqnarray}
        p_R\pa{D} \sim a \rho\pa{D} + b
        \end{eqnarray}


\indexfrr{probabilit�}{�mission}
\indexfr{Kullback-Leiber}

La distance $d_R\pa{w_i,w_j}$ est �gale � la distance entre les deux mod�les de reconnaissance associ�s aux mots $w_i$ et $w_j$. Soient deux �tats $e_i^k$ et $e_j^l$ des mod�les de reconnaissances associ�s aux mots $w_i$ et $w_j$, ils diff�rent par leurs probabilit�s d'�mission que l'on peut comparer gr�ce � une distance de Kullback-Leiber. Il est ensuite possible de construire une distance entre graphes de sorte qu'elle soit la somme des distances d'�ditions entre tous les chemins du premier graphe et tous ceux du second.










\subsection{Classes de symboles}
\indexfrr{n-grammes}{classes de symboles}
\indexfr{cha�ne de Markov}
\indexfr{HMM}\indexfr{MMC}

Plut�t que de mod�liser l'ensemble des n-grammes, il peut para�tre judicieux de regrouper certains symboles en classes puis de ne s'int�resser qu'aux transitions entre classes de symboles, ce que proposent les articles \citeindex{Yamamoto2003} et~\citeindex{Perraud2003}. Jusqu'ici, les n-grammes repr�sent�s sont assimilables � des cha�nes de Markov, mais les classes de symboles pourraient �tre les �tats de la cha�ne de Markov cach�e. Les mots peuvent par exemple �tre class�s par rapport � leur fonction grammaticale dans la phrase, cette classe serait dans le cas pr�sent l'observation cach�e. On peut donc imaginer que les �tats de la cha�ne de Markov repr�sentent des classes de mots et �mettent des mots. Le mod�le ainsi form� est une mod�lisation du langage. Soit $D = \vecteur{m_1}{m_d}$ une liste de symboles ou plus concr�tement de mots, on d�sire mod�liser les s�quences de mots. Les n-grammes des paragraphes pr�c�dents mod�lisent la probabilit� d'un s�quence $S=\vecteur{s_1}{s_T}$ par~:

                $$
                \pr{S} = \pr{ \vecteurno{s_1}{s_d} } \; \prody {i = d+1} {T} \, 
                                    \pr{     s_i \sac \vecteurno{s_{i-1}}{s_{i-d}} }
                $$

En classant les mots dans une liste de classes $\vecteur{C_1}{C_X}$ consid�r�e comme les �tats d'une cha�ne de Markov cach�e, soit $c = \vecteur{c_1}{c_T}$ une s�quence de classes, la probabilit� de la s�quence $S$ s'�crit maintenant~:

                $$
                \begin{array}{l}
                \pr {S} = \summyone{c} \; \cro { \prody{i=1}{T} \pr { s_i \sac c_i }  }
                                                                \pr{ \vecteurno{c_1}{c_d} } \;
                                                                \cro{ \prody {i = d+1} {T}
                                                                \pr{     c_i \sac \vecteurno{c_{i-1}}{c_{i-d}} }
                                                                } \\
                \text{Cette expression est calculable gr�ce � l'algorithme~\ref{hmm_algo_forward} (page~\pageref{hmm_algo_forward}). }
                \end{array}
                $$
                
\indexfr{perplexit�}
\indexfr{entropie}
                
Alors que l'article \citeindex{Perraud2003} �labore les classes de mani�re s�mantique (les mots sont class�s selon leur fonction grammaticale), l'article~\citeindex{Yamamoto2003} propose une m�thode permettant de d�terminer le nombre de classes ainsi qu'un crit�re d'�valuation nomm� \emph{perplexit�} et d�fini comme suit pour une liste de s�quence de symboles ${\vecteur{s_1^k}{s_{T_s}^k}}_{ 1 \leqslant k \leqslant K}$~:

                \begin{eqnarray}
                H &=&  - \frac{1}{K} \; \summy{k=1}{K} \ln \pr{ \vecteurno {s_1^k}{s_{T_s}^k}  } \nonumber\\
                P &=&  2^H  \label{ngram_perplexite}
                \end{eqnarray}

Par rapport � \citeindex{Perraud2003}, l'article \citeindex{Yamamoto2003} propose une mod�lisation plus complexe, alliant probabilit�s de transitions pour les sous-s�quences centrales de symboles et probabilit� de transitions entre classes pour les sous-s�quences au bord. Soit $n$ la dimension des n-grammes et $s = \vecteur{s_1}{s_T}$ une s�quence de symboles dont les classes associ�es sont $\vecteur{c_1}{c_T}$ (les classes sont connues)~:

            \begin{eqnarray*}
                 \pr{  \vecteurno{s_1}{s_d}, \vecteurno{s_{d+1}}{s_{T-d}}, \vecteurno {s_{T-d+1}}{s_T}}  &=&
                 \prody{i=1}{d} \pr { c_i \sac \vecteurno{c_1}{c_i} }  \; \pr{ s_i \sac s_i}  \\
            &&    \prody{i=d+1}{T-d} \pr{ s_i \sac \vecteurno{s_{i-1}}{s_{i_d}} }  \\
            &&    \prody{i=T-d+1}{T} \pr { c_i \sac \vecteurno{c_1}{c_i}  }   \; \pr{ s_i \sac s_i}
            \end{eqnarray*}



Dans cette expression, les d�buts et fin de mots, suppos�s moins fiables pour une estimation, sont mod�lis�s par des classes de caract�res tandis que pour la partie centrale, les caract�res sont directement mod�lis�s.



\subsection{Choix de la dimension de n-grammes}

\indexfr{perplexit�}
\indexfrr{n-grammes}{dimension}

La d�finition de la perplexit� (\ref{ngram_perplexite}) implique n�cessaire sa d�croissance lorsque la dimension $n$ cro�t ainsi que le montre la table~\ref{ngrams_perplexite_dimension} regroupant les calculs de perplexit� pour diff�rentes valeurs de la dimension. Comme dans toute mod�lisation, la question du choix de la dimension appropri�e se pose.


\indexfr{BIC}

A l'instar de l'article \citeindex{Bicego2003}, il est possible d'utiliser un crit�re d'information comme le BIC -~ou Bayesian Information Criterion~- afin de mettre en rapport la baisse de la perplexit� avec le nombre de coefficients ajout�s aux n-grammes lorsqu'on augmente leur dimension. Les notations utilis�es sont celles de l'expression (\ref{ngram_perplexite}). On d�finit $N_k$ comme �tant le nombre de param�tres libres pour le mod�le de dimension $k$ et $S$ repr�sente la somme des longueurs des s�quences d'observations. Le meilleur mod�le maximise le crit�re suivant~:

            \begin{eqnarray}
            BIC\pa{k} &=&     \summy{k=1}{K} \ln \pr{ \vecteurno {s_1^k}{s_{T_s}^k}  }  - \frac{N_k}{2} \ln S
            \end{eqnarray}

La table~\ref{ngrams_perplexite_dimension} montre les r�sultats obtenus pour un dictionnaire de cinq mille mots anglais courants. Le crit�re est maximum pour une dimension �gale � trois. 


                    \begin{table}[ht]
                    $$\begin{array}{|rrr|} \hline
                    \text{dimension} & \log_2 \text{-perplexit� } & \frac{BIC\pa{dimension}}{K} \\ \hline
                     2            &      19,75        &     -20,29        \\ 
                     3            &      16,20        &     -19,64        \\ 
                     4            &      12,23        &     -21,61        \\ 
                     5            &      9,64        &     -24,12        \\ 
                     6            &      8,81        &     -26,47        \\ 
                     7            &      8,60        &     -27,96        \\ 
                    8                 &      8,54        &     -28,69        \\ 
                    9                 &      8,52        &     -28,99        \\ 
                    10             &      8,52        &     -29,14        \\  \hline
                    \end{array}$$
                    \caption{    Log-perplexit� estim�e pour diff�rentes dimensions et sur un dictionnaire de 5000 mots anglais
                                        employ� de mani�re courante et contenant en moyenne entre cinq et six lettres. 
                                        La perplexit� d�cro�t lorsque la dimension augmente tandis que le crit�re $BIC$ pr�conise 
                                        une dimension �gale � trois pour laquelle il est minimum.}
                    \label{ngrams_perplexite_dimension}
                    \indexfr{perplexit�}
                    \end{table}
        
\indexfr{bi-grammes}        
\indexfr{tri-grammes}        

Il est possible de raffiner la m�thode afin de s�lectionner la meilleure dimension locale. Par exemple, dans le cas d'un mot incluant la lettre~"Z", il n'est pas n�cessaire de conna�tre la lettre pr�c�dant la lettre "Z" pour pr�voir celle qui suit. Pour la lettre "Z" les 2-grammes ou bi-grammes suffisent alors qu'avec la lettre "A", il est pr�f�rable de choisir des 3-grammes ou tri-grammes. Il s'agit donc ici d'estimer un mod�le de n-grammes avec un $n$ assez grand puis de supprimer certains coefficients jusqu'� ce que le crit�re $BIC$ ait atteint son minimum. Dans ce cas, les n-grammes peuvent �tre consid�r�s comme les �tats d'une cha�ne de Markov, �tre class�s par ordre d�croissant de probabilit� a posteriori\seeannex{hmm_ditribution_temporelle_etat}{probabilit� des �tats} puis �tre supprim�s selon cet ordre tant que le crit�re $BIC$ cro�t.








\subsection{Groupe de lettres r�currents}
\indexfr{groupe de lettres}
\indexfrr{n-grammes}{groupe de lettres}

Lors du traitement des erreurs de segmentation\seeannex{hmm_bi_lettre}{erreur graph�me}, la reconnaissance de l'�criture n�cessite la s�lection des groupes de lettres les plus fr�quents. Si le "." signifie le d�but ou la fin d'un mot ou l'espace, ".a.", ".de.", "tion." reviennent fr�quemment dans la langue fran�aise. On s'int�resse ici � des probabilit�s de transition entre des groupes de plusieurs lettres. Jusqu'� pr�sent, les mod�les de n-grammes pr�sent�s estime la probabilit� de la lettre suivant sachant la ou les lettres pr�c�dentes. Dans ce cas, on cherche la probabilit� des lettres suivantes sachant un pass� d'une ou plusieurs lettres.

        
\indexfr{Ast�rix le Gaulois}

La table~\ref{ngrams_asterix_gaulois} pr�sente un extrait des noms utilis�s dans la bande dessin�e \textit{Ast�rix le Gaulois} dans laquelle les suffixes $ix.$ et $us.$ sont couramment employ�s. Il est naturel d'envisager la probabilit� de ces triplets -~deux lettres plus la fin du mot~- sachant la lettre pr�c�dente. Il reste � estimer des probabilit�s comme $\pr{ u \sac t}$ et $ \pr{ us. \sac t }$. 

        \begin{table}[ht]
        $$\begin{tabular}{|l|l|l|l|} \hline
            Ast�rix                     & Ob�lix                         & Panoramix                     & Abraracourcix         \\
            Assurancetourix     & Ag�canonix                & Tragicomix                    & C�tautomatix            \\
            Id�fix                        & Plaintecontrix        & Ordralfab�tix                & Pneumatix                    \\
            Plantaquatix            & El�vedelix                & Analg�six                        & Monosyllabix            \\
            Uniprix                        & Linguistix                & Arrierboutix                & Ob�lodalix                \\
            Harenbaltix                & Choucroutgarnix        & Bellodalix                    & Z�roz�rosix             \\
            All�gorix                    & Boulimix                    & Porqu�pix                        & Aplusb�galix      \\
            Th�orix                      & Hom�opatix                & Tournedix                     & Squinotix              \\ \hline
            %
            Cumulonimbus             & Pleindastus                & Fleurdelotus                & Lang�lus                     \\
            Yenapus                        & Roideprus                    & Fanfrelus                        & Faipalgugus                \\
            D�tritus                    & Diplodocus                & Garovirus                        & Cubitus                     \\
            Diplodocus                & Infarctus                    & Suelburnus                    & Saugrenus                    \\
            Volfgangamad�us        & Soutienmordicus        & �pinedecactus                & C�tinconsensus        \\ \hline
        \end{tabular}$$
        \caption{ Pr�noms gaulois et romains extraits de la bande dessin�e 
                            \textit{Ast�rix le Gaulois}. Pour cet extrait, $\pr{ ix. \sac t} = \frac{3}{10}$, 
                            $\pr{ us. \sac t} = \frac{2}{10}$.}
        \label{ngrams_asterix_gaulois}
        \end{table}
                        
\indexfr{alphabet �tendu}
\indexfr{relation d'ordre partiel}

Pour ce faire, on d�finit un alphabet �tendu $A = \vecteur{s_1}{s_N}$ incluant tous les groupes de lettres dont on veut estimer les transitions. On d�finit �galement $s + t$ comme �tant la concat�nation de deux sympboles de l'alphabet, par exemple~: $t + us = tus$. Pour un mot donn�, on d�signe par $E_A\pa{m}$ toutes les mani�res possibles d'�crire le mot $m$ en utilisant les symboles inclus dans~$A$. On dispose d'une base de mots $\vecteur{m_1}{m_K}$. Les probabilit�s de transitions sont alors d�finies par~:

        \begin{eqnarray}
        N                        &=& \summy{k=1}{K} \card{ E_A\pa{m_k}} \nonumber \\
        \pr{ s }        &=&    \frac{1}{N} \;  \summy{k=1}{K} \;     \cro{ \summyone{ \vecteur{s_1}{s_n} \in E_A\pa{m_k} } \;  
                                                                                    \indicatrice{s_1 = s }} \\
        \pr{ t \sac s }        &=&    \frac{   \summy{k=1}{K} \;     \cro{ \summyone{ \vecteur{s_1}{s_n} \in E_A\pa{m_k} } \;  
                                                                                        \summy{i = 2}{n} \indicatrice{s_{i-1} = s \text{ et } s_i = t  }
                                                                                        }}
                                                             {
                                                                  \summy{k=1}{K} \;     \cro{ \summyone{ \vecteur{s_1}{s_n} \in E_A\pa{m_k} } \;  
                                                                                        \summy{i = 2}{n} \indicatrice{s_{i-1} = s }
                                                                                        }
                                                             }
        \end{eqnarray}

Cet ensemble n'est pas forc�ment r�duit � un seul �l�ment (voir table~\ref{ngrams_boa}). Avec ce formalisme, il est maintenant possible d'exprimer la probabilit� d'un mot $m$ comme �tant~:

        \begin{eqnarray}
        \pr{ m } = \summyone{ \vecteur{s_1}{s_n}  \in E_A\pa{m} } \;  \pr{ s_1} \, \prody{i = 2}{n} \, \pr{ s_i \sac s_{i-1} }
        \end{eqnarray}


        \begin{table}[ht]
        $$\begin{tabular}{|l|l|} \hline
        alphabet         & B - BO - O - OA - A     \\  \hline
        mot                     & BOA                            \\ \hline
        �criture 1    & B - OA                        \\ 
        �criture 2    & BO - A                        \\ 
        �criture 3    & B - O - A                    \\ \hline
        \end{tabular}$$
        \caption{     Diff�rentes mani�res d'�crire le mot "BOA". }
        \label{ngrams_boa}
        \end{table}

\indexfrr{segmentation}{graph�me}
\indexfrr{graph�me}{segmentation}

Cet outil permet d'estimer des probabilit�s de transitions entre des mod�les de Markoc cach�es mod�lisant des groupes de lettres\seeannex{hmm_seq_modele_mot}{groupes de lettres} pr�sent�s au paragraphes~\ref{hmm_bi_lettre} (page~\pageref{hmm_bi_lettre}). L'ensemble $E_A\pa{m}$ n'est ici pas pr�cis� et est suppos� �tre l'ensemble des �critures possibles et admises par l'aphabet $A$. Cependant, pour la reconnaissance de l'�criture, toutes les �critures ne sont pas �quiprobables puisqu'une �criture est d�finie comme �tant le r�sultat de la segmentation en graph�mes dont les erreurs (voir figure~\ref{image_grapheme_erreur}, page~\pageref{image_grapheme_erreur}) d�terminent l'ensemble $E_A\pa{m}$.






\subsection{Lissage des n-grammes}
\label{ngram_lissage_nnnn}
\indexfrr{lissage}{n-grammes}
\indexfrr{n-grammes}{lissage}

L'article \citeindex{B�chet2004} propose une m�thode permettant de lisser des probabilit�s de transitions entre mots et d'obtenir par exemple des probabilit�s non nulles de transitions entre couples de caract�res non repr�sent�s dans l'ensemble d'estimation. A partir des nombres de transitions $c_{ij}$ d'un contexte $h_i$ (un ou plusieurs mots) vers un mot $w_j$, les auteurs construisent un espace vectoriel $E_C$ de repr�sentation des contextes. L'objectif est de construire des compteurs de transitions augment�s $a_{ij}$ prenant en compte non seulement les transitions du contexte $h_i$ vers le mot $w_j$ mais aussi les transitions des contextes proches de $h_i$ vers un mot proche de $w_j$, la proximit� �tant mesur�e dans l'espace $E_C$ par une distance. L'article montre empiriquement que les performances dans une t�che de reconnaissance sont d'autant plus accrues par un tel lissage que la taille du vocabulaire est grande.





\firstpassagedo{
    \begin{thebibliography}{99}
    \input{ngrams_biblio.tex}
    \end{thebibliography}
    \input{../xthese/nb_citations.tex}
}



\input{../../common/livre_table_end.tex}
\input{../../common/livre_end.tex}



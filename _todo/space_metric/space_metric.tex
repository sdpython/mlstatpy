\input{../../common/livre_begin.tex}%
\firstpassagedo{\input{space_metric_titre.tex}}
\input{../../common/livre_table_begin.tex}%
\firstpassagedo{\input{space_metric_chapter.tex}}



\newcommand{\initialisation}[0]{initialisation : \\}
\newcommand{\step}[0]{body : \newline}
\newcommand{\terminaison}[0]{termination : \\}
\newcommand{\for}[2]{for $#1$ to $#2$ do}
\newcommand{\forend}[0]{end for \\}
\newcommand{\while}[1]{while $\pa { #1 }$ do\\}
\newcommand{\whilenotl}[1]{while $\pa { #1 }$ do}
\newcommand{\whileend}[0]{end while}

\newenvironment{algopar}{}{}
\newcommand{\ind}[0]{\null \quad}

\label{space_metric_introduction}


\sloppy


Chercher des mots identiques ou similaires dans un dictionnaire est un probl�me classique et peut �tre d�fini pour tout espace m�trique. \indexfr{dictionnaire}\indexfr{lexique}\indexfr{plus proches voisins}\indexfrr{distance}{�dition} Soit $\pa{E,d}$ un espace m�trique quelconque et $D \subset E$ un ensemble fini quelconque, $x \in E$ est un �l�ment de $E$ et $s \in \mathbb{R}_+$ un r�el positif. L'objectif est de trouver le sous-ensemble $D'\pa{x,s} \subset D$ des voisins les plus proches de $x$ tels que~:

    $$
    D'\pa{x,s} = \acc{  y \in D \sac d\pa{x,y} \infegal s}
    $$

Afin de d�terminer les voisins de $x$, une m�thode simple consiste � estimer toutes les distances entre $x$ et les �l�ments de $D$. Le co�t de cette m�thode est proportionnel au nombre d'�l�ments de $D$ et � la complexit� du calcul de la distance. On distingue g�n�ralement deux directions afin d'am�liorer la rapidit� des algorithmes de recherche~:

    \begin{enumerate}
    \item L'optimisation du calcul de la distance.
    \item L'optimisation de la recherche des voisins.
    \end{enumerate}


Les m�thodes pr�sent�es dans ce chapitre concerne la seconde direction, plus g�n�rale que la premi�re.








%-------------------------------------------------------------------------------------------------------------
\section{Classification ascendante hi�rarchique}
%-------------------------------------------------------------------------------------------------------------

Cette m�thode reprend celle d�crite dans l'article \citeindex{Dupr�2003}.


\subsection{Arbre de partitionnement}
\label{section_partitionning_tree}
\indexfr{partitionnement}\indexfrr{arbre}{partitionnement}

L'objectif de cette partie est de construire un arbre de partitionnement qui sera utilis� ensuite afin d'am�liorer la recherche des plus proches voisins au paragraphe~\ref{section_optimisation_distance}.


            \begin{xdefinition}{rayon et centre d'un ensemble discret}
            \indexfr{centre}\indexfr{rayon}
            \label{definition_center_radius}%
            Soit $D = \vecteur{y_1}{y_N} \subset E$ un ensemble fini de $E$, le centre $C\pa{D}$ de $D$ 
            est d�fini par~:
                $$
                C\pa{D} \in \underset {x \in D} {\arg \min} \cro{  \underset{y \in D} {\max} \; d\pa{x,y}}
                $$
            o� $d\pa{x,y}$ est la distance entre les �l�ments $x$ et $y$. On d�finit aussi le rayon $R\pa{D}$ 
            de $D$ par~:
                $$
                R\pa{D} = \underset{x \in D} {\max} \; d\pa{C\pa{D},y}
                $$
            \end{xdefinition}



\begin{xremark}{cas particulier}
Si $A,B \subset D$ et $A \subset B$, cela n'implique pas que $R\pa{A} \infegal R\pa{B}$ comme le montre la
figure~\ref{figure_partition_inclusion} o� $$R\pa{A} = d\pa{x,y} > d\pa{x,z} = R\pa{B}$$.
\end{xremark}



        \begin{figure}[ht]
    \[
    \unitlength 1mm
    \fbox{
    \filefig{../space_metric/fig_ray}
    }
    \]
    \caption{Exemple o� ajouter un �l�ment � un sous-ensemble aboutit � une r�duction du rayon.}
    \label{figure_partition_inclusion}
        \end{figure}



        \begin{xproperty}{rayon d'un couple d'�l�ments}\label{property_001}%
        Soit $\pa{x,y} \in E^2$ deux �l�ments de $E$, alors $R\acc{x,y} = d\pa{x,y}$.\\
        \end{xproperty}

L'algorithme~\ref{algorithm_AHC}\footnote{
Autre formulation~:

            \begin{xalgorithm}{Arbre de partionnement}
            \label{algorithm_AHC_prime}%
            Soit $D= \vecteur{y_1}{y_N}$ un ensemble fini de $\pa{E,d}$. Soit $N\pa{n_1,n_2,C,R}$ un n\oe ud 
            li� � ses deux pr�c�desseurs $n_1,n_2$ et qui d�finit une partie dont le centre est $C$ et le rayon $R$. 
            $L$ repr�sente un ensemble de n\oe uds. Si $x$ est un n\oe ud, $P\pa{x}$ d�signe la partie r�union 
            de tous les centres des anc�tres de $x$.
            
            \begin{xalgostep}{initialisation}
                Pour t$ y \in D$, on ajoute le n\oe ud$N\pa{ \emptyset,\emptyset,y,0}$ � $L$.
            \end{xalgostep}
                
            \begin{xalgostep}{recherche de la meilleure r�union}\label{space_metric_step_cah_cah_2}
                Soit $\pa{x,y} \in \underset{ x,y \in L, \, x \neq y }
                                                                                {\arg \min} \; R\pa{ P\pa{x} \cup P\pa{y} }$.
                Le n\oe ud \\
                                                    $    z =         N\pa{    x,y, 
                                                                    C\pa{ P\pa{x} \cup P\pa{y} } , 
                                                                    R\pa{ P\pa{x} \cup P\pa{y} } }$ est cr�� et 
                $L \longleftarrow L \cup {z} - \acc{x,y}$
            \end{xalgostep}
                
            \begin{xalgostep}{terminaison}
                    Si $L$ contient plus d'un n\oe ud, retour � l'�tape~\ref{space_metric_step_cah_cah_2}.
            \end{xalgostep}
            \end{xalgorithm}
} est bas� sur une classification ascendante hi�rarchique (voir \citeindex{Saporta1990}, \citeindex{Reinert1979}),\indexfr{CAH} il construit une hi�rarchie de partitions d�crite par un graphe. \indexfr{n\oe ud}





            \begin{xalgorithm}{classification ascendante hi�rarchique}\label{algorithm_AHC}%
            Soit $D= \vecteur{y_1}{y_N}$ un sous-ensemble fini de $\pa{E,d}$. On note $P_n$
            une partie contenant les �l�ments $P_n = \acc{p_{n,1} \; , ... , \; p_{n, \cro{card\pa{D}-n+1}}}$.
            L'algorithme a pour but de construire la suite de partitions $\pa{P_n}_{n \supegal 1}$ comme suit~:
            
            \begin{xalgostep}{initialisation}
                $n = 1$ et $P_1$ est la partition o� chaque �l�ment de $D$ est un �l�ment de $P_1$
            \end{xalgostep}
            
            \begin{xalgostep}{r�currence}\label{space_cah_algo_step}
                \begin{xfor}{n}{1}{N-1}
                    soit $\pa{i^*_n,j^*_n} \in \underset{ i \neq j }{\arg \min} \; R\pa{p_{ni} \cup p_{nj} }$, alors
                    $P_{n+1} = \acc{ \pa{p_{nk}}_{k \neq i^*_n,j^*_n}, p_{ni^*} \cup p_{nj^*} }$
                \end{xfor}
            \end{xalgostep}
            
            \end{xalgorithm}



La suite $\pa{P_n}_{1 \infegal n \infegal N}$ d�finit un graphe d'inclusion illustr� par la
figure~\ref{figure_partition_inclusion_graph}) pour cinq �l�ments.\\









\begin{xremark}{plusieurs minima}
L'�tape~\ref{space_cah_algo_step} impose de choisir un �l�ment dans un ensemble de minima. Lorsque celui-ci contient plus d'un �l�ment, une r�gle simple consiste � choisir le plus petit regroupement de deux parties parmi celles de rayon minimum. Cette r�gle a peu d'influence lorsque la construction de l'arbre s'effectue dans un espace continu. En revanche, pour un espace de mot muni d'une distance d'�dition comme celle de Levenstein (voir~\citeindex{Levenstein1966}), ce cas se produit fr�quemment puisque la distance est � valeurs enti�res. Cette r�gle accro�t sensiblement les performances.
\end{xremark}



Chaque n\oe ud du graphe obtenu avec l'algorithme~\ref{algorithm_AHC} satisfait les conditions suivantes~:

\begin{enumerate}
\item Il n'a aucun pr�d�cesseur\indexfr{pr�d�cesseur} et la partie d�sign�e par ce n\oe ud est un
        singleton\indexfr{singleton} dont le rayon est nul. \indexfr{rayon}
\item Il a deux pr�d�cesseurs et la partie point�e par ce n\oe ud contient plus d'un �l�ment, son rayon est strictement positif si au moins deux �l�ments sont diff�rents.
\item Il n'a aucun successeur\indexfr{successeur} et la partie que le n\oe ud d�signe est le sous-ensemble $D$.
\end{enumerate}


        \begin{figure}[ht]
    \[\frame{
    \filefig{../space_metric/fig_cah}
    }\]
    \caption{Graphe d'inclusion, la premi�re partition contient une partie par �l�ment de $D$ (les
                feuilles)\indexfr{feuille}, la seconde partition regroupe ensemble les plus 
                proches �l�ments,
                la troisi�me partition regroupe ensemble les couples d'�l�ments les plus proches, 
                la quatri�me partition
                construit la partition de rayon minimum, le choix est entre
                $\acc{x_1,x_2,x_3}$, $\acc{x_1,x_4,x_5}$, $\acc{x_2,x_3,x_4,x_5}$, la cinqui�me partition est
                l'ensemble $D$ complet.}
    \label{figure_partition_inclusion_graph}
        \end{figure}



            \begin{xproperty}{nombre de n\oe uds}\label{property_node}
            L'arbre construit par l'algorithme~\ref{algorithm_AHC} contient exactement $2N-1 = 2 * card\pa{D}-1$ 
            n\oe uds.
            \end{xproperty}




\begin{xdemo}{propri�t�}{\ref{property_node}}
L'arbre construit par l'algorithme~\ref{algorithm_AHC} contient $N$ n\oe uds sans pr�d�cesseur,
\indexfr{pr�d�cesseur} soit un n\oe ud par mot de $D$. A chaque it�ration, un n\oe ud est cr�� pour
assembler deux parties entre elles. $N-1$ it�rations sont n�cessaires pour passer de $N$ parties � une seule. Donc, le nombre de n\oe ud de l'arbre est~:
    $$ 
    N + \pa{N-1} = 2N-1
    $$
\end{xdemo}


Le th�or�me~\ref{theorem_hierarchy} montre que l'arbre construit par l'algorithme~\ref{algorithm_AHC} peut �tre
consid�r� comme une hi�rarchie pour un certain indice d�fini par le th�or�me.


            \begin{xtheorem}{hi�rarchie}\label{theorem_hierarchy}
            \indexfr{hi�rarchie}
            Soit $\pa{P_n}_{1 \infegal n \infegal N}$ la suite construite par l'algorithme~\ref{algorithm_AHC}. 
            Soit $i\pa{P_n}$ d�fini par~:
                $$
                i\pa{P_n} = \underset{p \in P_n} {\max} \, R\pa{p}
                $$
            Alors, la suite $\pa{i\pa{P_n}}_{1 \infegal n \infegal N}$ est croissante.
            \indexfrr{suite}{croissante}
            \end{xtheorem}





\begin{xdemo}{th�or�me}{\ref{theorem_hierarchy}}

Afin que la d�monstration soit plus claire, les partitions not�es~:
    $$
    \pa{P_{ni}}_{  \begin{subarray}{l} 1 \infegal n \infegal N \\ 1 \infegal i \infegal N-n+1 \end{subarray}  }%
    \text{ avec } \forall \pa{n,i}, \; P_{ni} \neq \emptyset
    $$
sont maintenant not�es~:
    $$
    \pa{P_{ni}}_{\begin{subarray}{l} 1 \infegal n \infegal N \\ 1 \infegal i \infegal N \end{subarray}} %
    \text{ avec } \forall n, \; card \acc{ i | P_{ni} = \emptyset } = N-n+1
    $$

Donc~:
    \begin{eqnarray*}
    P_1 = \vecteur{P_1^1}{P_1^N}        &=&     \vecteur{\acc{y_1}}{\acc{y_N}} \text{ et }
    P_k                     =     \vecteur{P_k^1}{P_k^N}
    \end{eqnarray*}

Selon l'algorithme~\ref{algorithm_AHC}, $\exists \pa{a_{k+1},b_{k+1}} \in \intervalle{1}{N}^2$ tel que $a_{k+1}                 <         b_{k+1}$ et~:
        $$
    \begin{array}{l}
    P_{k+1}^{a_{k+1}}       =         P_{k}^{a_{k+1}} \cup P_{k}^{b_{k+1}} \; , \quad 
    P_{k+1}^{b_{k+1}}       =         \emptyset  \; , \quad  
    P_k^{a_{k+1}}           \neq      \emptyset  \; , \quad  
    P_k^{b_{k+1}}           \neq      \emptyset \\
    \text{ et } \forall i \notin \acc{a_{k+1},b_{k+1}}, \, P_{k+1}^{i} = P_k^i
    \end{array}
    $$

Soit $R_k^i = R \pa{P_k^i}$ et $C_k^i = C \pa{P^k_i}$. On impose aussi que si $P^k_i = \emptyset$ alors $R_k^i = 0$. Si l'assertion (\ref{equation_1}) est vraie alors le th�or�me~\ref{theorem_hierarchy} le sera aussi~:
    \begin{eqnarray}
    \forall k \in \intervalle{1}{N-1}, \, \forall i \in \intervalle{1}{N}, \, R_{k+1}^{a_{k+1}} \supegal R_{k}^i
    \label{equation_1}
    \end{eqnarray}

Cette d�monstratin s'effectue par r�currence. L'assertion (\ref{equation_1}) est vraie de mani�re �vidente pour $k=2$ puisque~:

    $$
    \forall i \in \intervalle{1}{N}, \, R_{1}^i = 0
    $$

Donc, on suppose (\ref{equation_1}) est pour tout $k < N$, on cherche � montrer que (\ref{equation_1}) est aussi vraie pour $k+1$.


\itemdemo
\quad 
\para{$1^\circ$ cas : $\acc{a_k,b_k} \bigcap \acc{a_{k+1},b_{k+1}} = \emptyset$}

On a :

    $$
    \acc{a_k,b_k} \bigcap \acc{a_{k+1},b_{k+1}} = \emptyset \Longrightarrow P_{k+1}^{a_{k}} = P_{k}^{a_{k}}
    $$

Donc :

    \begin{eqnarray}
    \forall i \in \intervalle{1}{N}, \, && 
                             i \notin \acc{a_k,b_k} \Longrightarrow R_k^{a_k} \supegal R_{k-1}^{i} = R_{k}^{i} 
                              \label{equation_2}
    \end{eqnarray}

Mais l'algorithme~\ref{theorem_hierarchy} implique que~:

    \begin{eqnarray*}
    \acc{a_k,b_k} &\in& \underset{  \begin{subarray}{c} i < j \\ P_{k-1}^i \neq \emptyset \\ P_{k-1}^j \neq
    \emptyset\end{subarray} }
            {\arg \min} \, R\pa{P_{k-1}^i \bigcup P_{k-1}^j} \text{ et }
    \acc{a_{k+1},b_{k+1}} \in \underset{  \begin{subarray}{c} i < j \\ P_{k}^i \neq \emptyset \\ P_{k}^j \neq
    \emptyset\end{subarray} }
            {\arg \min} \, R\pa{P_{k}^i \bigcup P_{k}^j}
    \end{eqnarray*}

Par cons�quent, $\acc{a_k,b_k} \bigcap \acc{a_{k+1},b_{k+1}} = \emptyset$ implique que~:

    \begin{eqnarray*}
    \acc{a_{k+1},b_{k+1}} \in \underset{  \begin{subarray}{c} i < j \\ i \neq a_k, j\neq b_k\\
        P_{k}^i \neq \emptyset \\ P_{k}^j \neq \emptyset\end{subarray} }
        {\arg \min} \, R\pa{P_{k}^i \bigcup P_{k}^j}   
    &\Longrightarrow& \acc{a_{k+1},b_{k+1}} \in \underset{  \begin{subarray}{c} i < j \\ i \neq a_k, j\neq b_k\\
        P_{k-1}^i \neq \emptyset \\ P_{k-1}^j \neq \emptyset\end{subarray} }
        {\arg \min} \, R\pa{P_{k-1}^i \bigcup P_{k-1}^j}\\
    &\Longrightarrow& R\pa{P_k^{a_{k-1}} \bigcup P_k^{b_{k-1}}}   \infegal R\pa{P_k^{a_{k+1}} \bigcup P_k^{b_{k+1}}} \\
    &\Longrightarrow& R\pa{ P_k^{a_{k}} }   \infegal R\pa{ P_{k+1}^{a_{k+1}} }
    \end{eqnarray*}

D'o�~:
    \begin{eqnarray}
    \forall i \in \intervalle{1}{N}, \, R_{k+1}^{a_k} \supegal R_{k}^{i} \label{equation_reca_1}
    \end{eqnarray}




\itemdemo
\quad 
\para{$2^\circ$ cas : $\acc{a_k,b_k} \bigcap \acc{a_{k+1},b_{k+1}} \neq \emptyset$} 

Si $\acc{a_k,b_k} \bigcap \acc{a_{k+1},b_{k+1}} \neq \emptyset$, alors $a_{k+1} = a_k$ or $b_{k+1} = a_k$. Pour ces deux cas, la preuve est la m�me donc on suppose que $a_{k+1} = a_k$, alors :
    $$
    \forall i \in \intervalle{1}{N}, \, R_k^{a_k} \supegal R_{k}^{i} \quad \text{ (voir (\ref{equation_2}))}
    $$

Comme $a_{k+1} = a_k$, il suffit de prouver que~:
    $$
    R_{k+1}^{a_{k}} \supegal R_{k}^{a_{k}}
    $$

Mais :

    \begin{eqnarray*}
    P_{k+1}^{a_{k+1}}   &=&     P_{k}^{a_{k+1}} \bigcup P_{k}^{b_{k+1}}
                        =     P_{k-1}^{a_{k}} \bigcup P_{k-1}^{b_{k}} \bigcup P_{k}^{b_{k+1}}
    \end{eqnarray*}

Deux cas sont possibles~:

\begin{enumerate}
\item si $C_{k+1}^{a_k} \in P_{k}^{a_{k}}$, alors :
    \begin{eqnarray}
    R_k^{a_k}   &=&         \underset{y \in P_{k}^{a_{k}}}{\max} \, d\pa{C_{k}^{a_k},y} %\nonumber\\
                \infegal  \underset{y \in P_{k}^{a_{k}}}{\max} \, d\pa{C_{k+1}^{a_k},y} 
                \infegal  \underset{y \in P_{k+1}^{a_{k}}}{\max} \, d\pa{C_{k+1}^{a_k},y} %\nonumber\\
                \infegal  R_{k+1}^{a_k} \label{equation_reca_2}
    \end{eqnarray}
\item si $C_{k+1}^{a_k} \in P_{k}^{b_{k+1}}$, alors l'algorithm~\ref{algorithm_AHC} implique que~:
    \begin{eqnarray}
    R_{k+1}^{a_k}   &=&         \max \Bigg\{  
                                                            \underset{y \in P_{k}^{a_{k}}}{\max} \, d\pa{C_{k+1}^{a_k},y} , %\nonumber\\
                    %&& \quad \quad \quad \quad
                                \underset{y \in P_{k}^{b_{k+1}}}{\max} \, d\pa{C_{k+1}^{a_k},y} 
                                \Bigg \}\nonumber\\
                    &\supegal&  \max \acc{  \underset{y \in P_{k}^{a_{k}}}{\max} \, 
                                                                d\pa{C_{k+1}^{a_k},y} , R_k^{b_{k+1}} }\nonumber\\
                    &\supegal&  \max \Bigg\{  \underset{y \in P_{k-1}^{a_{k}}}{\max} \, 
                                                                d\pa{C_{k+1}^{a_k},y} , 
                                            \underset{y \in P_{k-1}^{b_{k}}}{\max} \, d\pa{C_{k+1}^{a_k},y} ,
                                            R_k^{b_{k+1}} \Bigg\}\nonumber\\
                    &\supegal&  \max \acc{  R_k^{a_{k}}  , R_k^{b_{k}} , R_k^{b_{k+1}} }\nonumber\\
                    &\supegal&  R_k^{a_{k}}  \label{equation_reca_3}
    \end{eqnarray}
\end{enumerate}


En conclusion, la r�currence est d�montr�e par les in�galit�s (\ref{equation_reca_1}), (\ref{equation_reca_2}),
(\ref{equation_reca_3}). Par cons�quent, la suite d'indices $\pa{i\pa{P_n}}_{1 \infegal n \infegal N}$ est croissante.


\end{xdemo}






            \begin{xcorollary}{majoration du rayon}\label{corollary_AHC}%
            Soit $D$ un sous-ensemble fini de $E$ et $A$ l'arbre obtenu gr�ce � l'algorithme~\ref{algorithm_AHC}, 
            soit $n$ un n\oe ud quelconque de cet arbre, alors~:\indexfr{successeur}
                \begin{eqnarray}
                \textnormal{le successeur } s\pa{n} \text { de } n\; { existe} 
                \Longrightarrow R\pa{n} \infegal R\pa{s\pa{n}}
                \end{eqnarray}
            \end{xcorollary}


Finalement, si $p_1$ et $p_2$ sont deux partitions de l'arbre de partitionnement, et $p_1 \subset p_2$, alors
$R\pa{p_1} \infegal R\pa{p_2}$. Cette conclusion n'�tait pas �vidente d'apr�s le cas particulier de la figure~\ref{figure_partition_inclusion}. 





L'objectif de cet algorithme est de grouper ensemble les �l�ments ou les parties les plus proches � chaque it�ration. La cons�quence attendue est que le voisinage d'un mot soit concentr� dans une branche de l'arbre. N�anmoins, le principal inconv�nient de cet algorithme est son co�t. Si on suppose que le co�t de la distance est approch�e par une constante $c$ et que l'ensemble � hi�rarchiser contient $n$~�l�ments, le co�t de l'algorithme est en $O\pa{c\,n^5}$. Ce co�t peut �tre r�duit en factorisant les calculs d'une it�ration � l'autre puisque la liste $L$ conserve $n-k-1$ n\oe uds inchang�s. Cette remarque permet de ne calculer le centre et le rayon que pour les parties nouvellement cr��es. De la m�me mani�re, il est possible de conserver pour chaque n\oe ud, le meilleur voisin qui, � l'it�ration suivante, peut �tre rest� le m�me ou �tre la partie qui vient d'�tre r�unie. Finalement, il est possible de faire descendre le co�t de l'algorithme � $O\pa{c \, n^3}$.

Toutefois, pour des ensembles de plusieurs milliers d'�l�ments, l'algorithme~\ref{algorithm_AHC} demeure tr�s long. Une optimisation consiste � adapter l'algorithme des centres mobiles\seeannex{emission_continue_centre_mobile}{centres mobiles}. L'algorithme s'inspire de l'algorithme~\ref{algo_centre_mobile} en rempla�ant toutefois la notion de barycentre d'une partie par son centre (d�finition~\ref{definition_center_radius}) et l'inertie d'une partie par son rayon.


            






\subsection{Ajouter un �l�ment au graphe}
\label{section_alternative}

L'algorithme~\ref{algorithm_AHC} ne permet d'ins�rer de nouveaux n\oe uds une fois que celui-ci est construit, inconv�nient auquel rem�die l'algorithme~\ref{algorithm_insertion}. Ce dernier ajoute des n\oe uds au graphe de partitionnement et pourrait �tre �galement utilis� pour construire l'arbre entier en ins�rant un � un tous les �l�ments de $D$ mais cette m�thode est moins efficace.



        \begin{xalgorithm}{insertion d'un n\oe ud}\label{algorithm_insertion}
        
        Soit $D$ un sous-ensemble fini de $E$ et $A$ un arbre binaire, soit $n$ un n\oe quelconque de $A$, alors~:
            \begin{enumerate}
            \item $n$ d�finit une partie not�e $P\pa{n}$ dont le rayon est $R\pa{n}$ et 
                            dont le centre est $C\pa{n}$.
            \item $n$ n'a pas de pr�d�cesseur\indexfr{pr�d�cesseur} si $P\pa{n}$ est 
                            un singleton ou deux predecessors
                    sinon. Dans ce second cas, $P\pa{n}$ est la r�union des parties de deux pr�d�cesseurs~:
                     $Pr\pa{n} = \acc{ p_1\pa{n} , p_2\pa{n}}$
            \item $n$ n'a pas de successeur\indexfr{successeur} et il d�finit la partiet $D$ 
                                    ou un successeur not� $Su\pa{n} = s\pa{n}$
            \end{enumerate}
        
        Le seul n\oe ud sans successeur est appel� \emph{racine}.\indexfr{racine} et not� $r$. 
        Soit $m$ un �l�ment et $x$
        le n\oe ud associ�, alors $P\pa{x} = \acc{m}$, $C\pa{x} = m$, $R\pa{x} = 0$, et $x$ n'a 
        pour le moment aucun succeseur
        et aucun pr�d�cesseur. $N$ d�finit un ensemble de n\oe ud, le n\oe ud $x$ est ins�r� dans l'arbre $A$ 
        selon les r�gles suivantes~:
        
        \begin{xalgostep}{intialisation}
            $N \longleftarrow \acc{r}$
        \end{xalgostep}
        
        %\possiblecut
        
        \begin{xalgostep}{insertion}
            \begin{xwhile}{non fin}
        
                \begin{xif}{$
                            \begin{array}{l} d\pa{m,C\pa{n}} > R\pa{n}
                            \hspace{10cm}
                            \refstepcounter{equation}(\theequation)
                            \label{amelioration_insertion_1}
                            \end{array}
                            $}
        
                    \begin{enumerate}
                    \item soit $ n\in \arg \min \acc{ d\pa{m,C\pa{n'}} \;|\; n' \in N}$, le n\oe ud $y$ est cr��
                    \item le successeur de $y$ devient : $s\pa{y} \longleftarrow s\pa{n}$
                    \item le pr�d�cesseur de $y$ devient : $p_1\pa{y} \longleftarrow x$ 
                                                        and $p_2\pa{y} \longleftarrow n$
                    \item si le successeur de $n$ existe alors :
                        $$
                        \exists i \in \acc{1,2} \text{ tel que } p_i\pa{s\pa{n}} = n \text{ et } 
                                            p_i\pa{s\pa{n}} \longleftarrow  p_i\pa{s\pa{n}} = y
                        $$
                    \item le successeur de $n$ devient : $s\pa{n} \longleftarrow y$
                    \item le successeur de $x$ devient : $s\pa{x} \longleftarrow y$
                    \item si $r = n$, alors $r \longleftarrow y$
                    \item le centre et le rayon sont reestim�s pour toutes les parties d�finies par
                            les �l�ments $\pa{n_1,...}$ de la suite the serie :
                                    $$
                                    n_0 = n \textnormal{ et pour } k \supegal 0, \;
                                                        n_{k+1} = \left\{
                                                                    \begin{array}{l}
                                                                    s\pa{n_k} \textnormal{ si } r \neq n_k \\
                                                                    r \textnormal{ sinon }
                                                                    \end{array}
                                                                    \right.
                                    $$
                    \item fin
                    \end{enumerate}
        
                \xelse
                    $$
                    \begin{array}{lr}
                    \exists n \in N \text{ tel que } d\pa{m,C\pa{n}} \infegal R\pa{n} &
                                                \hspace{6.5cm}\refstepcounter{equation}(\theequation)
                                                 \label{amelioration_insertion_2} \\
                    \text{et } N \longleftarrow \acc{N - \acc{n} } \cup \acc{p_1\pa{n},p_2\pa{n}} &
                    \end{array}
                    $$
                \end{xif}
            \end{xwhile}
        \end{xalgostep}
        \end{xalgorithm}


\begin{xremark}{hi�rarchie}
Il n'est pas d�montr� que l'arbre obtenu apr�s une ou plusieurs applications de l'algorithme~\ref{algorithm_insertion} v�rifie le corollaire~\ref{corollary_AHC}. \indexfr{hi�rarchie}
\end{xremark}



        \begin{figure}[ht]
    \[
    \begin{tabular}{|c|c|}
        \hline
        \filefig{../space_metric/fig_try1}
        &
        \filefig{../space_metric/fig_try2}
        \\
        \hline
        insertion de $x,y,z$ &   insertion de $y,z,x$ \\
        \hline
    \end{tabular}
    \]
    \caption{Graphe d'inclusion pour deux ordres diff�rents d'inclusion, le second est bien s�r meilleur.}
    \label{partition_inclusion_graphe_ordre_insertion}
        \end{figure}

\begin{xremark}{ordre d'insertion}
L'arbre final d�pend de l'ordre d'insertion des �l�ments comme le montre la \indexfrr{ordre}{insertion}
figure~\ref{partition_inclusion_graphe_ordre_insertion}. L'algorithme~\ref{algorithm_insertion} peut �tre am�lior� et devenir l'algorithme~$\ref{algorithm_insertion}^*$ en rempla�ant les lignes (\ref{amelioration_insertion_1}) et (\ref{amelioration_insertion_2}) par les suivantes, respectivement (\ref{amelioration_insertion_1_p}) et (\ref{amelioration_insertion_2_p})~:

    \begin{eqnarray}
    \text{si }      && \forall n \in N, \; d\pa{m,ArgC\pa{n}} > R\pa{n} \label{amelioration_insertion_1_p} \\
    \text{sinon }   && \exists n \in N \text{ tel que } d\pa{m,ArgC\pa{n}} \infegal R\pa{n}
    \label{amelioration_insertion_2_p}
    \end{eqnarray}

o�~:

    \begin{eqnarray*}
    ArgC\pa{n}            &=&     \underset{x \in P\pa{n}} {\arg \min} 
                                                                \cro{  \underset{y \in P\pa{n}} {\max} \; d\pa{x,y}} \text{ et }
    d\pa{m,ArgC\pa{n}}    =     \underset{y \in ArgC\pa{n}} {\min } d\pa{m,y}
    \end{eqnarray*}

On ne consid�re pas seulement un centre mais l'ensemble des centres possibles, � �gale distance de l'�l�ment � ins�rer. Comme la distance de Levenstein\indexfr{Levenstein} est � valeurs enti�res, cet ensemble est rarement r�duit � un singleton comme le montrera le paragraphe~\ref{section_test}. Cette version de l'algorithme~\ref{algorithm_insertion} est not�e~$\ref{algorithm_insertion}^*$. La construction de l'arbre de partitionnement est effectu�e par la r�p�tition de l'algorithme~\ref{algorithm_insertion} ou~$\ref{algorithm_insertion}^*$ tant qu'il reste des �l�ments � classer.


\end{xremark}













\subsection{Optimisation de la recherche des plus proches voisins}
\label{section_optimisation_distance}

Cette optimisation de la recherche utilise un des arbres construits par l'algorithme~\ref{algorithm_AHC} ou la r�p�tition de l'algorithme~\ref{algorithm_insertion} ou~\ref{algorithm_insertion}$^*$. Chaque n\oe ud d�finit une partie d�crite par un centre et un rayon. Le probl�me � r�soudre consiste ici � trouver pour un �l�ment $m$ la liste $B\pa{s}$ des voisins inclus dans le sous-ensemble $D\vecteur{y_1}{y_N}$ v�rifiant~:

            $$
            B\pa{s} = \acc{ x \in D \sachant d\pa{x,m} \infegal s }
            $$

Soit $P \subset E$ une partie dont le centre est $C\pa{P}$ et le rayon $R\pa{P}$, l'optimisation est bas�e sur les deux remarques suivantes~:

    \begin{eqnarray}
    d\pa{m,C\pa{P}} > s + R\pa{P}                 &\Longrightarrow& 
                                                                                \forall w \in P, \; d\pa{m,w} > s 
                                                    \Longrightarrow    B\pa{s} \cap P = \emptyset                    \label{equation_un}     \\
    d\pa{m,C\pa{P}} + R\pa{P} \infegal s     &\Longrightarrow& 
                                                                                \forall w \in P, \; d\pa{m,w} \infegal s 
                                                                            \Longrightarrow    B\pa{s} \subset  P \label{equation_deux} 
    \end{eqnarray}




        \begin{xalgorithm}{recherche rapide}\label{algorithm_optimisation}%
        Soit $r$ la racine de l'arbre $A$ obtenu par un des algorithmes~\ref{algorithm_AHC},
        \ref{algorithm_insertion_all},
        $\ref{algorithm_insertion}^*$. $N$ est un ensemble de n\oe uds. Soit $s \in \mathbb{R}_+$, $B\pa{s}$ est 
        l'ensemble cherch�, il est d�fini par $B\pa{s} = \acc{ x \in D \sachant d\pa{x,m} \infegal s }$.
        
        \begin{xalgostep}{initialisation}
            $N \longleftarrow r$ \\
            $B\pa{s} \longleftarrow \emptyset$
        \end{xalgostep}
        
        \begin{xalgostep}{suite}\label{space_algo_step_B}
            \begin{xwhile}{$N \neq \emptyset$}
                Soit $n \in N$ et $p$ la partie d�finie par $n$, $n$ est retir� de $N$ : 
                                            $N \longleftarrow N \backslash n$
                et~: \\
                \begin{xif}{$d\pa{m, C\pa{p}} + R\pa{p}\infegal s $}
                    $B\pa{s} \longleftarrow B\pa{s} \bigcup p$
                    
                        \xelseif{$ d\pa{m, C\pa{p}} > s + R\pa{p} $}
                                ne rien faire
                \xelse
                    \begin{xif}{$p = \acc{ w \in D}$}
                            \begin{xif}{$d\pa{m, w} \infegal s$}
                                $B\pa{s} \longleftarrow B\pa{s} \bigcup \acc{w}$
                            \end{xif}
                    \xelse
                        $N \longleftarrow N \bigcup \acc{p_1\pa{n},p_2\pa{n}}$\\
                        o� $\acc{p_1\pa{n},p_1\pa{n}}$ sont les deux pr�d�cesseurs de $n$
                    \end{xif}
                \end{xif}
            \end{xwhile}
        \end{xalgostep} 
        
        La liste $B\pa{s}$ contient tous les voisins de $m$ sans aucune approximation.
        \end{xalgorithm}


\begin{xremark}{m�morisation des distances}
Durant l'�tape~\ref{space_algo_step_B} de l'algorithme~\ref{algorithm_optimisation}, il est n�cessaire de calculer les distance entre l'�l�ment $m$ et le centre de certaines parties. Ces centres appartiennent au sous-ensemble $D$ et plusieurs parties peuvent avoir le m�me centre si elles sont incluses les unes dans les autres. Si le calcul de la distance $d$ est co�teux, il est int�ressant de conserver en m�moire les r�sultats du calcul des distances de $m$ aux centres visit�s. Cette m�morisation\indexfr{m�morisation} implique qu'il ne peut y avoir plus de $N$ calculs de distance si $N$ est le nombre d'�l�ments de $D$.
\end{xremark}






\subsection{Crit�re d'efficacit�}
\label{section_criterion}

Quelque soit la m�thode choisie pour construire l'arbre de partitionnement, l'algorithme~\ref{algorithm_optimisation} m�ne � la solution exacte. D'un autre c�t�, on peut se demander s'il existe des arbres meilleurs que d'autres lors de la recherche des plus proches voisins et s'il existe un arbre optimal. La premi�re id�e est de consid�rer que plus les parties d�finies par l'arbre sont petites, plus la recherche sera rapide. Selon cette id�e, le crit�re (\ref{critere_optimalite}) essaye d'�valuer la pertinence\indexfr{pertinence}\indexfr{efficacit�} d'un arbre $A$ construit � partir du sous-ensemble fini $D$~:

    \begin{eqnarray}
    Cr_1\pa{A}                            &=&  \left\{   \begin{array}{l}
                                                0 \text{ si } R\pa{D} = 0 \text{ ou } 
                                                                card\pa{D} \infegal 1 \\ \\  \textnormal{sinon }
                                                \dfrac{\summyone{n \in A} R\pa{n}}{\pa{card\pa{D}-1} * R\pa{D}}
                                                \end{array}%
                                                \right.
                                                \label{critere_optimalite} \\
        \text{o� }&& \nonumber\\
        card\pa{D}                      &&      \text{est le nombre d'�l�ments de } D           \nonumber   \\
        R\pa{D}                         &&      \text{est le rayon de } D                       \nonumber   \\
        n                               &&      \text{est un n\oe ud de } A                     \nonumber   \\
        R\pa{n}                         &&      \text{est le centre de la partie d�finie par } n    \nonumber
    \end{eqnarray}


Si l'arbre $A$ choisi est construit par l'algorithme~\ref{algorithm_AHC}, le corollaire~\ref{corollary_AHC} permet d'affirmer que si $n$ est un n\oe ud de $A$, alors $R\pa{n} \infegal R\pa{D}$. De plus, l'arbre contient au plus $card\pa{D}-1$ n\oe uds dont le rayon est strictement positif~:

    $$
    card  \acc{n \in A \; | \; R \pa{n} > 0} \infegal card\pa{D}-1
    $$

Par cons�quent, l'arbre $A$ construit par l'algorithme~\ref{algorithm_AHC} satisfait~:

    \begin{eqnarray}
    R\pa{D} \infegal    \summyone{n \in A} R\pa{n} \infegal \pa{card\pa{D}-1} * R\pa{D}
                     \label{inegalite_critere}
    \end{eqnarray}

L'in�galit� (\ref{inegalite_critere}) explique l'expression du crit�re (\ref{critere_optimalite}) puisque~:
    \begin{eqnarray}
    R\pa{D} > 0 \Longrightarrow \dfrac{1}{card\pa{D}-1} \infegal    Cr_1\pa{A} \infegal 1  &&
             \label{inegalite_critere2}
    \end{eqnarray}

\begin{xremark}{limites}
Les deux limites de l'in�galit� (\ref{inegalite_critere2}) sont atteintes pour un sous-ensemble $D$ ne contenant que deux �l�ments distincts. Pour un ensemble contenant plus de trois �l�ments, la propri�t� suivante r�pond partiellement � la question.
\end{xremark}





        \begin{xproperty}{limites} \label{property_bornes_atteintes}%
        Soit $\pa{E,d}$ un espace m�trique quelconque,\indexfrr{espace}{m�trique} soit $D \neq
        \emptyset$ un sous-ensemble fini de $E$, soit $A_D$ l'arbre construit par
        l'algorithme~\ref{algorithm_AHC}, alors les trois         propositions suivantes sont vraies~:
        
        \begin{description}
        \item[(1) ] $\forall n > 1,$ il existe $D_1 \subset E$ tel que~:
            $$
            \left\{
            \begin{array}{rcl}
            card\pa{D_1} &=& n \\
            Cr_1\pa{A_{D_1}} &=& \dfrac{1}{n-1}
            \end{array}
            \right.
            $$
        \item[(2) ] $\forall n > 1,$ il existe $D_2 \subset E$ tel que~:
            $$
            \left\{
            \begin{array}{l}
            card\pa{D_2} = n \\ %\\
            \forall \pa{x,y} \in D_2^2, \; x \neq y \Longrightarrow d\pa{x,y} = R\pa{D_2}
            \end{array}
            \right.
            $$
            alors $Cr_1\pa{A_{D_2}} = 1$. \newline
        \item[(3) ] si $E$ est un espace vectoriel de dimension infinie, alors, $\forall n > 1,$ 
                                    il existe $D_3 \subset E$ tel que~:
            $$
            \left\{
            \begin{array}{rcl}
            card\pa{D_3} &=& n \\
            Cr_1\pa{A_{D_3}} &=& 1
            \end{array}
            \right.
            $$
        \end{description}
        \end{xproperty}

\begin{xdemo}{propri�t�}{\ref{property_bornes_atteintes}}

Pour prouver \textbf{(1)}, il faut consid�rer l'ensemble $D_1 = \vecteur{x_1}{x_n}$ d�fini par~:

    $$
    \left\{
    \begin{array}{l}
    x_1 \neq x_2 \\
    \forall i \supegal 3, \, x_i = x_2
    \end{array}
    \right.
    $$
    
De mani�re �vidente~: $Cr_1\pa{A_{D_1}} = \frac{1}{n-1}$.

Prouver \textbf{(2)} est aussi �vident parce que si un tel ensemble $D_2$ existe, pour une partie $P$ quelconque de $D$, $R\pa{P} = R\pa{D_2} = Cr_1\pa{A}$.

Pour prouver \textbf{(3)}, on n'utilise \textbf{(2)} puisqu'un tel ensemble $D_2$ existe dans un espace vectoriel de dimension infinie. C'est pr�cis�ment le cas de l'espace des mots.

\end{xdemo}


%-------------------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------------------
\comment{ 


Pour prendre en compte la pertinence de la r�union de deux parties, un second crit�re est d�fini~:

            \begin{eqnarray}
            Cr_2\pa{A}      &=&     \left\{\begin{array}{l}
                                      0 \text{ si } R\pa{D} = 0 \text{ ou } card\pa{D} \infegal 1 \\
                              \dfrac{ \summyone{n \in A} \biggcro{ 2 R\pa{n} - d\pa{n} }} 
                                          {\pa{card\pa{D}-1} * R\pa{D}} \text{ sinon}
                                                            \end{array}%
                              \right.
                              \label{critere_optimalite_2} \\
            \text{o� }&& \nonumber\\
            p_i\pa{n}            &&      \text{est un des deux pr�d�cesseurs du n\oe ud } n \\
            \text{et }d\pa{n}    &=&     d\pa{C\pa{p_1\pa{n}}, C\pa{p_2\pa{n}}}  \nonumber\\
                \end{eqnarray}

De mani�re �vidente, ce crit�re v�rifie (\ref{inegalite_critere3})~:

    \begin{eqnarray}
    R\pa{D} > 0 \Longrightarrow 0 \infegal    Cr_2\pa{A} \infegal 2  && \label{inegalite_critere3}
    \end{eqnarray}

Ce second crit�re ne sera pas �voqu� par la suite car il corrobore les d�ductions obtenus avec le premier crit�re.

}
%-------------------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------------------








\subsection{R�sultats exp�rimentaux}
\label{section_test}


La premi�re exp�rience consiste � chercher les voisins dans un ensemble de points tir�s al�atoirement dans le carr� $\cro{0,1} \times \cro{0,1}$ (figure~\ref{space_metric_rnd_01_01}). L'exp�rience consiste d'abord � tirer $N$ points al�atoires dans ce carr�. Pour diff�rentes valeurs de seuil~$s$, $N$~points sont de nouveau tir�s al�atoirement pour lesquels le voisinage $V_s\pa{x}=\acc{y \sac d\pa{x,y} \infegal s}$ est calcul� selon les deux algorithmes~\ref{algorithm_AHC} et~$\ref{algorithm_insertion}^*$. Si $X$ est une variable al�atoire de l'espace m�trique~$E$ -~les �l�ments de $E$ sont �quiprobables~-, l'objectif est d'estimer le nombre moyen de calculs de distance $r_s\pa{N}$ effectu�s pour d�terminer les voisins d'un �l�ment~:

            \begin{eqnarray}
            r_s\pa{N} = \dfrac{1}{N} \; \esp{ \text{nombre de distances calcul�es pour $V_s\pa{X}$}}
            \label{gain_mot}
            \end{eqnarray}

                \begin{figure}[ht]
                $$
                \begin{array}{|c|}\hline
            \includegraphics[height=3cm, width=3cm]{\filext{../space_metric/image/rnd}} \\ \hline
            \end{array}
            $$
            \caption{Tirage al�atoire de points dans le carr� $\cro{0,1} \times \cro{0,1}$.}
            \label{space_metric_rnd_01_01}
            \end{figure}
            
L'algorithme~\ref{algorithm_AHC} est de loin le meilleur et ce quelle que soit la valeur du seuil $s$ choisi. Cette sup�riorit� est �galement traduite par la valeur de $Cr_1$ obtenu pour chacun des arbres (voir table~\ref{space_metric_rnd_gain}).

    
                \begin{table}[ht]
                \[
                \begin{tabular}{|c|c|c|c|c|c|} \hline
                seuil &   $\frac{\esp{\card{V_s\pa{X}}}}{N}$ & 
                                $\begin{subarray}{c} r_s\pa{N=2000} \\ 
                                algorithme~\ref{algorithm_insertion}^* \end{subarray}$     &
                                $\begin{subarray}{c} r_s\pa{N=2000} \\ 
                                algorithme~\ref{algorithm_AHC} \end{subarray}$                             &
                                $\begin{subarray}{c} r_s\pa{N=5000} \\ 
                                algorithme~\ref{algorithm_AHC} \end{subarray}$                             &
                                $\begin{subarray}{c} r_s\pa{N=10000} \\ 
                                algorithme~\ref{algorithm_AHC} \end{subarray}$         \\ \hline
                0,001        &        0 \,\%   &      6,4 \,\%   &      1,2 \,\%  & 0,6  \,\% &  0,3 \,\% \\ %\hline
                0,01        &        0 \,\%   &      6,8 \,\%   &      1,4 \,\%  & 0,7  \,\% &  0,4 \,\% \\ %\hline
                0,1            &        2 \,\%   &      11,8 \,\%  &      4,1 \,\%  & 2,7  \,\% &  1,9 \,\% \\ %\hline
                0,2            &        10 \,\%  &      17,2 \,\%  &      6,8 \,\%  & 4,5  \,\% &  3,2 \,\% \\ %\hline
                0,3            &        21 \,\%  &      21,7 \,\%  &      8,7 \,\%  & 5,7  \,\% &  4,0 \,\% \\ %\hline
                0,4            &        34 \,\%  &      25,1 \,\%  &      9,6 \,\%  & 6,3  \,\% &  4,5 \,\% \\ %\hline
                0,5            &        48 \,\%  &      26,9 \,\%  &      10,0 \,\% & 6,5  \,\% &  4,6 \,\% \\ %\hline
                0,6            &        62 \,\%  &      27,9 \,\%  &      9,4 \,\%  & 6,1  \,\% &  4,4 \,\% \\ %\hline
                0,7            &        74 \,\%  &      27,0 \,\%  &      8,3 \,\%  & 5,4  \,\% &  3,9 \,\% \\ %\hline
                0,8            &        85 \,\%  &      24,0 \,\%  &      6,7 \,\%  & 4,3  \,\% &  3,1 \,\% \\ %\hline
                0,9            &        92 \,\%  &      19,2 \,\%  &      4,6 \,\%  & 3,0  \,\% &  2,1 \,\% \\ %\hline
                1                &        98 \,\%  &      10,9 \,\%  &      2,3 \,\%  & 1,4  \,\% &  1,0 \,\% \\ %\hline
                2                &        100 \,\% &      0,1 \,\%   &      0,1 \,\%  & 0,0  \,\% &  0,0 \,\% \\ \hline
                $Cr_1$    &   -          &   0,135             &   0,039             & 0,023          &  0,017   \\ \hline
                        \begin{minipage}[c]{3cm} 
                        temps de calcul (arbre) 
                        \end{minipage} 
                                &  - &   $\sim$ 2 sec                      & $\sim $30 sec& $\sim $2 min  &  $\sim $10 min   \\ \hline
                \end{tabular}
                \]
                \caption{    Gain apport� lors de la recherche du voisinage pour diff�rentes valeurs de seuil.
                                    Le premier test utilise un nuage de 2000 points tir�s
                                    al�atoirement dans l'ensemble $\cro{0,1}^2$, le second en utilise 5000,
                                    le dernier 10000. A seuil fixe, 
                                    la part du voisinage observ� d�cro�t lorsque $N$ diminue. Dans les trois cas, 
                                    lorsque le seuil est fix�, les rapports tailles de 
                                    voisinages sur nombre d'�l�ments sont sensiblement �gales quel que soit $N$.
                                    On s'aper�oit que le crit�re $Cr_1$ d�cro�t �galement lorsque $N$ augmente. 
                                    Les temps de calcul sont estim�es avec un processeur Intel Pentium~III � 1~GHz et 
                                    d�signent le temps n�cessaire � la construction de l'arbre.}
                \indexfr{Intel}
                \indexfr{Pentium}
                \indexfr{temps de calcul}
                \label{space_metric_rnd_gain}
                \end{table}



Une exp�rience similaire est effectu�e dans un espace de mots et pour mesurer l'am�lioration obtenu par l'optimisation d�crite au paragraphe~\ref{section_optimisation_distance}, le test suivant est r�alis�~:

        \begin{itemize}
    \item Un dictionnaire $D$ de 2178 pr�noms\indexfr{pr�nom} est utilis�, son rayon est 10.
    \item Le test consiste en l'obtention du voisinage $V_s\pa{m}$ de n'importe quel mot $m$ du dictionnaire.
    \item La distance utilis�e est cette de Levenstein (\citeindex{Levenstein1966}, 
            \citeindex{Wagner1974}).\indexfr{Levenstein}
        \end{itemize}

Sans optimisation, pour un mot donn� $m$, toutes les distances de $m$ avec les autres mots doivent �tre calcul�es. En utilisant l'optimisation propos�e, il n'est pas n�cessaire de les calculer toutes. Les r�sultats sont illustr�s par le tableau~\ref{metric_test_optimisation}.

        \begin{table}[ht]
    %\newline
    $$
    \begin{tabular}{|c|c|cc|} \hline
    \textnormal{seuil} &    $\frac{\esp{\card{V_s\pa{X}}}}{N}$ &
                            $\begin{subarray}{c} r_s\pa{N=2178} \\ algorithme~\ref{algorithm_AHC} \end{subarray}$ &
                $\begin{subarray}{c} r_s\pa{N=2178} \\ 
                algorithme~\ref{algorithm_insertion}^* \end{subarray}$ 
                \\ \hline
                                1                &    0,1 \,\%        &    17,3 \,\%    & 34,1     \,\% \\
                                2                &    0,3 \,\%        &    30,2 \,\%    & 46,9     \,\% \\
                                3                &    1,5 \,\%        &    45,7 \,\%    & 60,4     \,\% \\
                                4                &    7,0 \,\%        &    63,1 \,\%    & 73,3     \,\% \\
                                5                &    21,8 \,\%        &    76,3 \,\%    & 83,0     \,\% \\
                                6                &    45,2 \,\%        &    79,8 \,\%    & 86,0     \,\% \\
                                7                &    68,2 \,\%        &    74,0 \,\%    & 81,2     \,\% \\
                                8                &    82,8 \,\%        &    59,6 \,\%    & 71,1     \,\% \\
                                9                &    90,9 \,\%        &    45,2 \,\%    & 58,0     \,\% \\
                                10            &    95,5 \,\%        &    30,7 \,\%    & 43,5     \,\% \\
                                11            &    96,7 \,\%        &    21,0 \,\%    & 28,8     \,\% \\
                                12            &    98,5 \,\%        &    12,5 \,\%    & 18,3     \,\% \\
                                13            &    99,3 \,\%        &    7,7 \,\%    & 11,0     \,\% \\
                                14            &    99,4 \,\%        &    4,9 \,\%    & 7,3     \,\% \\
                                15            &    99,5 \,\%        &    3,0 \,\%    & 4,2     \,\% \\
                                16            &    99,5 \,\%        &    2,2 \,\%    & 2,7     \,\% \\
                                17            &    99,6 \,\%        &    1,5 \,\%    & 2,0     \,\% \\
                                18            &    99,8 \,\%        &    0,9 \,\%    & 1,5     \,\% \\
                                19            &    99,7 \,\%        &    0,9 \,\%    & 1,3     \,\% \\
                                20            &    99,9 \,\%        &    0,6 \,\%    & 1,2     \,\% \\ \hline
                $Cr_1$     &  -                    & 0,153      &  0,221 \\ \hline
                        \begin{minipage}[c]{3cm} 
                        temps de calcul (arbre) 
                        \end{minipage} 
                                & -     & $\sim$5 min &  $\sim$1 h\\ \hline
    \end{tabular}
    $$
    \caption{    Am�lioration moyenne mesur�e par (\ref{gain_mot}), comparaison des
                         algorithmes~\ref{algorithm_AHC},
                         $\ref{algorithm_insertion}^*$. Le centre du dictionnaire est 
                MARIE-LOUISE et son rayon est 17. Dans le pire des cas, $s=6$, 
                20\% des calculs de distances sont �vit�s.
                Les temps de calcul correspondand � la construction de l'arbre
                sont estim�s avec un processeur Intel Pentium 1~GHz.}
                \indexfr{Intel}
                \indexfr{Pentium}
                \indexfr{temps de calcul}
    \label{metric_test_optimisation}
        \end{table}


\begin{xremark}{ordre d'insertion}
L'ordre d'insertion des mots dans l'arbre affecte les r�sultats. En ce qui concerne les \indexfrr{ordre}{insertion}
algorithmes~\ref{algorithm_insertion} et~$\ref{algorithm_insertion}^*$, les mots ont �t� ins�r�s par ordres croissant et d�croissant de taille, les diff�rences sont rendues par le tableau~\ref{test_optimisation_taille_2}.
\end{xremark}


        \begin{table}[ht]
       %\newline
    $$
    \fbox{$
    \begin{array}{ccccc}
    \text{seuil} &  \begin{subarray}{c} r \textnormal{ moyen} \\ algorithme~\ref{algorithm_insertion}^* \\
                    \textnormal{longueurs d�croissantes} \end{subarray} &
                    \begin{subarray}{c} r \textnormal{ moyen} \\ algorithme~\ref{algorithm_insertion}^* \\
                    \textnormal{longueurs croissantes} \end{subarray}
                    \\
    1            & 82,0 \,\% &    75,2 \,\%     \\
    2            & 65,8 \,\% &    60,6 \,\%     \\
    3            & 45,6 \,\% &    42,8 \,\%     \\
    4            & 26,7 \,\% &    25,5 \,\%     \\
    5            & 12,8 \,\% &    12,2 \,\%
    \end{array}
    $}
    $$
    \caption{Am�lioration moyenne mesur�e par (\ref{gain_mot}), comparaison des ordres d'insertion}
    \label{test_optimisation_taille_2}
        \end{table}





        \begin{table}[ht]
    %\newline
    $$
    \begin{tabular}{|c|cc|} \hline
    \textnormal{seuil} &    $\frac{\esp{\card{V_s\pa{X}}}}{N}$ &
                            $\begin{subarray}{c} r_s\pa{N=4987} \\ algorithme~\ref{algorithm_AHC} \end{subarray}$
                \\ \hline
                                1        &    0,0             \,\%    &    5,7         \,\%     \\
                                2        &    0,0             \,\%    &    10,4    \,\%     \\
                                3        &    0,0             \,\%    &    17,1    \,\%     \\
                                4        &    0,1             \,\%    &    26,4    \,\%     \\
                                5        &    0,4             \,\%    &    42,2    \,\%     \\
                                6        &    2,0             \,\%    &    61,6    \,\%     \\
                                7        &    9,0             \,\%    &    82,1    \,\%     \\
                                8        &    34,1         \,\%    &    94,7    \,\%     \\
                                9        &    77,4         \,\%    &    91,3    \,\%     \\
                                10    &    97,7         \,\%    &    73,1    \,\%     \\
                                11    &    99,4         \,\%    &    48,4    \,\%     \\
                                12    &    99,9         \,\%    &    31,0    \,\%     \\
                                13    &    100,0         \,\%    &    20,1    \,\%     \\
                                14    &    100,0         \,\%    &    11,1    \,\%     \\
                                15    &    100,0         \,\%    &    6,0         \,\%     \\
                                16    &    100,0         \,\%    &    3,0         \,\%     \\
                                17    &    100,0         \,\%    &    1,3         \,\%     \\
                                18    &    100,0         \,\%    &    0,0         \,\%     \\ \hline
                $Cr_1$     &  -                    & 0,210                \\ \hline
                temps de calcul & -     & $\sim$3 h   \\ \hline
                \end{tabular}
    $$
    \caption{    Am�lioration moyenne mesur�e par (\ref{gain_mot}), le test est effectu�
                        sur un dictionnaire de 4987 mots anglais de centre "POSITIVELY" de rayon 13. 
                        L'optimisation est plus pertinente dans ce cas o� le dictionnaire
                        contient plus du double de mots que celui utilis�
                        pour le test~\ref{metric_test_optimisation}.
                        }
    \label{metric_test_optimisation_dicos}
        \end{table}









%-------------------------------------------------------------------------------------------------------------------
\section{Voisinage dans un espace vectoriel}
%-------------------------------------------------------------------------------------------------------------------


Lorsque l'espace m�trique est aussi vectoriel, la recherche des plus proches voisins est facilit� car il est possible d'utiliser les coordonn�es des �l�ments comme dans l'algorithme \emph{Branch and Bound}. Ces coordonn�es permettent �galement d'obtenir des r�sultats th�orique plus avanc�s en ce qui concerne le co�t de cette recherche (voir \citeindex{Arya1994}.





\subsection{B+ tree}
\indexfr{B+ tree}

Ce premier algorithme s'applique dans le cas r�el afin d'ordonner des nombres dans un arbre de sorte que chaque n\oe ud ait un p�re et pas plus de $n$ fils (voir figure~\ref{space_metric_btree}). 


                \begin{figure}[ht]
                $$\begin{array}{|c|}\hline
            \includegraphics[height=5cm, width=7cm]{\filext{../space_metric/image/btree}} \\ \hline
            \end{array}$$
            \caption{Illustration d'un B+ tree.}
            \label{space_metric_btree}
            \end{figure}

        \begin{xdefinition}{B+ tree}
        Soit $B_n$ un B+ tree, soit $N$ un n\oe ud de $B_n$, il contient un vecteur $V\pa{N} = \vecteur{x_1}{x_t}$ 
        avec $0 \infegal t \infegal n$ et $x_1 < ... < x_t$. Ce n\oe ud contient aussi exactement $t-1$ n\oe uds fils 
        not�s $\vecteur{N_1}{N_{t-1}}$. On d�signe par $D\pa{N_t}$ l'ensemble des descendants du n\oe ud $N_t$ et 
        $G\pa{N_t} = \acc{ V\pa{M} \sac M \in D\pa{N_t}}$. Le n\oe ud $N$ v�rifie~:
                    \begin{eqnarray*}
                    && \forall x \in G\pa{N_t}, \; x_{t} \infegal x < x_{t+1} \\
                    && \text{avec par convention } x_0 = -\infty \text{ et } x_{t+1} = + \infty
                    \end{eqnarray*}
        \end{xdefinition}
        
\indexfr{quicksort}
\indexfrr{tri}{quicksort}
        
Cet arbre permet de trier une liste de nombres, c'est une g�n�ralisation du tri "quicksort" pour lequel $n=2$. Comme pour le tri quicksort, l'arbre est construit � partir d'une s�rie d'insertions et de cet ordre d�pend la rapidit� du tri. L'esp�rance du co�t (moyenne sur tous les permutations possibles de $k$ �l�ments), le co�t de l'algorithme est en $O\pa{k \log_n k}$. 








\subsection{R-tree ou Rectangular Tree}
\indexfr{R-tree}

L'arbre R-tree est l'adaptation du m�canisme du B+ tree au cas multidimensionnel (voir \citeindex{Guttman1984}). La construction de cet arbre peut se faire de mani�re globale -~construction de l'arbre sachant l'ensemble de points � classer~- ou de mani�re progressive -~insertion des points dans l'arbre les uns � la suite des autres~-. Ces arbres sont comparables � l'arbre de partitionnement construit par l'algorithme~\ref{algorithm_AHC} � ceci pr�s que la forme des ensembles est constitu�e de rectangles et non plus de cercles. L'appartenance d'un point � un rectangle d�pend dor�navant des comparaisons entre coordonn�es tandis que l'appartenance d'un point � un cercle n�cessite le calcul d'une distance, ce qui est plus lent. Toutefois, ces m�thodes sont resteintes � des espaces vectoriels.

                \begin{figure}[ht]
                $$\begin{array}{|c|c|}\hline
            \includegraphics[height=6cm, width=6cm]{\filext{../space_metric/image/rtree1}} &
            \includegraphics[height=5cm, width=11cm]{\filext{../space_metric/image/rtree2}} \\ \hline
            \end{array}$$
            \caption{    Illustration d'un R-tree en deux dimensions, 
                                figure extraite de \citeindexfig{Sellis1987}, la premi�re image montre des rectangles
                                pointill�s englobant d'autres rectangles en trait plein. Chaque style de trait correspond
                                � un niveau dans le graphe de la seconde image.
                                }
            \label{space_metric_rtree}
            \end{figure}
            
\indexfrr{bo�te}{englobante}
\indexfrr{bo�te}{objet}
\indexfrr{bo�te}{fen�tre}

Il n'existe pas une seule mani�re de construire un R-tree, les n\oe uds de ces arbres suivent toujours la contrainte des B+~Tree qui est d'avoir un p�re et au plus $n$ fils. Les R-Tree ont la m�me structure que les B+~Tree �t�e de leurs contraintes d'ordonnancement des fils. De plus, ces arbres organisent spatialement des rectangles ou bo�tes en plusieurs dimensions comme le sugg�re la figure~\ref{space_metric_rtree}. Les bo�tes � organiser seront nomm�s les objets, ces objets sont ensuite regroup�s dans des bo�tes englobantes. Un n\oe ud $n$ d'un R-tree est donc soit une feuille, auquel cas la bo�te qu'il d�signe est un objet, dans ce cas, il n'a aucun fils, soit le n\oe ud d�signe une bo�te englobante $B\pa{n}$. On d�signe par $\mathcal{B}$ l'ensemble des bo�tes d'un espace vectoriel quelconque et $v\pa{b}$ d�signe son volume. Pour un n\oe ud $n$ non feuille, $A\pa{n}$ d�signe l'ensemble des descendants de ce n\oe ud. $B\pa{n}$ est d�fini par~:

            $$
            B\pa{n} = \arg \min \acc{ v\pa{b} \sac b \in \mathcal{B} \text{ et } 
                                        \forall n' \in A\pa{n'}, \; B\pa{n'} \subset B\pa{n} }
            $$



La recherche dans un R-tree consiste � trouver toutes les objets ayant une intersection avec une autre bo�te ou fen�tre $W$, soit l'ensemble $L$~:

        $$
        L = \acc{ B\pa{n} \sac B\pa{n} \text{ est un objet et } B\pa{n} \cap W \neq \emptyset }
        $$ 


Cet ensemble est construit gr�ce � l'algorithme suivant~:


        \begin{xalgorithm}{recherche dans un R-tree}  \label{space_metric_algo_r_tree_search}
        Les notations sont celles utilis�es dans ce paragraphe. On d�signe par $r$ le n\oe ud racine d'un R-tree. 
        Soit $n$ un n\oe ud, on d�signe par $F\pa{n}$ l'ensemble des fils de ce n\oe ud.
        
        \begin{xalgostep}{initialisation}
        $L \longleftarrow 0$ \\
        $N \longleftarrow \acc{r}$
        \end{xalgostep}
        
        \begin{xalgostep}{it�ration}
        \begin{xwhile}{$N \neq \emptyset$}
            \begin{xforeach}{n}{N}
                \begin{xif}{$W \cap B\pa{n} \neq \emptyset$} 
                  $N \longleftarrow N \cup F\pa{n}$ \\
                    \begin{xif}{$B\pa{n}$ est un objet}
                            $L \longleftarrow B\pa{n}$ 
                    \end{xif}
                \end{xif}
            \end{xforeach}
        \end{xwhile}
        \end{xalgostep}
    
        $L$ est l'ensemble cherch�.
        
        \end{xalgorithm}


Il reste � construire le R-tree, op�ration effectu�e par la r�p�tition successive de l'algorithme~\ref{space_metric_algo_r_tree_insert} permettant d'ins�rer un objet dans un R-tree.

        \begin{xalgorithm}{insertion d'un objet dans un R-tree} \label{space_metric_algo_r_tree_insert}
        Les notations utilis�es sont les m�mes que celles de l'algorithme~\ref{space_metric_algo_r_tree_search}.
        On cherche � ins�rer l'object $E$ d�sign� par son n\oe ud feuille $e$. On suppose que l'arbre contient au
        moins un n\oe ud, sa racine $r$. On d�signe �galement par $p\pa{n}$ le p�re du n\oe ud $n$. Chaque n\oe ud 
        ne peut contenir plus de $s$ fils. On d�signe par  
        $v^*\pa{G} = \min \acc{ P \sac P \in \mathcal{B} \text{ et } 
                \unionone{g \in G} B\pa{g}  \subset P }$.
        
        \begin{xalgostep}{s�lection du n\oe ud d'insertion}
        $n^* \longleftarrow r$ \\
        \begin{xwhile}{$n^*$ n'est pas un n\oe ud feuille}
            On choisit le fils $f$ de $n^*$ qui minimise l'accroissement $v_f - v\pa{B\pa{f}}$ 
            du volume avec $v_f$ d�fini par~: 
                \begin{eqnarray}
                v_f = \min \acc{ v\pa{P} \sac P \in \mathcal{B} \text{ et } B\pa{f} \cup B\pa{e}  \subset P }  
                \label{space_metric_r_tree_b_n_update}
                \end{eqnarray}
                $n^* \longleftarrow f$
        \end{xwhile}
        \end{xalgostep}
        
        \begin{xalgostep}{ajout du n\oe ud}
        Si $p\pa{n^*}$ a moins de $s$ fils, alors le n\oe ud $e$ devient le fils de $p\pa{n^*}$ et $B\pa{p\pa{n^*}}$ est 
        mis � jour d'apr�s l'expression (\ref{space_metric_r_tree_b_n_update}). L'insertion est termin�e.
        Dans le cas contraire, on s�pare d�coupe le n\oe ud $p\pa{n^*}$ en deux gr�ce � l'�tape suivante.
        \end{xalgostep}
        
        %\possiblecut 

        \begin{xalgostep}{d�coupage des n\oe uds} \label{space_metric_insertion_decoupage_r_tree}
        L'objectif est de diviser le groupe $G$ compos� de $s+1$ n\oe uds en deux groupes $G_1$ et $G_1$. 
        Tout d'abord, on cherche 
        le couple $\pa{n_1,n_2}$ qui minimise le crit�re $$ d = v^*\pa{\acc{n_1,n_2}} - v\pa{B\pa{n_1}} - v\pa{B\pa{n_2}}$$ Alors~:
        $G_1 \longleftarrow n_1$, $G_2 \longleftarrow n_2$ et $G \longleftarrow G - G_1 \cup G_2$ \\
        \begin{xwhile}{$G \neq \emptyset$}
                On choisit un n\oe ud $n \in G$, on d�termine $i^*$ tel que $v\pa{\acc{n} \cup G_i} - v\pa{G_i}$ soit minimal. \\
                $G \longleftarrow G - \acc{n}$ \\
                $G_{i^*} \longleftarrow G_{i^*} \cup \acc{n}$
        \end{xwhile}
        \end{xalgostep}


        \end{xalgorithm}








\indexfr{R$^*$ tree}
\indexfr{R$^*$ tree}
\indexfr{R+ Tree}
Si la recherche est identique quel que soit l'arbre construit, chaque variante de la construction de l'arbre tente de minimiser les intersections des bo�tes et leur couverture. Plus pr�cis�ment, l'�tape~\ref{space_metric_insertion_decoupage_r_tree} qui permet de d�couper les n\oe uds est con�ue de mani�re � obtenir des bo�tes englobantes de volume minimale et/ou d'intersection minimale avec d'autres bo�tes englobantes. L'algorithme R+~Tree (voir \citeindex{Sellis1987}) essaye de minimiser les intersections entre bo�tes et les objets � organiser sont suppos�s n'avoir aucune intersection commune. La variante R$^*$~Tree (voir \citeindex{Beckmann1990}) effectue un compromis entre l'intersection et la couverture des bo�tes englobantes. L'algorithme X-Tree (voir \citeindex{Berchtold1996}) conserve l'historique de la construction de l'arbre ce qui lui permet de mieux �viter les intersections communes entre bo�tes.









\subsection{Branch and Bound}
\indexfr{Branch and Bound}
\indexfr{hi�rarchie}
\indexfrr{d�composition}{hi�rarchique}
\indexfr{espace vectoriel}

Les algorithmes regroup�s sous cette terminaison \emph{Branch and Bound} englobe la plupart des m�thodes pr�sent�es dans ce document, elles d�signent tout algorithme de recherche n�cessitant une premi�re �tape permettant de construire une d�composition hi�rarchique de l'ensemble des exemples ou exemples d'apprentissage, cet ensemble �tant inclus dans un espace vectoriel. La premi�re version de cette famille algorithme a �t� d�velopp�e dans \citeindex{Fukunaga1975}.

\indexfrr{loi}{normale}

La m�thode pr�sent�e dans \citeindex{D'Haes2003} utilise une analyse en composantes principales afin de construire une hi�rarchique adapt�e � un nuage de points ob�issant � une loi normale multidimensionnelle (voir figure~\ref{space_metric_dheas_1}).



                \begin{figure}[ht]
                $$\begin{array}{|c|}\hline
            \includegraphics[height=6cm, width=12cm]{\filext{../space_metric/image/dhaes}} \\ \hline
            \end{array}$$
            \caption{ Figure extraite de \citeindexfig{D'Haes2003} repr�sentant l'arbre de d�composition
                                de deux nuages de points ob�issant � des loi normales, uniforme dans le premier cas, 
                                avec deux variables corr�l�es dans le second cas.}
            \label{space_metric_dheas_1}
            \end{figure}

\indexfr{ACP}
\indexfr{m�diane}
\indexfr{analyse en composantes principales}

La m�thode propos�e dans cet article effectue une analyse en composantes principales afin de rep�rer l'axe principal du nuage qui correspond au vecteur propre $\vec{V}$ associ� � la plus grande des valeurs propres de la matrice $X'X$ o� chaque ligne de la matrice $X$ est un �l�ment du nuage de points $\vecteur{\vec{x_1}}{\vec{x_n}}$. On d�termine la m�diane $m$ de l'ensemble $\acc{ < \vec{V}, \vec{x_i}  >  \sac 1 \infegal i \infegal n}$. Le nuage de points est alors divis�e en deux sous-nuages de cardinaux �gaux selon que le produit scalaire $< \vec{V}, \vec{x_i} >$ est inf�rieur ou sup�rieur � $m$. Chaque sous-nuage est � nouveau divis� selon le m�me processus incluant une analyse en composantes principales et la recherche de la m�diane. L'algorithme s'arr�te lorsque les sous-ensembles sont r�duits � un seul �l�ment. 







\subsection{M�thodes approch�es}
\indexfrr{kPPV}{m�thode approch�e}

Toutes les m�thodes propos�es jusqu'� pr�sent permettent de d�terminer le voisinage exact d'un �l�ment inclus dans un ensemble d'exemples, les diff�rences concernant l'organisation de cet ensemble afin d'optimiser la recherche. L'autre direction de recherche concerne la recherche du voisinage notamment par des m�thodes approch�es, plus rapide, mais ne retournant pas le voisinage exact. Ces m�thodes ne seront pas plus d�velopp�es ici, l'article \citeindex{Arya1994} propose une �tude th�orique de ce probl�me. L'article \citeindex{Ramasubramanian2000} compare plusieurs m�thodes de recherche du voisinage.





















%-------------------------------------------------------------------------------------------------------------
\section{Autres alternatives}
%-------------------------------------------------------------------------------------------------------------

Il existe de nombreuses alternatives en ce qui concerne la recherche des plus proches voisins dans un espace m�trique quelconque, pass�es en revue dans les articles \citeindex{Bustos2001}, \citeindex{Chavez1999}, \citeindex{Navarro2001}, certaines utilisant plus particuli�rement les arbres comme \citeindex{Ulhmann1991} ou \citeindex{Yianilos1993}. Lorsque cette recherche s'applique aux mots ou aux s�quences, l'optimisation de la distance est envisag�e (voir \citeindex{Apostolico1985} ou \citeindex{Madhvanath2001}). L'algorithme qui suit est une de ces alternatives pr�f�r�e aux autres en raison de sa simplicit�. Il ne n�cessite pas la construction d'un arbre et son co�t est ais�ment calculable.


\subsection{LAESA}

\label{space_metric_laesa_laesa}
\indexfr{LAESA}\indexfr{pivot}

Cet algorithme permet de chercher les plus proches voisins dans un ensemble inclus dans un espace m�trique quelconque. Il s'appuie encore sur l'in�galit� triangulaire appliqu�e de mani�re semblable � (\ref{equation_un}). Le voisinage d'un point $x$ doit �tre cherch� dans un ensemble $E$. L'algorithme LAESA (Linear Approximating Eliminating Search Algorithm, voir \citeindex{Rico-Juan2003}) consiste � �viter un trop grand nombre de calculs de distances en se servant de distances d�j� calcul�es entre les �l�ments de $E$ et un sous-ensemble $B$ inclus dans $E$ contenant des "pivots". 

La s�lection des pivots demeure un probl�me ouvert. Ceux-ci pourrait �tre les n\oe uds d'un coupe de l'arbre construit par l'algorithme~\ref{algorithm_AHC}. Il existe d'autres possibilit�s comme l'algorithme~\ref{space_metric_algo_laesa_prime} plus simple et nettement moins co�teux -~les deux algorithmes n'ont pas �t� compar�s en terme de performances en classification. 




            \begin{xalgorithm}{LAESA}
            \label{space_metric_algo_laesa}
            
            Soit $E = \ensemble{y_1}{y_N}$ un ensemble de points, $B = \ensemble{p_1}{p_P} \subset E$ 
            un ensemble de pivots inclus dans $E$. On cherche � d�terminer le voisinage $V\pa{x}$ de $x$ 
            inclus dans $E$ v�rifiant~:
            
                        $$
                        \forall y \in V\pa{x}, \; d\pa{x,y} \infegal \rho
                        $$
                        
            On suppose que la matrice $M = \pa{m_{ij}}_ { \begin{subarray} 1 \infegal i \infegal P \\ 
            1 \infegal j \infegal N \end{subarray} }$ a �t� calcul�e pr�alablement comme suit~:
            
                        $$
                        \forall \pa{i,j}, m_{ij} = d\pa{p_i, y_j}
                        $$
                        
            \begin{xalgostep}{initialisation}
                $\begin{array}{ll}
                \forall y \in E, & g\pa{y} \longleftarrow 0 \\
                \forall y \in E, & h\pa{y} \longleftarrow 1
                \end{array}$
            \end{xalgostep}        
            
            \begin{xalgostep}{choix d'un pivot et mise � jour de la fonction $g$} \label{classif_laesa_step_b}
                $B' \longleftarrow B \cap \acc{ y \in E \sac h\pa{y} = 0}$ \\
                \begin{xif}{$B' \neq \emptyset$}
                    Soit $i$ tel que $p_i$ soit un �l�ment de $B'$ tir� au hasard tel que~: \\ 
                                    $$p_i \in \arg \min \acc{ g\pa{y} \sac y \in B'}$$
                    $\begin{array}{lll}
                    \alpha         &\longleftarrow& d\pa{p_i,x} \\
                    h\pa{p_i} &\longleftarrow& 1 
                    \end{array}$ \\
                    $\begin{array}{rl}
                    \forall y_j \in E \text{ tel que } h\pa{y_j} = 0, \; g\pa{y_j} \longleftarrow &
                                            \max \acc {    g\pa{y_j} , \abs{ \alpha - m_{ij} }  } \\ = &
                                            \max \acc {    g\pa{y_j} , \abs{ d\pa{x,p_i} - d\pa{p_i, y_j} }  } 
                    \end{array}$
                \xelse        
                    Choisir un �l�ment $s$ de $E$ tel que $h\pa{s} = 0$. \\
                    $\begin{array}{lll}
                    \alpha         &\longleftarrow& d\pa{s,x} \\
                    h\pa{s}   &\longleftarrow& 1 
                    \end{array}$ \\
                    $\begin{array}{rl}
                    \forall y   \in E \text{ tel que } h\pa{y} = 0, \; g\pa{y} \longleftarrow & d\pa{x,y}
                    \end{array}$
                
                \end{xif}
                    
                    
                    
            \end{xalgostep}
            
            \possiblecut
            
            
            \begin{xalgostep}{�limination}
                $\begin{array}{rl}
                \forall y_j \in E \text{ tel que } h\pa{y_j} = 0, \text{ si } g\pa{y_j} > \rho \text{ alors }
                h\pa{y_j} = 1
                \end{array}$
            \end{xalgostep}
            
            \begin{xalgostep}{terminaison}
                $A \longleftarrow \acc{ y \in E \sac h\pa{y} = 0 }$ \\
                \begin{xif}{$A \neq \emptyset$}
                Retour � l'�tape~\ref{classif_laesa_step_b}.
                \xelse
                Fin, l'ensemble cherch� correspond � $\acc{y \in E \sac g\pa{y}} \infegal \rho$.
                \end{xif}
            \end{xalgostep}
            
            \end{xalgorithm}




La s�lection des pivots est assur�e par un autre algorithme d�crit dans l'article~\citeindex{Moreno2003}.


            \begin{xalgorithm}{LAESA : s�lection des pivots}
            \label{space_metric_algo_laesa_pivtos_sel}
            \indexfrr{pivot}{s�lection}
            
            Soit $E = \ensemble{y_1}{y_N}$ un ensemble de points, on cherche � d�terminer 
            l'ensemble $B = \ensemble{p_1}{p_P} \subset E$ utilis� par l'algorithme~\ref{space_metric_algo_laesa}.
            
            \begin{xalgostep}{initialisation}
                $B \longleftarrow y \in E$ choisi arbitrairement.
            \end{xalgostep}
            
            \begin{xalgostep}{calcul de la fonction $g$} \label{space_metric_laesa_pivots_sel_b}
                    \begin{xforeach}{y}{E - B}
                        $g\pa{y} \longleftarrow 0$ \\
                        \begin{xforeach}{p}{B}
                        $g\pa{y} \longleftarrow g\pa{y} + d\pa{y,p}$
                        \end{xforeach}
                    \end{xforeach}
            \end{xalgostep}
            
            \begin{xalgostep}{mise � jour de $B$}
                    Trouver $p^* \in \arg \max \acc { g\pa{p} \sac p \in E - B}$\\
                    $B \longleftarrow B \cup \acc{ p^*}$ \\
                    Si $\card{B} < P$, retour � l'�tape~\ref{space_metric_laesa_pivots_sel_b} sinon fin.
            \end{xalgostep}
            
            \end{xalgorithm}




Cet article~\citeindex{Moreno2003} am�liore �galement l'algorithme~\ref{space_metric_algo_laesa} par le suivant~:



            \begin{xalgorithm}{LAESA'}
            \label{space_metric_algo_laesa_prime}
            \indexfr{LAESA'}
            
            Soit $E = \ensemble{y_1}{y_N}$ un ensemble de points, $B = \ensemble{p_1}{p_P} \subset E$ 
            un ensemble de pivots inclus dans $E$. On cherche � d�terminer le voisinage $V\pa{x}$ de $x$ 
            inclus dans $E$ v�rifiant~:
            
                        $$
                        \forall y \in V\pa{x}, \; d\pa{x,y} \infegal \rho
                        $$
                        
            On suppose que la matrice $M = \pa{m_{ij}}_ { \begin{subarray} 1 \infegal i \infegal P \\ 
            1 \infegal j \infegal N \end{subarray} }$ a �t� calcul�e pr�alablement comme suit~:
            
                        $$
                        \forall \pa{i,j}, \; m_{ij} = d\pa{p_i, y_j}
                        $$
                        
            \begin{xalgostep}{initialisation}
                $\forall i \in \ensemble{1}{P}, \; d_i \longleftarrow d\pa{x,p_i}$
            \end{xalgostep}        
            
            \begin{xalgostep}{fonction $g$} \label{classif_laesa_prime_step_b}
                $\forall j \in \ensemble{1}{N}, \;  g\pa{y_j} \longleftarrow \underset{  i \in \ensemble{1}{P} }{\min}  \abs{ m_{ij} - d_i} $
            \end{xalgostep}
            
            \begin{xalgostep}{tri}
                Tri l'ensemble $g\pa{y_i}$ par ordre croissant $\longrightarrow g\pa{y_{\sigma\pa{j}}}$. \\
                \begin{xfor}{j}{1}{N}
                   \begin{xif}{$g\pa{y_{\sigma\pa{j}}} \infegal \rho$}
                   $g\pa{y_{\sigma\pa{j}}} = d\pa{x,y_{\sigma\pa{j}}}$
                   \end{xif}
                \end{xfor} 
                
                Fin, l'ensemble cherch� correspond � $\acc{y \in E \sac g\pa{y}} \infegal \rho$.
            \end{xalgostep}

\end{xalgorithm}


\indexfr{TLAESA}

Il existe d'autres versions de l'algorithme LAESA comme TLAESA (Tree - LAESA) (voir \citeindex{Mic\'o1996}). Cet algorithme associe un arbre � l'algorithme LAESA et fait le lien entre les algorithmes~\ref{algorithm_AHC} et~\ref{space_metric_algo_laesa}.




\subsection{R�sultats th�oriques}

\indexfr{mesure}\indexfr{densit�}
L'article~\citeindex{Farag\'o1993} d�montre �galement qu'il existe une majoration du nombre moyen de calcul de distances pour peu que la mesure de l'espace contenant l'ensemble $E$ et l'�l�ment $x$ soit connue et que l'ensemble $B = \ensemble{p_1}{p_P}$ des pivots v�rifie~:

            \begin{eqnarray}
            \exists \pa{\alpha,\beta} \in \mathbb{R}^+_* \text{ tels que } && \nonumber\\
            \forall \pa{x,y} \in E^2, \; \forall i\, && \alpha \, d\pa{x,y} \supegal 
                            \abs{d\pa{x,p_i} - d\pa{p_i,y}} \label{space_metric_cond_1} \\
            \forall \pa{x,y} \in E^2, && \underset{i}{\max} \; \abs{d\pa{x,p_i} - d\pa{p_i,y}} \supegal 
                            \beta \, d\pa{x,y} \label{space_metric_cond_1}
            \end{eqnarray}


L'algorithme d�velopp� dans~\citeindex{Farag\'o1993} permet de trouver le point de plus proche d'un �l�ment $x$ dans un ensemble $E = \ensemble{x_1}{x_N}$ selon l'algorithme suivant~:


            \begin{xalgorithm}{plus proche voisin d'apr�s [Farag\'o1993]}\label{space_metric_algo_farago}
            Soit $E = \ensemble{x_1}{x_N}$ et $B = \ensemble{p_1}{p_P} \subset E \subset X$. Soit $x \in X$ 
            un �l�ment quelconque. 
            On suppose que les valeurs $m_{ij} = d\pa{x_i, p_j}$ ont �t� pr�alablement calcul�es.
            
            \begin{xalgostep}{initialisation}
            On calcule pr�alablement les coefficients $\gamma\pa{x_i}$~:
                              $$
                                \forall i \in \ensemble{1}{N}, \; \gamma\pa{x_i} \longleftarrow \underset{j 
                                                        \in \ensemble{1}{P} } {\max} \;
                                            \abs{ m_{ij} - d\pa{x,p_j} }
                                $$
            \end{xalgostep}        
            
            \begin{xalgostep}{�laguage}
            On d�finit $t_0 \longleftarrow \underset{i} {\min} \; \gamma\pa{x_i}$. \\
            Puis on construit l'ensemble $F\pa{x} = \acc{ x_i \in E \sac \gamma\pa{x_i} }\infegal
                         \frac{\alpha}{\beta} \, t_0$.
            \end{xalgostep}        
            
            \begin{xalgostep}{plus proche voisin}
            Le plus proche $x^*$ voisin est d�fini par~: $x^* \in \arg \min \acc{ d\pa{x,y} \sac y \in F\pa{x}}$.
            \end{xalgostep}        
            
            \end{xalgorithm}



            \begin{xtheorem}{[Farag\'o1993]$^1$}
                                                                \label{space_metric_farago_1}
            Les notations sont celles de l'algorithme~\ref{space_metric_algo_farago}.        
            L'algorithme~\ref{space_metric_algo_farago} retourne le plus proche voisin $x^*$ de $x$ inclus dans $E$. 
            Autrement dit, $\forall x \in X, \; x^* \in F\pa{x}$.
            \end{xtheorem}



\begin{xremark}{mesure de dissimilarit�}
L'algorithme~\ref{space_metric_algo_farago} est en fait valable pour une distance mais aussi pour une mesure de dissimilarit�. Contrairement � une distance, une mesure de dissimalit� ne v�rifie pas l'in�galit� triangulaire.
\indexfrr{mesure}{dissimilarit�}\indexfr{dissimilarit�}
\indexfr{in�galit� triangulaire}
\end{xremark}


            \begin{xtheorem}{[Farag\'o1993]$^2$}
                                                                \label{space_metric_farago_2}
            Les notations sont celles de l'algorithme~\ref{space_metric_algo_farago}. On d�finit une mesure 
            sur l'ensemble $X$, $B\pa{x,r}$ d�signe la boule de centre $x$ et de rayon $r$, $Z \in X$ une variable 
            al�atoire, de plus~:
                        $$
                        p\pa{x,r} = P_X \pa{B\pa{x,r}} = \pr{  Z \in B\pa{x,r}}
                        $$
                        
            On suppose qu'il existe $d > 0$ et une fonction $f : X \longrightarrow \mathbb{R}$ tels que~:
                        $$
                        \underset { r \rightarrow 0 } { \lim } \; \frac{ p\pa{x,r} } { r^d } = f\pa{x} > 0
                        $$
          La convergence doit �tre uniforme et presque s�re.
          On note �galement $F_N$ le nombre de calculs de dissimilarit� effectu�s par 
          l'algorithme~\ref{space_metric_algo_farago} o� $N$ est le nombre d'�l�ment de $E$, 
          $P$ d�signe toujours le nombre de pivots, alors~:
          
                      $$
                      \underset{ n \rightarrow \infty } { \lim \sup } \;
                                      \esp{F_N} \infegal k + \pa{\frac{\alpha}{\beta}}^{2d}
                      $$
                                                                            
            \end{xtheorem}









\subsection{Suppression des voisins inutiles}
\label{space_metric_suppression_voisins_inutile}
\indexfr{voisins inutiles}
\indexfr{plus proches voisins}
\indexfr{classification}
\indexfr{capacit� d'attraction}

L'article \citeindex{Wu2002} propose une id�e int�ressante qui consiste � supprimer les voisins inutiles. Cette m�thode s'applique dans le cas d'une classification � l'aide de plus proches voisins. A un �l�ment � classer, cette m�thode attribue la classe du point le plus proche, il faut donc a priori calculer les distances du point en question � tous ceux d�j� class�s. Certains points de cet ensemble ont une forte "capacit� d'attraction"~: ils sont le centre d'une r�gion dans laquelle seuls des points de la m�me classe figurent. Plus concr�tement, soient un point $y$ � classer et une base de points class�s not� $\vecteur{x_1}{x_N}$ de classe $\vecteur{c\pa{x_1}}{c\pa{x_N}}$, on suppose que $x_i$ et $x_j$ sont deux points, enfin, on d�finit~:

        \begin{eqnarray}
        y^*                     = \arg \min \acc{ d\pa{x_k, y} \sac 1 \infegal k \infegal n }
        \end{eqnarray}

On suppose �galement que $x_i$ est un point de forte capacit� et que~:

        \begin{eqnarray}
        \forall y, \; y^* = x_i \Longrightarrow \exists l \text{ tel que } 
                                        x_l = \arg \min \acc{ d\pa{x_k, y} \sac k \neq i } \text{ et } c\pa{x_l} = c\pa{x_i}
        \end{eqnarray}

Autrement dit, $x_i$ est un point attracteur si quel que soit le point $y$ proche de $x_i$, il sera toujours possible de trouver un voisin de $x_i$ proche et $y$ et appartenant � la m�me classe. Dans ce cas, il ne sert � rien de calculer la distance de $x_i$ � $y$, le point $x_i$ peut alors �tre �limin� de la base des points class�s. Il reste maintenant � traiter la base de points class�s de mani�re � en garder le moins possible. De cette fa�on, l'ensemble des points class�s ne gardera que des points situ�s pr�s des fronti�res entre classes.

L'article \citeindex{Wu2002} d�finit le rayon d'un point~:

        \begin{eqnarray}
        r\pa{x}  = \max\acc { 0, \max \acc{ d\pa{x, y} \sac c\pa{x} = c\pa{y} } }
        \end{eqnarray}
        
L'algorithme de suppression des points attracteurs est le suivant~:

            \begin{xalgorithm}{suppression des points attracteurs}
            Soit $\Gamma$ un seuil positif, les notations sont celles utilis�es dans les paragraphes qui pr�c�dent, 
            on note �galement $\Omega$ l'ensemble des points class�s.
            
            \begin{xalgostep}{calcul des rayons}\label{space_metric_attracteur_step_a}
            Pour chaque $x \in \Omega$, on calcule $r\pa{x}$, et on d�signe par $x^*$ le point qui v�rifie~:
            $r\pa{x^*} = \underset{x \in \Omega} {\max} \; { r\pa{x} }$
            \end{xalgostep}
            
            \begin{xalgostep}{suppression}
            Si $r\pa{x^*} \supegal \Gamma$, alors le point $x^*$ est supprim� de l'ensemble $\Omega$, et retour 
            � l'�tape~\ref{space_metric_attracteur_step_a}. Dans le cas contraire, l'algorithme s'arr�te.
            \end{xalgostep}
            
            \end{xalgorithm}
            


\begin{xremark}{m�thode approch�e}
Le fait de supprimer les points dont le rayon attracteur est sup�rieur � un certain seuil peut entra�ner une modification de la classification si ce seuil est trop petit.
\end{xremark}

\indexfr{LVQ}\indexfrr{prototype}{LVQ}
Cette m�thode poursuit le m�me objectif que celui des m�thodes LVQ\seeannex{clas_super_principe_lvq}{LVQ} ou Learning Vector Quantization qui permettent de r�duire un ensemble de points utilis�s dans une classification de plus proches voisins � un ensemble de prototypes.



    
\subsection{Lien vers la classification}


D�terminer le voisinage d'un point est un passage oblig� lorsqu'on applique une classification � l'aide de plus proches voisins puisque chaque �l�ment est class� � partir des classes de ses voisins\seeannex{clas_super_kppv_simple}{classification k-PPV}. Toutefois, les r�sultats obtenus par cette m�thode d�pendent fortement de la distance utilis�e. Sans a priori sur celle-ci, c'est souvent une distance euclidienne qui est choisie. 

Dans le cas des espaces vectoriels, il est possible d'utiliser une distance pond�rant diff�remment chaque dimension et d'estimer cette pond�ration � partir d'un �chantillon repr�sentatif du probl�me de classification � r�soudre. Deux m�thodes sont pr�sent�es aux chapitre~\ref{classification_graphem_carac_dist} et~\ref{classification_distance_voisinage}.

















\newpage


\firstpassagedo{
    \begin{thebibliography}{99}
    \input{space_metric_biblio.tex}
    \end{thebibliography}
}

\input{../../common/livre_table_end.tex}%
\input{../../common/livre_end.tex}%

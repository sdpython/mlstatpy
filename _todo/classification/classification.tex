\input{../../common/livre_begin.tex}
\firstpassagedo{\input{classification_titre.tex}}
\input{../../common/livre_table_begin.tex}
\firstpassagedo{\input{classification_chapter.tex}}


Cette annexe recense diff�rents moyens d'effectuer une classification non supervis�e et de d�terminer le nombre de classes appropri�.

\label{classification_non_supervisee}













\subsection{Neural gas}
\indexfr{neural gas}


\indexsee{quantification vectorielle}{LVQ}
\indexfr{LVQ}
\indexsee{learning vector quantization}{LVQ}

Cette m�thode propos�e dans~\citeindex{Martinetz1993} constitue une m�thode non supervis�e de quantification vectorielle (learning vector quantization, LVQ). Toutefois, elle peut aussi �tre consid�r�e comme une extension de la m�thode RPCL vue au paragraphe~\ref{class_rpcl}. L'article \citeindex{Camastra2003} l'applique dans le cadre de reconnaissance caract�re et le compare aux diff�rents algorithmes LVQ~(1,2,3) et aux cartes de Kohonen (voir paragraphe~\ref{classification_carte_kohonen}).


        \begin{xalgorithm}{Neural Gas}
        \label{classif_algo_neural_gas}
        Soient $\vecteur{X_1}{X_N}$, $N$ vecteurs � classer et $T$ classes de centres $\vecteur{C_1}{C_T}$. 
        Soient quatre r�els $\epsilon_i$,  $\epsilon_f$, $\lambda_i$, $\lambda_f$ et un nombre 
        d'it�rations maximum $t_f$ (des valeurs pratiques pour ces param�tres sont donn�es 
        dans~\citeindex{Martinetz1993}).

        \begin{xalgostep}{initialisation}
        Tirer al�atoirement les centres $\vecteur{C_1}{C_T}$. \\
        \end{xalgostep}

        \begin{xalgostep}{mise � jour} \label{class_neural_gas_step_2}
        Choisir al�atoirement un point $X_i$. \\
        Classer les centres $C_k$ par proximit� croissante de $X_i$ de sorte que~:
        $d\pa{X_i,C_{\sigma\pa{1}}} \infegal ... \infegal d\pa{X_i,C_{\sigma\pa{T}}}$ \\
        \begin{xfor}{j}{1}{C}
        $
        \begin{array}{lcl}
        C_j^{t+1} &\longleftarrow&  C_j^t +      \epsilon_j \pa{\dfrac{\epsilon_f}{\epsilon_j}}^{\frac{t}{t_f}} \; 
                                                                                    exp\pa{
                                                                                            - \biggcro{ \sigma\pa{j} - 1 }  
                                                                                            \cro{ \lambda_j \pa{\dfrac{\lambda_f}{\lambda_j}}^{\frac{t}{t_f}} } ^{-1}
                                                                                    }
                                                                                    \; \pa{ X_i - C_j^t    }
        \end{array}                                                                
        $
        \end{xfor} \\
        $ t \longleftarrow t+1$
        \end{xalgostep}

        \begin{xalgostep}{terminaison} \label{class_rpcl_step_3}
        si $t < t_f$ alors retour � l'�tape~\ref{class_neural_gas_step_2}
        \end{xalgostep}

        \end{xalgorithm}


Cet algorithme ressemble � celui des cartes de Kohonen (paragraphe~\ref{classification_carte_kohonen}) sans toutefois imposer de topologie entre les diff�rentes classes. Il ressemble �galement � l'algorithme RPCL~(\ref{classif_algo_rpcl}) � ceci pr�s que lorsqu'un point $X_i$ est choisi al�atoirement, tous les centres des classes sont rapproch�s � des degr�s diff�rents alors que l'algorithme RPCL rapproche le centre le plus proche et repousse le second centre le plus proche.



















\subsection{Classification ascendante hi�rarchique}
\label{classification_ascendante_hierarchique_CAH}
\indexfrr{classification}{ascendante hi�rarchique (CAH)}
\indexfr{CAH}

Comme l'algorithme des centres mobiles (\ref{algo_centre_mobile}), cet algorithme permet �galement d'effectuer une classification non supervis�e des donn�es. Soit un ensemble $E = \vecteur{x_1}{x_N}$ � classer, on suppose �galement qu'il existe une distance entre ces �l�ments not�e $d\pa{x,y}$. De cette distance, on en d�duit un crit�re ou une inertie entre deux parties ne poss�dant pas d'intersection commune. Par exemple, soient deux parties non vide $A$ et $B$ de $E$ telles    que $A \cap B = \emptyset$, on note $\abs{A}$ le nombre d'�l�ments de $A$. Voici divers crit�res possibles~:

        \begin{eqnarray*}
        \text{le diam�tre } D\pa{A,B}  &=& \max \acc{ d\pa{x,y} \sac x,y \in A \cup B } \\
        \text{l'inertie }     I\pa{A,B}  &=& \frac{1}{\abs{A \cup B}} \; \summyone{x \in A \cup B} \; d\pa{x,G_{A \cup B}} \\
                                && \text{o� } G_{A \cup B} \text{ est le barycentre de la partie } A \cup B
        \end{eqnarray*}


On note $C\pa{A,B}$ le crit�re de proximit� entre deux parties, la classification ascendante hi�rarchique consiste � regrouper d'abord les deux parties minimisant le crit�re $C\pa{A,B}$.


        \begin{xalgorithm}{CAH}
        Les notations sont celles utilis�es dans les paragraphes pr�c�dents. 
        Soit l'ensemble des singletons $P = \vecteur{\acc{x_1}}{\acc{x_N}}$.

        \begin{xalgostep}{initialisation}
        $t \longrightarrow 0$
        \end{xalgostep}

        \begin{xalgostep}{choix des deux meilleures parties}\label{classif_cah_step_a}
        Soit le couple de parties $\pa{A,B}$ d�fini par~:
                $$\begin{array}{l}
                C\pa{A,B} = \min \acc{ C\pa{M,N} \sac M,N \in P, \text{ et } M \neq N }
                \end{array}$$
        \end{xalgostep}

        \begin{xalgostep}{mise � jour}
        $\begin{array}{lll}
        c_t &\longleftarrow& C\pa{A,B} \\
        P     &\longleftarrow& P - \acc{A} -     \acc{B} \\
        P     &\longleftarrow& P \cup \acc{ A \cup B}
        \end{array}$
        Tant que $P \neq \acc{E}$, $t \longleftarrow t+1$ et retour � l'�tape~\ref{classif_cah_step_a}.
        \end{xalgostep}
        
        \end{xalgorithm}

L'�volution de l'ensemble des parties $P$ est souvent repr�sent�e par un graphe comme celui de la figure~\ref{classification_fig_cah}. C'est ce graphe qui permet de d�terminer le nombre de classes appropri� � l'ensemble $E$ par l'interm�diaire de la courbe $\pa{t,c_t}$. Le bon nombre de classe est souvent situ� au niveau d'un changement de pente ou d'un point d'inflexion de cette courbe. Cette m�thode est d�crite de mani�re plus compl�te dans \citeindex{Saporta1990}.


        \begin{figure}[ht]
        $$\begin{tabular}{|c|}\hline
        \includegraphics[height=5cm, width=7cm]{\filext{../classification/image/cah_ex}}
        %\filefig{../classification/fig_cah}
        \\ \hline \end{tabular}$$
        \caption{ Repr�sentation classique de l'arbre obtenu par une CAH. Chaque palier indique un regroupement
                            de deux parties et la valeur du crit�re de proximit� correspondant.}
        \indexfr{CAH}
        \label{classification_fig_cah}
        \end{figure}
        






\subsection{Classification � partir de graphes}
\label{classification_graphe_voisinage}
\indexfr{graphe}
\indexfr{Kruskal}
\indexfrr{arbre}{poids minimal}


L'article \citeindex{Bandyopadhyay2004} propose une m�thode qui s'appuie sur les graphes et permettant de classer automatiquement un nuage de points organis� sous forme de graphe. Chaque �l�ment est d'abord reli� � ses plus proches voisins, les arcs du graphe obtenus sont pond�r�s par la distance reliant les �l�ments associ�s chacun � un n\oe ud. Les ar�tes sont ensuite class�es par ordre croissant afin de d�terminer un seuil au del� duquel ces arcs relient deux �l�ments appartenant � deux classes diff�rentes. Ceci m�ne � l'algorithme~\ref{classification_graphe_band}. La figure~\ref{classification_fig_Bandyopadhyay2004} illustre quelques r�sultats obtenus sur des nuages de points difficiles � segmenter par des m�thodes apparent�es aux nu�es dynamiques.

        \begin{xalgorithm}{classification par graphe de voisinage}
        \label{classification_graphe_band}
        On d�signe par $e_{ij}$ les arcs du graphe $G(S,A)$ 
        reliant les �l�ments $i$ et $j$ et pond�r�s par $d_{ij} = d\pa{x_i,x_j}$ la distance
        entre les �l�ments $x_i$ et $x_j$ de l'ensemble $\vecteur{x_1}{x_N}$. $S$ d�signe l'ensemble
        des sommets et $A$ l'ensemble des arcs $A = \pa{e_{ij}}_{ij}$. 
        On num�rote les ar�tes de $1$
        � $N^2$ de telle sorte qu'elles soient tri�es~: $w_{\sigma(1)} \infegal w_{\sigma(2)} \infegal ... \infegal
        w_{\sigma(N^2)}$. On �limine dans cette liste les arcs de m�me poids, on construit donc la fonction $\sigma'$
        de telle sorte que~: $w_{\sigma'(1)} < w_{\sigma'(2)} < ... < w_{\sigma'(n)}$ avec $n \infegal N^2$. On pose
        $\lambda = 2$.
        
        \begin{xalgostep}{d�termination de l'ensemble des arcs � conserver}
        On d�signe par $X$ l'ensemble des arcs � conserver. $X = A$. Si $w_{\sigma'(n)} < \lambda w_{\sigma'(1)}$ alors $X$
        est inchang� et on passe � l'�tape suivante. Sinon, on construit la suite 
        $\delta_i = w_{\sigma'(i+1)} - w_{\sigma'(i)}$ pour $i \in \ensemble{1}{n-1}$. La suite $\delta_{\phi(i)}$
        correspond � la m�me suite tri�e~: $\delta_{\phi(1)} \infegal  ... \infegal \delta_{\phi(n-1)}$. On d�finit 
        $t = \frac{\delta_{\phi(1)} + \delta_{\phi(n-1)}} {2}$. On d�finit alors le seuil $\alpha$ tel que~:
                    $$
                    \alpha = \min \acc{ w_{\sigma(i)} \sac
                                                            1 \infegal i \infegal n-1 \text{ et } 
                                                            w_{\sigma'(i+1)} - w_{\sigma'(i)} \supegal t \text{ et }
                                                            w_{\sigma'(i)} \supegal \lambda w_{\sigma'(1)}}
                    $$
        Si $\alpha$ n'est pas d�fini, $X$ est inchang� et on passe � l'�tape suivante, sinon~:
                    $$
                    X = \acc{ e_{ij} \in A \sac d_{ij} \infegal \alpha}
                    $$
        \end{xalgostep}
        
        \begin{xalgostep}{d�termination des classes}
        Si $X = A$ alors l'algorithme ne retourne qu'une seule classe. Dans le cas contraire,
        on extrait du graphe $G(S,X)$ l'ensemble des composantes connexes $\ensemble{C_1}{C_p}$ o�
        $p$ d�signe le nombre de composantes connexes du graphe.
        Si $p > \sqrt{ \card{X}}$, l'algorithme m�ne � une sur-segmentation, on ne retourne � nouveau qu'une seule
        classe. Dans le cas contraire, on applique ce m�me algorithme � chacune des composantes connexes $(C_k)$
        extraites du graphe. 
        \end{xalgostep}
        
        L'algorithme est donc appliqu� de mani�re r�cursive tant qu'un sous-ensemble
        peut �tre segment�.
        \end{xalgorithm}



        \begin{figure}[p]
        $$\begin{tabular}{|cc|cc|}\hline
        $(a)$ & \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band21}} &
        \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band22}} & $(d)$ \\ \hline
        $(b)$ & \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band23}} &
        \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band24}} & $(e)$ \\ \hline
        $(c)$ & \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band25}} &
        \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band26}} & $(f)$ 
        \\ \hline \end{tabular}$$
        \caption{    Figures extraites de \citeindexfig{Bandyopadhyay2004}, 
                            diff�rents nuages de points bien segment�s par l'algorithme~\ref{classification_graphe_band}
                            et de mani�re �vidente impossible � traiter avec des m�thodes apparent�es aux nu�es dynamiques
                            puisque les classes obtenues ne sont pas convexes. L'image $(a)$ permet de v�rifier 
                            qu'un nuage compact distribu� selon une loi normale n'est pas segment�. L'image $(b)$ 
                            repr�sente un nuage compos�e de deux classes bien segment�es. Les autres images montrent
                            des probl�mes o� les classes ne sont plus circulaires $(d)$ ou non convexes $(c)$, $(e)$, $(f)$.
                            }
        \indexfrr{classification}{voisinage}
        \indexfrr{classification}{graphe}
        \label{classification_fig_Bandyopadhyay2004}
        \end{figure}

L'algorithme~\ref{classification_graphe_band}, puisqu'il est appliqu� r�cursivement, permet de construire une hi�rarchie de classes comme celle obtenue par une classification ascendante hi�rarchique\seeannex{classification_ascendante_hierarchique_CAH}{classification ascendante hi�rarchique} mais cette fois-ci, l'arbre final est obtenu depuis la racine jusqu'aux feuilles. Le seuil caract�risant les cas de sur-segmentation (ici $\sqrt{X}$) est celui choisi dans l'article \citeindex{Bandyopadhyay2004} permettant de traiter les cas de la figure~\ref{classification_fig_Bandyopadhyay2004}. Celui-ci peut �tre modifi� en fonction du probl�me � r�soudre. 

Cet article pr�cise aussi que l'algorithme peut former des classes de tr�s petites tailles qui devront �tre agr�g�es avec leurs voisines � moins que celles-ci ne soient trop �loign�es, la distance entre classes �tant ici la distance minimum entre leurs �l�ments. La r�gle choisie dans l'article \citeindex{Bandyopadhyay2004} est que une classe sera unie � sa voisine si le diam�tre de la premi�re est inf�rieur � $\mu$ fois la distance qui les s�pare, avec $\mu = 3 \supegal 2$. Ce param�tre peut diff�rer selon les probl�mes.

%-----------------------------------------------------------------------------------------------------------------------
\section{Prolongations}
%-----------------------------------------------------------------------------------------------------------------------

\subsection{Classe sous-repr�sent�e}

\indexfrr{classification}{classe sous-repr�sent�e}

Ce paragraphe regroupe quelques pistes de lecture. Les remarques qui suivent s'appliquent de pr�f�rence � une classification supervis�e mais peuvent �tre �tendues au cas non supervis�. Le premier article \citeindex{Barandela2003} r�sume les id�es concernant le cas d'un probl�me de classification incluant une classe sous-repr�sent�e. Par exemple, pour un probl�me � deux classes~A et~B lorsque~A regroupe 98\% des exemples, r�pondre~A quelque soit l'exemple correspond � une erreur de 2\%. Avec plus de 2\% d'erreur, une m�thode de classification serait moins performante et pourtant les classes sous-repr�sent�es favorise cette configuration. Diverses m�thodes sont utilis�es pour contrecarrer cet inconv�nient comme la pond�ration des exemples sous-repr�sent�s, la multiplication de ces m�mes exemples, bruit�es ou non bruit�es ou encore la r�duction des classes sur-repr�sent�es � un �chantillon repr�sentatif. Cette derni�re option est celle discut�e par l'article \citeindex{Barandela2003} qui envisage diff�rentes m�thodes de s�lection de cet �chantillon.








\subsection{Apprentissage d'une distance}


\label{classification_graphem_carac_dist}
\indexfrr{caract�ristiques}{distance}
\indexfrr{distance}{apprentissage}
\indexfrr{apprentissage}{distance}

Jusqu'� pr�sent, seule la classification a �t� trait�e mais on peut se demander quelle est la distance la mieux adapt�e � une classification. La distance euclidienne accorde un poids �gal � toutes les dimensions d'un vecteur. On peut se demander quelle est la pond�ration optimale pour un probl�me de classification donn�. On d�finit une distance $d_W$ avec $W = \vecteur{W_1}{W_d}$ pond�rant les dimensions de mani�re non uniforme~:

            \begin{eqnarray}
            d_W\pa{X^1,X^2} = \summy{k=1}{d} \, W_k^2 \, \pa{X^1_k - X^2_k}^2
            \end{eqnarray}
            
\indexfr{prototype}            

Il reste � d�terminer le vecteurs de poids $W = \vecteur{W_1}{W_d}$ en s'inspirant par exemple de la m�thode d�velopp�e par \citeindex{Waard1995}. On consid�re $P$ vecteurs aussi appel�s prototypes et not�s $\vecteur{X^1}{X^p}$ extrait du nuage $\vecteur{X^1}{X^N}$. On note ensuite pour tout $p \in \ensemble{1}{P}$~:

        \begin{eqnarray}
        y_p\pa{X} = \frac{1}{1 + \exp\pa{d_{W}\pa{X,X^p} + b}}
        \end{eqnarray}
        
On cherche � minimiser le crit�re~:

        \begin{eqnarray}
        E = \summyone{\pa{p,l} \in A} \pa{y_p\pa{X_l} - d_{pl}}^2 \text{ o� } 
                A = \ensemble{1}{P} \times \ensemble{1}{N}
        \end{eqnarray}
                
\indexfr{r�seau de neurones}                

Cette minimisation peut �tre effectu�e par une descente de gradient ou dans un algorithme similaire � ceux utilis�s pour l'apprentissage des r�seaux de neurones (voir paragraphe~\ref{rn_section_train_rn}). Chaque prototype $X_p$ appartient � une classe $C_p$, les coefficients $d_{pl} \in \cro{0,1}$ sont choisis de mani�re � d�crire l'appartenance du vecteur $X_l$ � la classe $C_p$. 

Cette classification pourrait �tre obtenue � partir d'une classification non supervis�e (centres mobiles, classification ascendante hi�rarchique) mais cela suppose de disposer d�j� d'une distance (comme celle par exemple d�crite au paragraphe~\ref{reco_graphem_contour}). Il est possible de r�p�ter le processus jusqu'� convergence, la premi�re classification est effectu�e � l'aide d'une distance euclidienne puis une seconde distance est ensuite apprise gr�ce � la m�thode d�velopp�e dans ce paragraphe. Cette seconde distance induit une nouvelle classification qui pourra � son tour d�finir une troisi�me distance. Ce processus peut �tre r�p�t� jusqu'� la classification n'�volue plus.













\subsection{Classification � partir de voisinages}
\label{classification_distance_voisinage}

\indexfrr{classification}{voisinage}
\indexfrr{voisinage}{classification}


L'id�e de cette classification est d�velopp�e dans l'article \citeindex{ZhangYG2004}. Elle repose sur la construction d'un voisinage pour chaque �l�ment d'un ensemble $E = \ensemble{x_1}{x_n}$ � classer. La classification est ensuite obtenue en regroupant ensemble les voisinages ayant une intersection commune. L'objectif �tant de proposer une r�ponse au probl�me d�crit par la figure~\ref{classification_fig_zhang1}.


        \begin{figure}[ht]
        $$\begin{tabular}{|c|}\hline
        \includegraphics[height=5cm, width=6cm]{\filext{../classification/image/zhang1}}
        \\ \hline \end{tabular}$$
        \caption{    Figure extraite de \citeindexfig{ZhangYG2004}, probl�me classique de classification consistant
                            � s�parer deux spirales imbriqu�es l'une dans l'autre.}
        \indexfrr{classification}{voisinage}
        \label{classification_fig_zhang1}
        \end{figure}



Pour chaque $x_i \in E$, on d�finit son voisinage local $\omega_i = \ensemble{x_{i_1}}{x_{i_K}}$. $K$ est le nombre de voisins et ceux-ci sont class�s par ordre de proximit� croissante. Par la suite, $x_i$ sera �galement not� $x_{i_0}$. On d�finit ensuite la matrice de covariance $S_i$ locale associ�e � $\omega_i$~:

            \begin{eqnarray}
            m_i = \frac{1}{K+1} \; \summy{k=0}{K} x_{i_k} \text{ et } 
            S_i = \frac{1}{K+1} \; \summy{k=0}{K} \pa{x_{i_k} - m_i } \pa{x_{i_k} - m_i }'
            \label{classif_zhang_eq1}
            \end{eqnarray}

Le vecteur $\lambda_i = \vecteur{\lambda_{i,1}}{\lambda_{i,d}}'$ v�rifiant $\lambda_{i,1} \supegal ... \supegal \lambda_{i,d}$, $d$ est la dimension de l'espace vectoriel. L'\emph{adaptibilit�} $a_i$  de l'ensemble $\omega_i$ est d�finie par~: \indexfr{adaptabilit�}

            \begin{eqnarray}
            \overline{\lambda_{i,j}}  = \frac{1}{K} \; \summyone{t \in \ensemble{i_1}{i_K} } \lambda_{t,j} \text{ et }
            a_i = \frac{1}{d} \; \summy{j=1}{d} \frac{ \lambda_{i,j} } { \overline { \lambda_{i,j}} }
            \label{classif_zhang_eq2}
            \end{eqnarray}
            
On note �galement~:

            \begin{eqnarray}
            E\pa{a_i} = \frac{1}{N} \; \summy{i=1}{N} a_i \text{ et } 
            D\pa{a_i} = \sqrt{ \frac{1}{N} \; \summy{i=1}{N} \pa{ a_i - E\pa{a_i} }^2 }
            \label{classif_zhang_eq3}
            \end{eqnarray}
            
            
Dans un premier temps, les voisinages d�termin�s par cette m�thode vont �tre nettoy�s des voisins ind�sirables. Ce syst�me est souvent repr�sent� sous forme de graphe, chaque n\oe ud repr�sente un �l�ment, chaque arc d�termine l'appartenance d'un �l�ment au voisinage d'un autre. Ces graphes sont appel�s "\emph{mutual neighborhood graph}" ou \emph{graphe des voisinages mutuels}.
\indexfr{mutual neighborhood graph}
\indexfr{graphe des voisinages mutuels}


            \begin{xalgorithm}{nettoyage des voisinages}
            Les notations utilis�es sont celles des expressions (\ref{classif_zhang_eq1}), 
            (\ref{classif_zhang_eq2}), (\ref{classif_zhang_eq3}).
            
            \begin{xalgostep}{estimation}\label{classif_algo_zhang_1}
            Les valeurs $a_i^l$ sont calcul�es pour chaque ensemble $\omega_i$ priv� de $x_{i_l}$ pour �l�ment
            de l'ensemble $\omega_i$.
            \end{xalgostep}

            \begin{xalgostep}{suppression}
            Si $a_i^l > E\pa{a_i} + D\pa{a_i}$, alors l'algorithme s'arr�te. Sinon, l'�l�ment $x_{i_s}$ correspondant
            � la plus petite valeur $x_{i_l}$ est supprim�e de l'ensemble $\omega_i$. 
            On retourne ensuite � l'�tape~\ref{classif_algo_zhang_1}.
            \end{xalgostep}
            
            \end{xalgorithm}


\indexfr{composante connexe}\indexfrr{distance}{euclidienne}

La classification correspond aux composantes connexes du graphe nettoy� qui d�termine par ce biais le nombre de classes. L'article sugg�re �galement d'associer � chaque �l�ment $x_i$ le vecteur $\pa{x_i, \beta \lambda_i}'$ o� $\beta$ est un param�tre de normalisation. Le vecteur $\beta \lambda_i$ caract�rise le voisinage. Ainsi, la distance entre deux points d�pend � la fois de leur position et de leur voisinage. Les auteurs proposent �galement d'autres distances que la distance euclidienne. Il reste toutefois � d�terminer les param�tres $K$ et $\beta$. 





\subsection{Mod�lisation de la densit� des observations}


\label{classification_modelisation_densite}
\indexfrr{densit�}{semi-param�trique}

L'article \citeindex{Hoti2004} pr�sente une mod�lisation semi-param�trique. Soit $Z = \pa{X,Y}$ une variable al�atoire compos�e du couple $\pa{X,Y}$. La densit� de $z$ est exprim�e comme suit~:

        \begin{eqnarray}
        f_{X,Y}(x,y) = f_{Y | X=x}(y) \, f_X(x)
        \end{eqnarray}

Dans cet article, la densit� $f_X\pa{x}$ est estim�e de fa�on non param�trique tandis que $f_{Y|X}\pa{y}$ est mod�lis�e par une loi gaussienne. On note $p$ la dimension de $X$ et $q$ celle de $Y$. On note $K_H \pa{x} = \frac{1}{\det H} K \pa{H^{-1} X}$ o� $H \in M_p\pa{\mathbb{R}}$ est une matrice carr�e d�finie strictement positive et $K$ un noyau v�rifiant $\int_{\mathbb{R}^p} K\pa{x} dx = 1$. $K$ peut par exemple �tre une fonction gaussienne. Les notations reprennent celles du paragraphe~\ref{modification_janvier_2004_new} (page~\pageref{modification_janvier_2004_new}). On suppose �galement que la variable $Y | X=x \sim \loinormale{\mu(x)}{\sigma(x)}$. Par cons�quent, la densit� de la variable $Z =\pa{X,Y}$ s'exprime de la fa�on suivante~:


        \begin{eqnarray}
        f_{X,Y}(x,y) =  \frac{ f_X(x) } { \sqrt{ \pa{2 \pi}^q \, \det \sigma^2(x) } } \;
                                        \exp \pa{ - \frac{1}{2} \cro{y - \mu(x)} \, \sigma^{-1}(x) \, \cro{y - \mu(x)}' }
        \end{eqnarray}


La densit� $f_X$ est estim�e avec un estimateur � noyau � l'aide de l'�chantillon $\pa{X_i,Y_i}_{1 \infegal i \infegal N}$~:

        
        \begin{eqnarray}
        \widehat{f_X} (x) = \frac{1}{N} \; \summy{i=1}{N} K_{H} \pa{ x - X_i}
        \end{eqnarray}

On note~:

        \begin{eqnarray}
        W_H\pa{x - X_i} =  \frac{K_H\pa{x - X_i}} {\summy{i=1}{N} K_H\pa{x - X_i} }
        \end{eqnarray}
        
Les fonctions $\mu(x)$ et $\sigma(x)$ sont estim�es � l'aide d'un estimateur du maximum de vraisemblance~:
        
        \begin{eqnarray}
        \widehat{\mu}(x)         &=& \summy{i=1}{n} W_H\pa{x - X_i} \, Y_i \\
        \widehat{\sigma}(x) &=& \summy{i=1}{n} W_H\pa{x - X_i} \, 
                                                        \cro{Y_i - \widehat{\mu}(x)}' \cro{Y_i - \widehat{\mu}(x)} 
        \end{eqnarray}

Le paragraphe~\ref{modification_janvier_2004_new} (page~\pageref{modification_janvier_2004_new}) discute du choix d'une matrice $H$ appropri�e (voir �galement \citeindex{Silverman1986}). En ce qui concerne le probl�me de classification �tudi�e ici, la variable $X$ est simplement discr�te et d�signe la classe de la variable $Y$. Cette m�thode est proche de celle d�velopp�e au paragraphe~\ref{classification_melange_loi_normale} � la seule diff�rence que l'information $X_i$ est ici connue. L'int�r�t de cette m�thode est sa g�n�ralisation au cas o� $X$ est une variable continue comme par exemple un vecteur form� des distances du point $X_i$ aux centres des classes d�termin�es par un algorithme de classification non supervis�e. L'article \citeindex{Hoti2004} discute �galement d'un choix d'une densit� $f_X$ param�trique.









\firstpassagedo{
    \begin{thebibliography}{99}
    \input{classification_bibliographie.tex}
    \end{thebibliography}
}


\input{../../common/livre_table_end.tex}
\input{../../common/livre_end.tex}

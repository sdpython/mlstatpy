\input{../../common/livre_begin.tex}
\firstpassagedo{\input{classification_titre.tex}}
\input{../../common/livre_table_begin.tex}
\firstpassagedo{\input{classification_chapter.tex}}


Cette annexe recense différents moyens d'effectuer une classification non supervisée et de déterminer le nombre de classes approprié.

\label{classification_non_supervisee}













\subsection{Neural gas}
\indexfr{neural gas}


\indexsee{quantification vectorielle}{LVQ}
\indexfr{LVQ}
\indexsee{learning vector quantization}{LVQ}

Cette méthode proposée dans~\citeindex{Martinetz1993} constitue une méthode non supervisée de quantification vectorielle (learning vector quantization, LVQ). Toutefois, elle peut aussi être considérée comme une extension de la méthode RPCL vue au paragraphe~\ref{class_rpcl}. L'article \citeindex{Camastra2003} l'applique dans le cadre de reconnaissance caractère et le compare aux différents algorithmes LVQ~(1,2,3) et aux cartes de Kohonen (voir paragraphe~\ref{classification_carte_kohonen}).


		\begin{xalgorithm}{Neural Gas}
		\label{classif_algo_neural_gas}
		Soient $\vecteur{X_1}{X_N}$, $N$ vecteurs à classer et $T$ classes de centres $\vecteur{C_1}{C_T}$. 
		Soient quatre réels $\epsilon_i$,  $\epsilon_f$, $\lambda_i$, $\lambda_f$ et un nombre 
		d'itérations maximum $t_f$ (des valeurs pratiques pour ces paramètres sont données 
		dans~\citeindex{Martinetz1993}).

		\begin{xalgostep}{initialisation}
		Tirer aléatoirement les centres $\vecteur{C_1}{C_T}$. \\
		\end{xalgostep}

		\begin{xalgostep}{mise à jour} \label{class_neural_gas_step_2}
		Choisir aléatoirement un point $X_i$. \\
		Classer les centres $C_k$ par proximité croissante de $X_i$ de sorte que~:
		$d\pa{X_i,C_{\sigma\pa{1}}} \infegal ... \infegal d\pa{X_i,C_{\sigma\pa{T}}}$ \\
		\begin{xfor}{j}{1}{C}
		$
		\begin{array}{lcl}
		C_j^{t+1} &\longleftarrow&  C_j^t +  	\epsilon_j \pa{\dfrac{\epsilon_f}{\epsilon_j}}^{\frac{t}{t_f}} \; 
																					exp\pa{
																							- \biggcro{ \sigma\pa{j} - 1 }  
																							\cro{ \lambda_j \pa{\dfrac{\lambda_f}{\lambda_j}}^{\frac{t}{t_f}} } ^{-1}
																					}
																					\; \pa{ X_i - C_j^t	}
		\end{array}																
		$
		\end{xfor} \\
		$ t \longleftarrow t+1$
		\end{xalgostep}

		\begin{xalgostep}{terminaison} \label{class_rpcl_step_3}
		si $t < t_f$ alors retour à l'étape~\ref{class_neural_gas_step_2}
		\end{xalgostep}

		\end{xalgorithm}


Cet algorithme ressemble à celui des cartes de Kohonen (paragraphe~\ref{classification_carte_kohonen}) sans toutefois imposer de topologie entre les différentes classes. Il ressemble également à l'algorithme RPCL~(\ref{classif_algo_rpcl}) à ceci près que lorsqu'un point $X_i$ est choisi aléatoirement, tous les centres des classes sont rapprochés à des degrés différents alors que l'algorithme RPCL rapproche le centre le plus proche et repousse le second centre le plus proche.



















\subsection{Classification ascendante hiérarchique}
\label{classification_ascendante_hierarchique_CAH}
\indexfrr{classification}{ascendante hiérarchique (CAH)}
\indexfr{CAH}

Comme l'algorithme des centres mobiles (\ref{algo_centre_mobile}), cet algorithme permet également d'effectuer une classification non supervisée des données. Soit un ensemble $E = \vecteur{x_1}{x_N}$ à classer, on suppose également qu'il existe une distance entre ces éléments notée $d\pa{x,y}$. De cette distance, on en déduit un critère ou une inertie entre deux parties ne possédant pas d'intersection commune. Par exemple, soient deux parties non vide $A$ et $B$ de $E$ telles	que $A \cap B = \emptyset$, on note $\abs{A}$ le nombre d'éléments de $A$. Voici divers critères possibles~:

		\begin{eqnarray*}
		\text{le diamètre } D\pa{A,B}  &=& \max \acc{ d\pa{x,y} \sac x,y \in A \cup B } \\
		\text{l'inertie } 	I\pa{A,B}  &=& \frac{1}{\abs{A \cup B}} \; \summyone{x \in A \cup B} \; d\pa{x,G_{A \cup B}} \\
								&& \text{où } G_{A \cup B} \text{ est le barycentre de la partie } A \cup B
		\end{eqnarray*}


On note $C\pa{A,B}$ le critère de proximité entre deux parties, la classification ascendante hiérarchique consiste à regrouper d'abord les deux parties minimisant le critère $C\pa{A,B}$.


		\begin{xalgorithm}{CAH}
		Les notations sont celles utilisées dans les paragraphes précédents. 
		Soit l'ensemble des singletons $P = \vecteur{\acc{x_1}}{\acc{x_N}}$.

		\begin{xalgostep}{initialisation}
		$t \longrightarrow 0$
		\end{xalgostep}

		\begin{xalgostep}{choix des deux meilleures parties}\label{classif_cah_step_a}
		Soit le couple de parties $\pa{A,B}$ défini par~:
				$$\begin{array}{l}
				C\pa{A,B} = \min \acc{ C\pa{M,N} \sac M,N \in P, \text{ et } M \neq N }
				\end{array}$$
		\end{xalgostep}

		\begin{xalgostep}{mise à jour}
		$\begin{array}{lll}
		c_t &\longleftarrow& C\pa{A,B} \\
		P 	&\longleftarrow& P - \acc{A} - 	\acc{B} \\
		P 	&\longleftarrow& P \cup \acc{ A \cup B}
		\end{array}$
		Tant que $P \neq \acc{E}$, $t \longleftarrow t+1$ et retour à l'étape~\ref{classif_cah_step_a}.
		\end{xalgostep}
		
		\end{xalgorithm}

L'évolution de l'ensemble des parties $P$ est souvent représentée par un graphe comme celui de la figure~\ref{classification_fig_cah}. C'est ce graphe qui permet de déterminer le nombre de classes approprié à l'ensemble $E$ par l'intermédiaire de la courbe $\pa{t,c_t}$. Le bon nombre de classe est souvent situé au niveau d'un changement de pente ou d'un point d'inflexion de cette courbe. Cette méthode est décrite de manière plus complète dans \citeindex{Saporta1990}.


		\begin{figure}[ht]
		$$\begin{tabular}{|c|}\hline
		\includegraphics[height=5cm, width=7cm]{\filext{../classification/image/cah_ex}}
		%\filefig{../classification/fig_cah}
		\\ \hline \end{tabular}$$
		\caption{ Représentation classique de l'arbre obtenu par une CAH. Chaque palier indique un regroupement
							de deux parties et la valeur du critère de proximité correspondant.}
		\indexfr{CAH}
		\label{classification_fig_cah}
		\end{figure}
		






\subsection{Classification à partir de graphes}
\label{classification_graphe_voisinage}
\indexfr{graphe}
\indexfr{Kruskal}
\indexfrr{arbre}{poids minimal}


L'article \citeindex{Bandyopadhyay2004} propose une méthode qui s'appuie sur les graphes et permettant de classer automatiquement un nuage de points organisé sous forme de graphe. Chaque élément est d'abord relié à ses plus proches voisins, les arcs du graphe obtenus sont pondérés par la distance reliant les éléments associés chacun à un n\oe ud. Les arêtes sont ensuite classées par ordre croissant afin de déterminer un seuil au delà duquel ces arcs relient deux éléments appartenant à deux classes différentes. Ceci mène à l'algorithme~\ref{classification_graphe_band}. La figure~\ref{classification_fig_Bandyopadhyay2004} illustre quelques résultats obtenus sur des nuages de points difficiles à segmenter par des méthodes apparentées aux nuées dynamiques.

		\begin{xalgorithm}{classification par graphe de voisinage}
		\label{classification_graphe_band}
		On désigne par $e_{ij}$ les arcs du graphe $G(S,A)$ 
		reliant les éléments $i$ et $j$ et pondérés par $d_{ij} = d\pa{x_i,x_j}$ la distance
		entre les éléments $x_i$ et $x_j$ de l'ensemble $\vecteur{x_1}{x_N}$. $S$ désigne l'ensemble
		des sommets et $A$ l'ensemble des arcs $A = \pa{e_{ij}}_{ij}$. 
		On numérote les arêtes de $1$
		à $N^2$ de telle sorte qu'elles soient triées~: $w_{\sigma(1)} \infegal w_{\sigma(2)} \infegal ... \infegal
		w_{\sigma(N^2)}$. On élimine dans cette liste les arcs de même poids, on construit donc la fonction $\sigma'$
		de telle sorte que~: $w_{\sigma'(1)} < w_{\sigma'(2)} < ... < w_{\sigma'(n)}$ avec $n \infegal N^2$. On pose
		$\lambda = 2$.
		
		\begin{xalgostep}{détermination de l'ensemble des arcs à conserver}
		On désigne par $X$ l'ensemble des arcs à conserver. $X = A$. Si $w_{\sigma'(n)} < \lambda w_{\sigma'(1)}$ alors $X$
		est inchangé et on passe à l'étape suivante. Sinon, on construit la suite 
		$\delta_i = w_{\sigma'(i+1)} - w_{\sigma'(i)}$ pour $i \in \ensemble{1}{n-1}$. La suite $\delta_{\phi(i)}$
		correspond à la même suite triée~: $\delta_{\phi(1)} \infegal  ... \infegal \delta_{\phi(n-1)}$. On définit 
		$t = \frac{\delta_{\phi(1)} + \delta_{\phi(n-1)}} {2}$. On définit alors le seuil $\alpha$ tel que~:
					$$
					\alpha = \min \acc{ w_{\sigma(i)} \sac
															1 \infegal i \infegal n-1 \text{ et } 
															w_{\sigma'(i+1)} - w_{\sigma'(i)} \supegal t \text{ et }
															w_{\sigma'(i)} \supegal \lambda w_{\sigma'(1)}}
					$$
		Si $\alpha$ n'est pas défini, $X$ est inchangé et on passe à l'étape suivante, sinon~:
					$$
					X = \acc{ e_{ij} \in A \sac d_{ij} \infegal \alpha}
					$$
		\end{xalgostep}
		
		\begin{xalgostep}{détermination des classes}
		Si $X = A$ alors l'algorithme ne retourne qu'une seule classe. Dans le cas contraire,
		on extrait du graphe $G(S,X)$ l'ensemble des composantes connexes $\ensemble{C_1}{C_p}$ où
		$p$ désigne le nombre de composantes connexes du graphe.
		Si $p > \sqrt{ \card{X}}$, l'algorithme mène à une sur-segmentation, on ne retourne à nouveau qu'une seule
		classe. Dans le cas contraire, on applique ce même algorithme à chacune des composantes connexes $(C_k)$
		extraites du graphe. 
		\end{xalgostep}
		
		L'algorithme est donc appliqué de manière récursive tant qu'un sous-ensemble
		peut être segmenté.
		\end{xalgorithm}



		\begin{figure}[p]
		$$\begin{tabular}{|cc|cc|}\hline
		$(a)$ & \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band21}} &
		\includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band22}} & $(d)$ \\ \hline
		$(b)$ & \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band23}} &
		\includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band24}} & $(e)$ \\ \hline
		$(c)$ & \includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band25}} &
		\includegraphics[height=7cm, width=7cm]{\filext{../classification/image/band26}} & $(f)$ 
		\\ \hline \end{tabular}$$
		\caption{	Figures extraites de \citeindexfig{Bandyopadhyay2004}, 
							différents nuages de points bien segmentés par l'algorithme~\ref{classification_graphe_band}
							et de manière évidente impossible à traiter avec des méthodes apparentées aux nuées dynamiques
							puisque les classes obtenues ne sont pas convexes. L'image $(a)$ permet de vérifier 
							qu'un nuage compact distribué selon une loi normale n'est pas segmenté. L'image $(b)$ 
							représente un nuage composée de deux classes bien segmentées. Les autres images montrent
							des problèmes où les classes ne sont plus circulaires $(d)$ ou non convexes $(c)$, $(e)$, $(f)$.
							}
		\indexfrr{classification}{voisinage}
		\indexfrr{classification}{graphe}
		\label{classification_fig_Bandyopadhyay2004}
		\end{figure}

L'algorithme~\ref{classification_graphe_band}, puisqu'il est appliqué récursivement, permet de construire une hiérarchie de classes comme celle obtenue par une classification ascendante hiérarchique\seeannex{classification_ascendante_hierarchique_CAH}{classification ascendante hiérarchique} mais cette fois-ci, l'arbre final est obtenu depuis la racine jusqu'aux feuilles. Le seuil caractérisant les cas de sur-segmentation (ici $\sqrt{X}$) est celui choisi dans l'article \citeindex{Bandyopadhyay2004} permettant de traiter les cas de la figure~\ref{classification_fig_Bandyopadhyay2004}. Celui-ci peut être modifié en fonction du problème à résoudre. 

Cet article précise aussi que l'algorithme peut former des classes de très petites tailles qui devront être agrégées avec leurs voisines à moins que celles-ci ne soient trop éloignées, la distance entre classes étant ici la distance minimum entre leurs éléments. La règle choisie dans l'article \citeindex{Bandyopadhyay2004} est que une classe sera unie à sa voisine si le diamètre de la première est inférieur à $\mu$ fois la distance qui les sépare, avec $\mu = 3 \supegal 2$. Ce paramètre peut différer selon les problèmes.

%-----------------------------------------------------------------------------------------------------------------------
\section{Prolongations}
%-----------------------------------------------------------------------------------------------------------------------

\subsection{Classe sous-représentée}

\indexfrr{classification}{classe sous-représentée}

Ce paragraphe regroupe quelques pistes de lecture. Les remarques qui suivent s'appliquent de préférence à une classification supervisée mais peuvent être étendues au cas non supervisé. Le premier article \citeindex{Barandela2003} résume les idées concernant le cas d'un problème de classification incluant une classe sous-représentée. Par exemple, pour un problème à deux classes~A et~B lorsque~A regroupe 98\% des exemples, répondre~A quelque soit l'exemple correspond à une erreur de 2\%. Avec plus de 2\% d'erreur, une méthode de classification serait moins performante et pourtant les classes sous-représentées favorise cette configuration. Diverses méthodes sont utilisées pour contrecarrer cet inconvénient comme la pondération des exemples sous-représentés, la multiplication de ces mêmes exemples, bruitées ou non bruitées ou encore la réduction des classes sur-représentées à un échantillon représentatif. Cette dernière option est celle discutée par l'article \citeindex{Barandela2003} qui envisage différentes méthodes de sélection de cet échantillon.








\subsection{Apprentissage d'une distance}


\label{classification_graphem_carac_dist}
\indexfrr{caractéristiques}{distance}
\indexfrr{distance}{apprentissage}
\indexfrr{apprentissage}{distance}

Jusqu'à présent, seule la classification a été traitée mais on peut se demander quelle est la distance la mieux adaptée à une classification. La distance euclidienne accorde un poids égal à toutes les dimensions d'un vecteur. On peut se demander quelle est la pondération optimale pour un problème de classification donné. On définit une distance $d_W$ avec $W = \vecteur{W_1}{W_d}$ pondérant les dimensions de manière non uniforme~:

			\begin{eqnarray}
			d_W\pa{X^1,X^2} = \summy{k=1}{d} \, W_k^2 \, \pa{X^1_k - X^2_k}^2
			\end{eqnarray}
			
\indexfr{prototype}			

Il reste à déterminer le vecteurs de poids $W = \vecteur{W_1}{W_d}$ en s'inspirant par exemple de la méthode développée par \citeindex{Waard1995}. On considère $P$ vecteurs aussi appelés prototypes et notés $\vecteur{X^1}{X^p}$ extrait du nuage $\vecteur{X^1}{X^N}$. On note ensuite pour tout $p \in \ensemble{1}{P}$~:

		\begin{eqnarray}
		y_p\pa{X} = \frac{1}{1 + \exp\pa{d_{W}\pa{X,X^p} + b}}
		\end{eqnarray}
		
On cherche à minimiser le critère~:

		\begin{eqnarray}
		E = \summyone{\pa{p,l} \in A} \pa{y_p\pa{X_l} - d_{pl}}^2 \text{ où } 
				A = \ensemble{1}{P} \times \ensemble{1}{N}
		\end{eqnarray}
				
\indexfr{réseau de neurones}				

Cette minimisation peut être effectuée par une descente de gradient ou dans un algorithme similaire à ceux utilisés pour l'apprentissage des réseaux de neurones (voir paragraphe~\ref{rn_section_train_rn}). Chaque prototype $X_p$ appartient à une classe $C_p$, les coefficients $d_{pl} \in \cro{0,1}$ sont choisis de manière à décrire l'appartenance du vecteur $X_l$ à la classe $C_p$. 

Cette classification pourrait être obtenue à partir d'une classification non supervisée (centres mobiles, classification ascendante hiérarchique) mais cela suppose de disposer déjà d'une distance (comme celle par exemple décrite au paragraphe~\ref{reco_graphem_contour}). Il est possible de répéter le processus jusqu'à convergence, la première classification est effectuée à l'aide d'une distance euclidienne puis une seconde distance est ensuite apprise grâce à la méthode développée dans ce paragraphe. Cette seconde distance induit une nouvelle classification qui pourra à son tour définir une troisième distance. Ce processus peut être répété jusqu'à la classification n'évolue plus.













\subsection{Classification à partir de voisinages}
\label{classification_distance_voisinage}

\indexfrr{classification}{voisinage}
\indexfrr{voisinage}{classification}


L'idée de cette classification est développée dans l'article \citeindex{ZhangYG2004}. Elle repose sur la construction d'un voisinage pour chaque élément d'un ensemble $E = \ensemble{x_1}{x_n}$ à classer. La classification est ensuite obtenue en regroupant ensemble les voisinages ayant une intersection commune. L'objectif étant de proposer une réponse au problème décrit par la figure~\ref{classification_fig_zhang1}.


		\begin{figure}[ht]
		$$\begin{tabular}{|c|}\hline
		\includegraphics[height=5cm, width=6cm]{\filext{../classification/image/zhang1}}
		\\ \hline \end{tabular}$$
		\caption{	Figure extraite de \citeindexfig{ZhangYG2004}, problème classique de classification consistant
							à séparer deux spirales imbriquées l'une dans l'autre.}
		\indexfrr{classification}{voisinage}
		\label{classification_fig_zhang1}
		\end{figure}



Pour chaque $x_i \in E$, on définit son voisinage local $\omega_i = \ensemble{x_{i_1}}{x_{i_K}}$. $K$ est le nombre de voisins et ceux-ci sont classés par ordre de proximité croissante. Par la suite, $x_i$ sera également noté $x_{i_0}$. On définit ensuite la matrice de covariance $S_i$ locale associée à $\omega_i$~:

			\begin{eqnarray}
			m_i = \frac{1}{K+1} \; \summy{k=0}{K} x_{i_k} \text{ et } 
			S_i = \frac{1}{K+1} \; \summy{k=0}{K} \pa{x_{i_k} - m_i } \pa{x_{i_k} - m_i }'
			\label{classif_zhang_eq1}
			\end{eqnarray}

Le vecteur $\lambda_i = \vecteur{\lambda_{i,1}}{\lambda_{i,d}}'$ vérifiant $\lambda_{i,1} \supegal ... \supegal \lambda_{i,d}$, $d$ est la dimension de l'espace vectoriel. L'\emph{adaptibilité} $a_i$  de l'ensemble $\omega_i$ est définie par~: \indexfr{adaptabilité}

			\begin{eqnarray}
			\overline{\lambda_{i,j}}  = \frac{1}{K} \; \summyone{t \in \ensemble{i_1}{i_K} } \lambda_{t,j} \text{ et }
			a_i = \frac{1}{d} \; \summy{j=1}{d} \frac{ \lambda_{i,j} } { \overline { \lambda_{i,j}} }
			\label{classif_zhang_eq2}
			\end{eqnarray}
			
On note également~:

			\begin{eqnarray}
			E\pa{a_i} = \frac{1}{N} \; \summy{i=1}{N} a_i \text{ et } 
			D\pa{a_i} = \sqrt{ \frac{1}{N} \; \summy{i=1}{N} \pa{ a_i - E\pa{a_i} }^2 }
			\label{classif_zhang_eq3}
			\end{eqnarray}
			
			
Dans un premier temps, les voisinages déterminés par cette méthode vont être nettoyés des voisins indésirables. Ce système est souvent représenté sous forme de graphe, chaque n\oe ud représente un élément, chaque arc détermine l'appartenance d'un élément au voisinage d'un autre. Ces graphes sont appelés "\emph{mutual neighborhood graph}" ou \emph{graphe des voisinages mutuels}.
\indexfr{mutual neighborhood graph}
\indexfr{graphe des voisinages mutuels}


			\begin{xalgorithm}{nettoyage des voisinages}
			Les notations utilisées sont celles des expressions (\ref{classif_zhang_eq1}), 
			(\ref{classif_zhang_eq2}), (\ref{classif_zhang_eq3}).
			
			\begin{xalgostep}{estimation}\label{classif_algo_zhang_1}
			Les valeurs $a_i^l$ sont calculées pour chaque ensemble $\omega_i$ privé de $x_{i_l}$ pour élément
			de l'ensemble $\omega_i$.
			\end{xalgostep}

			\begin{xalgostep}{suppression}
			Si $a_i^l > E\pa{a_i} + D\pa{a_i}$, alors l'algorithme s'arrête. Sinon, l'élément $x_{i_s}$ correspondant
			à la plus petite valeur $x_{i_l}$ est supprimée de l'ensemble $\omega_i$. 
			On retourne ensuite à l'étape~\ref{classif_algo_zhang_1}.
			\end{xalgostep}
			
			\end{xalgorithm}


\indexfr{composante connexe}\indexfrr{distance}{euclidienne}

La classification correspond aux composantes connexes du graphe nettoyé qui détermine par ce biais le nombre de classes. L'article suggère également d'associer à chaque élément $x_i$ le vecteur $\pa{x_i, \beta \lambda_i}'$ où $\beta$ est un paramètre de normalisation. Le vecteur $\beta \lambda_i$ caractérise le voisinage. Ainsi, la distance entre deux points dépend à la fois de leur position et de leur voisinage. Les auteurs proposent également d'autres distances que la distance euclidienne. Il reste toutefois à déterminer les paramètres $K$ et $\beta$. 





\subsection{Modélisation de la densité des observations}


\label{classification_modelisation_densite}
\indexfrr{densité}{semi-paramétrique}

L'article \citeindex{Hoti2004} présente une modélisation semi-paramétrique. Soit $Z = \pa{X,Y}$ une variable aléatoire composée du couple $\pa{X,Y}$. La densité de $z$ est exprimée comme suit~:

		\begin{eqnarray}
		f_{X,Y}(x,y) = f_{Y | X=x}(y) \, f_X(x)
		\end{eqnarray}

Dans cet article, la densité $f_X\pa{x}$ est estimée de façon non paramétrique tandis que $f_{Y|X}\pa{y}$ est modélisée par une loi gaussienne. On note $p$ la dimension de $X$ et $q$ celle de $Y$. On note $K_H \pa{x} = \frac{1}{\det H} K \pa{H^{-1} X}$ où $H \in M_p\pa{\R}$ est une matrice carrée définie strictement positive et $K$ un noyau vérifiant $\int_{\R^p} K\pa{x} dx = 1$. $K$ peut par exemple être une fonction gaussienne. Les notations reprennent celles du paragraphe~\ref{modification_janvier_2004_new} (page~\pageref{modification_janvier_2004_new}). On suppose également que la variable $Y | X=x \sim \loinormale{\mu(x)}{\sigma(x)}$. Par conséquent, la densité de la variable $Z =\pa{X,Y}$ s'exprime de la façon suivante~:


		\begin{eqnarray}
		f_{X,Y}(x,y) =  \frac{ f_X(x) } { \sqrt{ \pa{2 \pi}^q \, \det \sigma^2(x) } } \;
										\exp \pa{ - \frac{1}{2} \cro{y - \mu(x)} \, \sigma^{-1}(x) \, \cro{y - \mu(x)}' }
		\end{eqnarray}


La densité $f_X$ est estimée avec un estimateur à noyau à l'aide de l'échantillon $\pa{X_i,Y_i}_{1 \infegal i \infegal N}$~:

		
		\begin{eqnarray}
		\widehat{f_X} (x) = \frac{1}{N} \; \summy{i=1}{N} K_{H} \pa{ x - X_i}
		\end{eqnarray}

On note~:

		\begin{eqnarray}
		W_H\pa{x - X_i} =  \frac{K_H\pa{x - X_i}} {\summy{i=1}{N} K_H\pa{x - X_i} }
		\end{eqnarray}
		
Les fonctions $\mu(x)$ et $\sigma(x)$ sont estimées à l'aide d'un estimateur du maximum de vraisemblance~:
		
		\begin{eqnarray}
		\widehat{\mu}(x) 		&=& \summy{i=1}{n} W_H\pa{x - X_i} \, Y_i \\
		\widehat{\sigma}(x) &=& \summy{i=1}{n} W_H\pa{x - X_i} \, 
														\cro{Y_i - \widehat{\mu}(x)}' \cro{Y_i - \widehat{\mu}(x)} 
		\end{eqnarray}

Le paragraphe~\ref{modification_janvier_2004_new} (page~\pageref{modification_janvier_2004_new}) discute du choix d'une matrice $H$ appropriée (voir également \citeindex{Silverman1986}). En ce qui concerne le problème de classification étudiée ici, la variable $X$ est simplement discrète et désigne la classe de la variable $Y$. Cette méthode est proche de celle développée au paragraphe~\ref{classification_melange_loi_normale} à la seule différence que l'information $X_i$ est ici connue. L'intérêt de cette méthode est sa généralisation au cas où $X$ est une variable continue comme par exemple un vecteur formé des distances du point $X_i$ aux centres des classes déterminées par un algorithme de classification non supervisée. L'article \citeindex{Hoti2004} discute également d'un choix d'une densité $f_X$ paramétrique.









\firstpassagedo{
	\begin{thebibliography}{99}
	\input{classification_bibliographie.tex}
	\end{thebibliography}
}


\input{../../common/livre_table_end.tex}
\input{../../common/livre_end.tex}

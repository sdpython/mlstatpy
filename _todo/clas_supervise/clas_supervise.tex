\input{../../common/livre_begin.tex}
\firstpassagedo{\input{clas_super_titre.tex}}
\input{../../common/livre_table_begin.tex}
\firstpassagedo{\input{clas_super_chapter.tex}}


Cette annexe recense diff�rents moyens d'effectuer une classification supervis�e. Cette t�che consiste � �tiqueter un �l�ment $x$ sachant qu'on conna�t d�j� cet �tiquetage pour un certain nombre d'�l�ments $\vecteur{x_1}{x_N}$ dont les labels sont $\vecteur{c\pa{x_1}}{c\pa{x_N}}$.

\label{classification_supervisee}









%--------------------------------------------------------------------------------------------------------------------
\section{Plus proches voisins}
%--------------------------------------------------------------------------------------------------------------------
\indexfr{plus proches voisins}
\label{clas_super_ppv_par}

Cette m�thode est la plus simple puisqu'elle consiste � associer � $x$, l'�l�ment � classer, le label $c\pa{x_{i^*}}$ de l'�l�ment le plus proche $x_{i^*}$ dans l'ensemble $\vecteur{x_1}{x_N}$. Ceci m�ne � l'algorithme de classification suivant~:


        \begin{xalgorithm}{1-PPV ou plus proche voisin}
        \label{clas_super_1ppv_algo}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$. Soit $x$
        un �l�ment � classer, on cherche � d�terminer la classe $\hat{c}(x)$ associ�e � $x$. On d�finit $x_{i^*}$ 
        comme �tant~:
                        \begin{eqnarray*}
                        x_{i^*} &=& \underset{i \in \intervalle{1}{N}}{\arg \min} \; d\pa{x_i,x}
                        \end{eqnarray*}
        Alors~: $\hat{c}(x) = c\pa{x_i^*}$
        \end{xalgorithm}

\indexfrr{PPV}{1-PPV}
\indexfrr{PPV}{k-PPV}
\indexfr{nearest neighbors}

Cet algorithme est souvent appel� \emph{1-PPV} (ou \emph{1-NN} pour Nearest Neighbors). Il existe une version am�lior�e \emph{k-PPV} qui consiste � attribuer � $x$ la classe la plus repr�sent�e parmi ses $k$ plus proches voisins.



        \begin{xalgorithm}{k-PPV ou k plus proches voisins}
        \label{clas_super_kppv_simple}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$. Soit $x$
        un �l�ment � classer, on cherche � d�terminer la classe $c(x)$ associ�e � $x$. On d�finit l'ensemble $S^*_k$
        incluant les $k$-plus proches voisins de $x$, cet ensemble v�rifie~:
                        \begin{eqnarray*}
                        \card{S^*_k} = 0 \text{ et } 
                                                                         \underset{y \in S^*_k}{\max} \; d\pa{y,x} \infegal
                                                                         \underset{y \in X - S^*_k}{\min} \; d\pa{y,x} 
                        \end{eqnarray*}
        On calcule les occurrences $f(i)$ de chaque classe $i$ dans l'ensemble $S^*_k$~: 
                        \begin{eqnarray}
                        f(i) = \summyone{y \in S^*_k} \, \omega\pa{x,y} \, \indicatrice{c(y) = i} 
                        \label{class_super_kppv_contribution_eq}
                        \end{eqnarray}
        On assigne alors � $x$ la classe $c(x)$ choisie dans l'ensemble~:
                        \begin{eqnarray*}
                        \hat{c}(x) \in \underset{i \in \N}{\arg \max} \; f(i)
                        \end{eqnarray*}
        \end{xalgorithm}

Dans sa version la plus simple, la fonction $\omega\pa{x,y}$ utilis�e lors du calcul de la contribution $f$ (\ref{class_super_kppv_contribution_eq}) est constante. Mais il est possible de lui affecter une valeur tenant compte de la proximit� entre $x$ et $y$. La table~\ref{clas_super_omega_contribution} donne quelques exemples de contributions possibles.


        \begin{table}[ht]
        $$\begin{tabular}{|ll|} \hline 
        fonction constante     & $\omega\pa{x,y} = 1$   \\ \hline
        distance inverse        &    $\omega\pa{x,y} = \frac{1}{1 + d\pa{x,y}}$ \\  \hline
        noyau                                & $\omega\pa{x,y} = \exp\pa{ - d^2 \pa{x,y}}$ \\ \hline
        \end{tabular}$$
        \caption{Exemple de contribution $w\pa{x,y}$ pour l'algorithme~\ref{clas_super_kppv_simple} des k-PPV. 
                            Ces fonctions sont toutes d�croissantes (strictement ou non) par rapport � la distance $d$.}
        \label{clas_super_omega_contribution}
        \end{table}

L'inconv�nient majeur de la m�thode des plus proches voisins est sa longueur puisqu'elle implique le calcul des distances entre $x$ et chacun des �l�ments de l'ensemble $\vecteur{x_1}{x_N}$. C'est pourquoi de nombreuses m�thodes d'optimisation ont �t� d�velopp�es afin d'acc�l�rer ce processus. Il est possible d'optimiser le calcul de la distance ou bien d'�viter un trop nombre de calculs en utilisant des �l�ments pivots\seeannex{space_metric_introduction}{recherche dans un espace m�trique}. L'optimisation de la vitesse est souvent pr�conis�e lorsque l'espace m�trique $E$ n'est pas vectoriel, comme un espace de suites finies. En revanche, l'utilisation de pivots de mani�re � �viter l'exploration de la totalit� de l'ensemble $X$ est valable pour tout espace m�trique. Ces m�thodes sont l'objet de l'annexe~\ref{space_metric_introduction}.



%--------------------------------------------------------------------------------------------------------------------
\section{Support Vector Machines (SVM)}
%--------------------------------------------------------------------------------------------------------------------
\indexfr{SVM}
\indexsee{Support Vector Machines}{SVM}
\indexfrr{plus proches voisins}{SVM}
\indexfrr{hyperplan}{SVM}
\label{clas_super_svm_par}

L'algorithme~\ref{clas_super_kppv_simple} utilise une contribution not�e $\omega$ lors du calcul de $f$ (\ref{class_super_kppv_contribution_eq}). Si celle-ci est d�finie de mani�re explicite, on reste dans le cadre des plus proches voisins. En revanche, si celle-ci est estim�e � partir d'un �chantillon suppos� repr�sentatif du probl�me de classification � r�soudre, on se place dans le cadre des \emph{Support Vector Machines}. Ce formalisme introduit par Vapnik (\citeindex{Vapnik1998}) n'est pas simplement un prolongement de la m�thode des plus proches voisins mais peut aussi �tre interpr�t� comme la recherche du meilleur hyperplan de s�paration entre deux classes. Cette m�thode est pr�sent�e plus en d�tail par l'annexe~\ref{annexe_svm}\seeannex{annexe_svm}{SVM}.




%--------------------------------------------------------------------------------------------------------------------
\section{R�seaux de neurones}
%--------------------------------------------------------------------------------------------------------------------
\indexfrr{r�seau de neurones}{classification}
\label{clas_super_nn_par}

Cette m�thode est pr�sent�e plus en d�tail aux paragraphes~\ref{subsection_classifieur} (page~\pageref{subsection_classifieur}) et~\ref{classification} (page~\pageref{classification})\seeannex{annexe_reseau_neurone}{r�seau de neurones}. Elle permet de construire une fonction $f\pa{x} = y = \vecteur{y_1}{y_C} \in \R^C$ o� $C$ est le nombre de classes, $y_i \in \cro{0,1}$ et $\sum^C_1 y_i = 1$. Chaque sortie $y_i$ du r�seau de neurones correspond � la probabilit� que le vecteur $x$ appartient � la classe~$i$. Contrairement aux deux m�thodes pr�c�dentes, les r�seaux de neurones permettent de construire une fonction $f$ ind�pendante du nombre d'exemples permettant de l'estimer. N�anmoins, les param�tres de cette fonction ne sont plus aussi interpr�tables que les contributions �voqu�es aux paragraphes~\ref{clas_super_ppv_par} et~\ref{clas_super_svm_par}. Ceci explique que le fort int�r�t de ces mod�les depuis les ann�es 1980 au milieu des ann�es 1990 ait d�cru au profit d'autres solutions comme les SVM.







%--------------------------------------------------------------------------------------------------------------------
\section{Learning Vector Quantization (LVQ)}
%--------------------------------------------------------------------------------------------------------------------
\indexfr{LVQ}
\indexsee{Learning Vector Quantization}{LVQ}
\indexfr{plus proches voisins}
\indexfrr{prototype}{LVQ}

Cette m�thode est souvent associ� � des m�thodes de classification par plus proches voisins �voqu�es dans l'annexe~\ref{space_metric_introduction}. Lors de la classification d'un �l�ment, on recherche dans un ensemble le plus proche �l�ment et on attribue � l'�l�ment � classer la classe de l'�l�ment trouv�. Alors que l'annexe~\ref{space_metric_introduction} cherche � acc�l�rer la recherche de l'�l�ment le plus proche, la m�thode LVQ essaye de r�sumer l'information au travers de prototypes. Plus simplement, les m�thodes abord�es ici permettent tente r�duire au minimum l'ensemble dans lequel seront cherch�s les voisins sans changer ou sans trop changer le r�sultat de la classification. 

En ce qui concerne les nu�es dynamiques, les prototypes sont les centres des classes d�termin�es par l'algorithme des centres mobiles. Pour les diff�rentes versions LVQ qui suivent, les prototypes doivent repr�senter au mieux une classification impos�e. L'article~\citeindex{Bezdek2001} propose une revue r�cente de ces m�thodes que reprend en partie seulement un article~\citeindex{Kim2003} sur lequel s'appuie les paragraphes qui suivent.



\subsection{Principe}
\label{clas_super_principe_lvq}

Lors de l'algorithme~\ref{clas_super_1ppv_algo} qui permet de classer un �l�ment $x$ en l'associant � la m�me classe que son plus proche voisin, il faut calculer toutes les distances de $x$ aux voisins possibles $X$. Les m�thodes LVQ ont pour objectif de r�duire l'ensemble $X$ � une taille raisonnable en utilisant l'information de la classe. Apr�s r�duction, l'algorithme de classification doit retourner les m�mes r�ponses. Par cons�quent, l'algorithme suivant~\ref{clas_super_1ppv_lvq_algo} doit retourner les m�mes r�ponses que la m�thode 1-ppv~\ref{clas_super_1ppv_algo}.



        \begin{xalgorithm}{1-PPV avec LVQ}
        \label{clas_super_1ppv_lvq_algo}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$. Soit $x$
        un �l�ment � classer, on cherche � d�terminer la classe $c(x)$ associ�e � $x$.
        
        \begin{xalgostep}{LVQ}\label{clas_super_lvq_step_identity}
        On r�duit l'ensemble $X$ � un ensemble de prototypes $X'$ qui n'est pas forc�ment
        inclus dans $X$. On note $X' = \vecteur{x'_1}{x'_n}$ avec de pr�f�rence $n << N$.
        La classe de l'�l�ment $x_i'$ est toujours not�e $c\pa{x'_i}$. Les algorithmes effectuant 
        cette r�duction sont pr�sent�es dans les paragraphes qui suivent comme~\ref{clas_super_lvq_cnn}
        ou~\ref{clas_super_lvq_pnn}.
        \end{xalgostep}
        
        \begin{xalgostep}{classification}\label{clas_super_lvq_step_clas_b}
        On d�finit $x'_{i^*}$ comme �tant~:
                        \begin{eqnarray*}
                        x'_{i^*} &=& \underset{i \in \intervalle{1}{n}}{\arg \min} \; d\pa{x'_i,x}
                        \end{eqnarray*}
        Alors~: $\hat{c}(x) = c\pa{x'_{i^*}}$
        \end{xalgostep}
        \end{xalgorithm}

\indexfrr{prototype}{LVQ}

Les paragraphes qui suivent pr�sentent des algorithmes permettant de calculer un ensemble $X'$ satisfaisant � l'�tape~\ref{clas_super_lvq_step_identity} et le plus r�duit possible. Cette �tape~\ref{clas_super_lvq_step_identity} est un fait un pr�traitement, elle n'est effectu�e qu'une seule fois tandis que l'�tape~\ref{clas_super_lvq_step_clas_b} intervient pour chaque nouvel �l�ment � classer. L'ensemble $X'$ est appel� l'ensemble des \emph{prototypes}. Les chapitres qui suivent concernent essentiellement les espaces vectoriels except� pour le paragraphe~\ref{clas_super_lvq_cnn}. Pour des espaces m�triques non vectoriels, l'annexe~\ref{space_metric_introduction} pr�sente d'autres m�thodes de s�lection de prototypes\seeannex{space_metric_suppression_voisins_inutile}{suppression des voisins inutiles}.







\subsection{Condensed nearest neighbors rule (CNN)}
\label{clas_super_lvq_cnn}
\indexfr{CNN}
\indexsee{Condensed nearest neighbors}{CNN}

Cette m�thode est d�velopp�e dans \citeindex{Hart1968}. Elle consiste � construire un ensemble $X'$ � partir des �l�ments de $X$. Un premier �l�ment est choisi al�atoirement puis plac� dans $X'$. On parcourt ensuite l'ensemble $X$, pour chaque �l�ment $x$, on applique l'algorithme~\ref{clas_super_1ppv_lvq_algo}. Si le r�sultat ne correspond pas � la classe $c(x)$, cet �l�ment est ajout� � l'ensemble $X'$. Ceci m�ne � l'algorithme suivant~:

        
        \begin{xalgorithm}{CNN}\label{clas_super_algorithme_cnn_choice}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$. 
        
        \begin{xalgostep}{initialisation}
        Soit $x$ un �l�ment de $X$,     $X' \longleftarrow \acc{ x}$ et $Y \longleftarrow \acc{x}$.
        \end{xalgostep}
        
        \begin{xalgostep}{construction de $X'$}\label{clas_super_cnn_step_b}
        \begin{xwhile}{$Y \neq X$}
        Soit $x \in X - Y$, on applique l'�tape~\ref{clas_super_lvq_step_clas_b} de 
        l'algorithme~\ref{clas_super_1ppv_lvq_algo} � l'�l�ment $x$.\\
            \begin{xif}{$\hat{c}(x) \neq c(x)$}
            $X' \longleftarrow X' \cup \acc{x}$
            \end{xif}\\
            $Y \longleftarrow Y \cup \acc{x}$
        \end{xwhile}
        \end{xalgostep}
        
        \end{xalgorithm}

\indexfrr{ordre d'insertion}{LVQ}

Cet algorithme n'impose pas un nombre pr�cis de prototypes. De plus, puisque $X' \subset X$, cette m�thode est applicable � tout espace m�trique, il ne n�cessite pas qu'il soit vectoriel. Toutefois, l'algorithme est sensible � l'ordre dans lequel sont trait�s les �l�ments de $X$.



\subsection{Prototype for nearest neighbors (PNN)}
\indexfr{PNN}
\indexsee{Prototype nearest neighbors}{PNN}
\label{clas_super_lvq_pnn}

Cette m�thode est d�velopp�e dans \citeindex{Chang1974}. Contrairement � l'algorithme pr�c�dent~\ref{clas_super_algorithme_cnn_choice}, l'ensemble $X'$ n'est plus inclus dans $X$ et est construit de mani�re � obtenir autant que faire ce peu des barycentres des classes. Il ne s'applique donc qu'� des espaces vectoriels. Au d�part, tous les �l�ments de $X$ sont consid�r�s comme des prototypes. Puis les plus proches d'entre eux appartenant � la m�me classe vont �tre agr�g�s si aucune erreur de classification n'est constat�e.



        \begin{xalgorithm}{PNN}\label{clas_super_algorithme_pnn_choice}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$. 
        
        \begin{xalgostep}{initialisation}
        On d�finit les ensembles $A \longleftarrow \emptyset$ et $B \longleftarrow X$ ainsi que la suite
        $\vecteur{p\pa{x_1}}{p\pa{x_N}}$ telle que $\forall i, \; p\pa{x_i} = 1$. 
        $t\longleftarrow 0$ et $\epsilon_0 \longleftarrow \infty$.
        \end{xalgostep}
        
        \begin{xalgostep}{construction de $B$}\label{clas_super_pnn_step_b}
        \begin{xwhile}{$B \neq \emptyset$}
            $m \longleftarrow 0$.    On d�finit $x_A \in A$ et $x_B \in B$ tels que~: 
                        $$d\pa{x_A,x_B} = \min \acc{d\pa{x,y} \sac x \in A, y \in B}$$
            \begin{xif}{$c\pa{x_A} \neq c\pa{x_B}$}
                    $B \longleftarrow B - \acc{x_B}$ et $A \longleftarrow A \cup \acc{x_B}$
            \xelse
                    $x \longleftarrow \dfrac{ p\pa{x_A} \,x_A + p\pa{x_B} \,x_B }{ p\pa{x_A} + p\pa{x_B}}$ \\
                    $p\pa{x} \longleftarrow p\pa{x_A} + p\pa{x_B}$ \\
                    On note $\epsilon_t$ le taux de classification obtenu avec l'ensemble de prototypes
                    $X' = A \cup \acc{ x }$. \\
                    \begin{xif}{$\epsilon_t > \epsilon_{t-1}$}
                        $B \longleftarrow B - \acc{x_B}$ et $A \longleftarrow A \cup \acc{x_B}$
                    \xelse
                        $B \longleftarrow B - \acc{x_B}$ et $A \longleftarrow \cro{ A - \acc{x_A}} \cup \acc{ x }$     \\
                        $m \longleftarrow m + 1$
                    \end{xif}
            \end{xif}
        \end{xwhile}
        \end{xalgostep}
        
        \begin{xalgostep}{terminaison}
        \begin{xif}{$ m > 0 $}
            $B \longleftarrow A$ et $A  \longleftarrow \emptyset$. \\
            On retourne � l'�tape~\ref{clas_super_pnn_step_b}.
        \xelse
            L'algorithme s'arr�te et l'ensemble cherch� $X' \longleftarrow A$.
        \end{xif}
        \end{xalgostep}

        \end{xalgorithm}

L'article \citeindex{Bezdek2001} sugg�re de ne consid�rer lors de l'�tape~\ref{clas_super_pnn_step_b} que des paires $\pa{x_A,x_B}$ appartenant � une m�me classe de mani�re � ce que le r�sultat obtenu soit plus consistent. Ce second algorithme est plus lent que l'algorithme~\ref{clas_super_algorithme_cnn_choice} mais la remarque � propos l'ordre d'insertion ne le concerne plus.





\subsection{LVQ1, ..., LVQ4}

Les LVQ ont �t� introduits dans \citeindex{Linde1980}, adapt�s par la suite par Kohonen � la reconnaissance des formes (\citeindex{Kohonen1982}, \citeindex{Kohonen1995}). Historiquement, le premier algorithme LVQ1 est d� � Kohonen et permet de d�terminer un nombre fix� de prototypes, contrairement aux deux algorithmes~\ref{clas_super_algorithme_cnn_choice} et~\ref{clas_super_algorithme_pnn_choice} des paragraphes pr�c�dents ne n�cessitant aucun a priori sur leur nombre.

\indexfrr{LVQ}{LVQ1}


        \begin{xalgorithm}{LVQ1}\label{clas_super_lvq1_algo}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$.
        Soit $P = \vecteur{p_1}{p_k}$ $k$ prototypes tir�s al�atoirement.    
        On associe une classe $\overline{c}\pa{p_k}$ � chaque 
        prototype. La suite $\pa{\alpha_t}$ est une suite positive v�rifiant~: 
        $\sum_t \alpha_t = \infty$ et $\sum_t \alpha_t^2 < \infty$. Enfin, $t \longleftarrow 0$.

        \begin{xalgostep}{meilleur prototype}\label{clas_super_lvq1_step1}
        $t \longleftarrow t +1$ \\
        On choisit al�atoirement un �l�ment $x \in X$.
        On d�termine $p^* = \underset{p \in P}{\arg \min} \, d\pa{x,p}$.
        \end{xalgostep}

        \begin{xalgostep}{mise � jour}
        $p^*  \longleftarrow p^*  + \left\{ \begin{array}{rl}
                                    \alpha_t \pa{ x - p^*}         & \text{si } \overline{c}\pa{p^*} = c\pa{x}\\
                                    - \alpha_t \pa{ x - p^*}     & \text{si } \overline{c}\pa{p^*} \neq c\pa{x}
                                    \end{array} \right.$ \\
        On retourne � l'�tape~\ref{clas_super_lvq1_step1} tant que les prototypes continuent d'�voluer.
        \end{xalgostep}
        \end{xalgorithm}
        
Le nombre de prototypes est fix� au d�part ainsi que la classe qui est associ�e � chacun d'eux. Cet algorithme est souvent utilis� avec autant de prototypes qu'il y a de classes. La suite $\pa{\alpha_t}$ est en principe une suite d�croissante mais qui peut �tre choisie de telle mani�re que~:

        \begin{eqnarray}
        \alpha_{t+1} = \left\{ \begin{array}{ll}
                                                \frac{\alpha_t}{1 + \alpha_t} & \text{si } \overline{c}\pa{p^*} = c\pa{x}\\
                                                \frac{\alpha_t}{1 - \alpha_t} & \text{si } \overline{c}\pa{p^*} \neq c\pa{x}
                                                \end{array} \right.
        \end{eqnarray}
        
\indexfrr{Algorithme}{OLVQ1}        
        
Le pas d'apprentissage $\alpha_t$ cro�t si le prototype le plus proche est d'une classe diff�rente de celle de l'�l�ment $x$. Cette version de l'algorithme LVQ1 est appel� \emph{Optimized LVQ1}. Cette optimisation est valable pour tous les algorithmes de la famille LVQ qui suivent. La seconde version de cet algorithme propose la mise � jour simultan�e de deux prototypes qui permet d'am�liorer les fronti�res de d�cision. 

\indexfrr{LVQ}{LVQ2}

        \begin{xalgorithm}{LVQ2}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$.
        Soit $P = \vecteur{p_1}{p_k}$ $k$ prototypes tir�s al�atoirement.    
        On associe une classe $\overline{c}\pa{p_k}$ � chaque 
        prototype. La suite $\pa{\alpha_t}$ est une suite positive v�rifiant~: 
        $\sum_t \alpha_t = \infty$ et $\sum_t \alpha_t^2 < \infty$. On pose �galement $t \longleftarrow 0$. 
        Soit $w \in \left]0,1\right[$.

        \begin{xalgostep}{meilleur prototype}\label{clas_super_lvq2_step1}
        $t \longleftarrow t +1$ \\
        On choisit al�atoirement un �l�ment $x \in X$. On d�termine~: \\
        $p^*_1 = \arg \min \acc{ d\pa{x,p} \sac p \in P, \, \overline{p}\pa{p} = c\pa{x} }$ et \\
        $p^*_2 = \arg \min \acc{ d\pa{x,p} \sac p \in P, \, \overline{p}\pa{p} \neq c\pa{x} }$. 
        \end{xalgostep}

        \begin{xalgostep}{mise � jour}
        \begin{xif}{$\frac{1-w}{1+w} < \frac{d\pa{x,p^*_1}}{d\pa{x,p^*_2}} < \frac{1+w}{1-w}$}
            $p^*_1  \longleftarrow p^*_1  + \alpha_t \pa{ x - p^*_1}$ \\
            $p^*_2     \longleftarrow p^*_2  - \alpha_t \pa{ x - p^*_2}$ 
        \end{xif} \\
        On retourne � l'�tape~\ref{clas_super_lvq2_step1} tant que les prototypes continuent d'�voluer.
        \end{xalgostep}
        \end{xalgorithm}
        
        
\indexfrr{LVQ}{LVQ3}        

Le livre \citeindex{Kohonen1995} sugg�re de choisir $w \in \cro{0,2 \,;\, 0,3 }$. L'algorithme LVQ3 qui suit propose une extension de l'algorithme LVQ2 pour des prototypes $p^*_1$ et $p^*_2$ appartenant � la m�me classe.

        \begin{xalgorithm}{LVQ3}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$.
        Soit $P = \vecteur{p_1}{p_k}$ $k$ prototypes tir�s al�atoirement.    
        On associe une classe $\overline{c}\pa{p_k}$ � chaque 
        prototype. La suite $\pa{\alpha_t}$ est une suite positive v�rifiant~: 
        $\sum_t \alpha_t = \infty$ et $\sum_t \alpha_t^2 < \infty$. On pose �galement $t \longleftarrow 0$. 
        Soit $w \in \left]0,1\right[$ et $\epsilon \in \cro{0,1 \,;\, 0,5}$.

        \begin{xalgostep}{meilleur prototype}\label{clas_super_lvq2_step1}
        $t \longleftarrow t +1$ \\
        On choisit al�atoirement un �l�ment $x \in X$. On d�termine~: \\
        $p^*_1 = \arg \min \acc{ d\pa{x,p} \sac p \in P, \, \overline{p}\pa{p} = c\pa{x} }$ et \\
        $p^*_2 = \arg \min \acc{ d\pa{x,p} \sac p \in P, \, p^*_2 \neq p^*_1}$. 
        \end{xalgostep}

        \begin{xalgostep}{mise � jour}
        \begin{xif}{$\overline{c}\pa{p^*_1} \neq \overline{c}\pa{p^*_2}$}
            \begin{xif}{$\frac{1-w}{1+w} < \frac{d\pa{x,p^*_1}}{d\pa{x,p^*_2}} < \frac{1+w}{1-w}$}
                $p^*_1  \longleftarrow p^*_1  + \alpha_t \pa{ x - p^*_1}$ \\
                $p^*_2     \longleftarrow p^*_2  - \alpha_t \pa{ x - p^*_2}$ 
            \end{xif} 
        \xelse
            $p^*_1  \longleftarrow p^*_1  + \epsilon \alpha_t \pa{ x - p^*_1}$ \\
            $p^*_2     \longleftarrow p^*_2  + \epsilon \alpha_t \pa{ x - p^*_2}$ 
        \end{xif}            
        On retourne � l'�tape~\ref{clas_super_lvq2_step1} tant que les prototypes continuent d'�voluer.
        \end{xalgostep}
        \end{xalgorithm}

L'algorithme LVQ4 est la version la plus r�cente. Il s'inspire de l'algorithme LVQ1 mais modifie le poids de l'apprentissage $\alpha_t$ de mani�re plus pertinente.


        \begin{xalgorithm}{LVQ4}
        Soit $X = \vecteur{x_1}{x_N} \subset E$ un ensemble d'�l�ments d'un espace m�trique quelconque, 
        soit $\vecteur{c\pa{x_1}}{c\pa{x_N}}$ les classes associ�es � chacun des �l�ments de $X$. On note 
        $d$ la distance d�finie sur l'espace m�trique $E$.
        Soit $P_t = \vecteur{p_{t,1}}{p_{t,k}}$ $k$ prototypes tir�s al�atoirement.    
        On associe une classe $\overline{c}\pa{p_{t,k}}$ � chaque 
        prototype. La suite $\pa{\alpha_t}$ est une suite positive v�rifiant~: 
        $\sum_t \alpha_t = \infty$ et $\sum_t \alpha_t^2 < \infty$. On pose �galement $t \longleftarrow 0$. 
        On suppose que $\forall t, \, \alpha_t \in ]0,1[$. Soit $\lambda > 1$.

        \begin{xalgostep}{meilleur prototype}\label{clas_super_lvq4_step1}
        $t \longleftarrow t +1$ \\
        On choisit al�atoirement un �l�ment $x \in X$.
        On d�termine $p^*_t = \underset{p \in P_t}{\arg \min} \, d\pa{x,p}$.
        \end{xalgostep}

        \begin{xalgostep}{mise � jour}
        $p^*_t  \longleftarrow p^*_t  + s\pa{p^*_t} \, \alpha_t \pa{ x - p^*_t}$ o�
        $$ s\pa{p^*_t} = \left\{ \begin{array}{ll}
                 \lambda                                                     & \text{si } \overline{c}\pa{p^*_t} = c\pa{x} \text{ et } M\pa{p^*_t} = 0\\
                 \frac{B\pa{p^*_t}}{M\pa{p^*_t}}     & \text{si } \overline{c}\pa{p^*_t} = c\pa{x} \text{ et } M\pa{p^*_t} > 0\\
                 -1                                                             & \text{si } \overline{c}\pa{p^*_t} \neq c\pa{x}
                 \end{array} \right.$$
        
        $B\pa{p^*_t}$ repr�sente le nombre d'exemples bien class�s avec le prototype $p^*_t$ 
        tandis que $M\pa{p^*_t}$ est le nombre d'exemples mal class�s. 
        On retourne � l'�tape~\ref{clas_super_lvq4_step1} tant que les prototypes continuent d'�voluer.
        \end{xalgostep}
        \end{xalgorithm}

L'inconv�nient de cet algorithme est le calcul co�teux de $B\pa{p^*_t}$ et $M\pa{p^*_t}$. L'�valuation des nombres $B\pa{p}$ et $M\pa{t}$ pour $p \in P_t$ devrait �tre effectu�e � chaque it�ration $t$, c'est-�-dire � chaque fois qu'un prototype est actualis�. Afin d'acc�l�rer l'algorithme, cette �valuation n'est pas effectu�e � chaque it�ration mais toutes les $T$ it�rations o� $T$ serait la p�riode de mise � jour. Durant une p�riode, ces nombres peuvent �tre consid�r�s comme constants o� �voluer en tenant de compte leur pass�. Diff�rentes variantes de l'algorithme $LVQ4$ sont propos�es et discut�es dans l'article \citeindex{Vakil2003}.





%--------------------------------------------------------------------------------------------------------------------
\section{Prolongations}
%--------------------------------------------------------------------------------------------------------------------

\subsection{Liens entre LVQ et la r�tropropagation}

\indexfrr{LVQ}{r�tropropagation}
\indexfrr{r�tropropagation}{LVQ}
\indexfr{RBF}
\indexsee{Radial basis function}{RBF}
\indexsee{fonction � base radiale}{RBF}

L'article \citeindex{Frasconi1997} met en rapport l'algorithme LVQ1~(\ref{clas_super_lvq1_algo}) et l'algorithme de r�tropropagation~\ref{algo_retropropagation_class} dans un r�seau de neurones sont les fonctions de transfert sont des fonctions � base radiale ou RBF\seeannex{rnn_fonction_base_radiale_rbf}{fonction � base radiale}. Ce r�seau de neurones contient autant de neurones sur la couche cach�e qu'il y a de prototypes dans l'algorithme LVQ1. La sortie des neurones cach�s est donn�e par~:

                    $$
                    z_i = \exp\cro{ - \frac{\norme{p - x}^2}{\sigma^2}}
                    $$

$p$ est un prototype, $x$ est un �l�ment, l'�l�ment pour lequel on �value les sorties du r�seau de neurones. L'article~\citeindex{Frasconi1997} que lorsque $\sigma \longrightarrow 0$, on construit ensuite le nombre~:

                    $$
                    z'_i = \frac{z_i}{\summyone{i}z_i}
                    $$

Lorsque $\sigma\longrightarrow 0$, le vecteur $\vecteur{z'_1}{z'_k}$ converge vers un vecteur presque nul sauf pour le prototype $i$ le plus proche. De m�me, lorsque $\sigma\longrightarrow 0$, une it�ration d'un apprentissage par r�tropropagation d'un tel r�seau de neurones est �quivalente � une it�ration de l'algorithme LVQ1.



\firstpassagedo{
    \begin{thebibliography}{99}
    \input{clas_super_biblio.tex}
    \end{thebibliography}
}


\input{../../common/livre_table_end.tex}
\input{../../common/livre_end.tex}
